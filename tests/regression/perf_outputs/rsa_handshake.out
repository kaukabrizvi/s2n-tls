--------------------------------------------------------------------------------
-- Metadata
--------------------------------------------------------------------------------
Invocation:       /usr/bin/cg_annotate cachegrind.out.139115
Command:          target/debug/rsa_handshake
Events recorded:  Ir
Events shown:     Ir
Event sort order: Ir
Threshold:        0.1%
Annotation:       on

--------------------------------------------------------------------------------
-- Summary
--------------------------------------------------------------------------------
Ir_________________ 

78,880,058 (100.0%)  PROGRAM TOTALS

--------------------------------------------------------------------------------
-- File:function summary
--------------------------------------------------------------------------------
  Ir_______________________  file:function

< 71,798,872 (91.0%, 91.0%)  /home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/x86_64-mont5.S:
  54,908,926 (69.6%)           aws_lc_0_19_0_bn_sqr8x_internal
  15,699,024 (19.9%)           mul4x_internal
   1,114,840  (1.4%)           __bn_post4x_internal

<  1,551,316  (2.0%, 93.0%)  /home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:
     676,336  (0.9%)           __ecp_nistz256_mul_montq
     475,750  (0.6%)           __ecp_nistz256_sqr_montq
      95,732  (0.1%)           aws_lc_0_19_0_ecp_nistz256_point_double

<    833,553  (1.1%, 94.0%)  /home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/sha256-x86_64.S:
     830,671  (1.1%)           sha256_block_data_order_avx

<    557,835  (0.7%, 94.8%)  /home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/x86_64-mont.S:
     493,032  (0.6%)           bn_mul4x_mont

<    464,810  (0.6%, 95.3%)  /home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/crypto/fipsmodule/bn/asm/x86_64-gcc.c:
     323,480  (0.4%)           aws_lc_0_19_0_bn_mul_add_words
      90,486  (0.1%)           aws_lc_0_19_0_bn_mul_words

<    375,409  (0.5%, 95.8%)  /home/ubuntu/proj/s2n/bindings/rust/s2n-tls-sys/lib/stuffer/s2n_stuffer.c:
     235,223  (0.3%)           s2n_stuffer_validate

<    370,621  (0.5%, 96.3%)  /home/ubuntu/proj/s2n/bindings/rust/s2n-tls-sys/lib/utils/s2n_blob.c:
     352,205  (0.4%)           s2n_blob_validate

<    179,455  (0.2%, 96.5%)  /home/ubuntu/proj/s2n/bindings/rust/s2n-tls-sys/lib/crypto/s2n_hmac.c:
     119,012  (0.2%)           s2n_tls_hmac_init

<    164,230  (0.2%, 96.7%)  /home/ubuntu/proj/s2n/bindings/rust/s2n-tls-sys/lib/utils/s2n_result.c:
     164,016  (0.2%)           s2n_result_is_ok

<    123,691  (0.2%, 96.9%)  ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S:__memset_avx2_unaligned_erms

<    106,888  (0.1%, 97.0%)  /home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/sha512-x86_64.S:
     106,762  (0.1%)           sha512_block_data_order_avx

--------------------------------------------------------------------------------
-- Function:file summary
--------------------------------------------------------------------------------
  Ir_______________________  function:file

> 54,908,926 (69.6%, 69.6%)  aws_lc_0_19_0_bn_sqr8x_internal:/home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/x86_64-mont5.S

> 15,699,024 (19.9%, 89.5%)  mul4x_internal:/home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/x86_64-mont5.S

>  1,114,840  (1.4%, 90.9%)  __bn_post4x_internal:/home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/x86_64-mont5.S

>    830,671  (1.1%, 92.0%)  sha256_block_data_order_avx:/home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/sha256-x86_64.S

>    676,336  (0.9%, 92.8%)  __ecp_nistz256_mul_montq:/home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S

>    493,032  (0.6%, 93.5%)  bn_mul4x_mont:/home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/x86_64-mont.S

>    475,750  (0.6%, 94.1%)  __ecp_nistz256_sqr_montq:/home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S

>    352,205  (0.4%, 94.5%)  s2n_blob_validate:/home/ubuntu/proj/s2n/bindings/rust/s2n-tls-sys/lib/utils/s2n_blob.c

>    323,480  (0.4%, 94.9%)  aws_lc_0_19_0_bn_mul_add_words:/home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/crypto/fipsmodule/bn/asm/x86_64-gcc.c

>    235,223  (0.3%, 95.2%)  s2n_stuffer_validate:/home/ubuntu/proj/s2n/bindings/rust/s2n-tls-sys/lib/stuffer/s2n_stuffer.c

>    164,016  (0.2%, 95.4%)  s2n_result_is_ok:/home/ubuntu/proj/s2n/bindings/rust/s2n-tls-sys/lib/utils/s2n_result.c

>    123,691  (0.2%, 95.6%)  __memset_avx2_unaligned_erms:./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S

>    119,012  (0.2%, 95.7%)  s2n_tls_hmac_init:/home/ubuntu/proj/s2n/bindings/rust/s2n-tls-sys/lib/crypto/s2n_hmac.c

>    106,762  (0.1%, 95.9%)  sha512_block_data_order_avx:/home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/sha512-x86_64.S

>     95,732  (0.1%, 96.0%)  aws_lc_0_19_0_ecp_nistz256_point_double:/home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S

>     90,486  (0.1%, 96.1%)  aws_lc_0_19_0_bn_mul_words:/home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/crypto/fipsmodule/bn/asm/x86_64-gcc.c

--------------------------------------------------------------------------------
-- Annotated source file: ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S
--------------------------------------------------------------------------------
Unannotated because one or more of these original files are unreadable:
- ./string/../sysdeps/x86_64/multiarch/memset-vec-unaligned-erms.S

--------------------------------------------------------------------------------
-- Annotated source file: /home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/crypto/fipsmodule/bn/asm/x86_64-gcc.c
--------------------------------------------------------------------------------
Ir___________ 

-- line 90 ----------------------------------------
     .                     : "cc");                                                       \
     .             (r) = (carry);                                                         \
     .             (carry) = high;                                                        \
     .           } while (0)
     .         #undef sqr
     .         #define sqr(r0, r1, a) __asm__("mulq %2" : "=a"(r0), "=d"(r1) : "a"(a) : "cc");
     .         
     .         BN_ULONG bn_mul_add_words(BN_ULONG *rp, const BN_ULONG *ap, size_t num,
 2,952 (0.0%)                            BN_ULONG w) {
   328 (0.0%)    BN_ULONG c1 = 0;
     .         
   656 (0.0%)    if (num == 0) {
     .             return (c1);
     .           }
     .         
 7,392 (0.0%)    while (num & ~3) {
77,464 (0.1%)      mul_add(rp[0], ap[0], w, c1);
74,096 (0.1%)      mul_add(rp[1], ap[1], w, c1);
74,096 (0.1%)      mul_add(rp[2], ap[2], w, c1);
74,096 (0.1%)      mul_add(rp[3], ap[3], w, c1);
 3,368 (0.0%)      ap += 4;
 3,368 (0.0%)      rp += 4;
 3,368 (0.0%)      num -= 4;
     .           }
   656 (0.0%)    if (num) {
     .             mul_add(rp[0], ap[0], w, c1);
     .             if (--num == 0) {
     .               return c1;
     .             }
     .             mul_add(rp[1], ap[1], w, c1);
     .             if (--num == 0) {
     .               return c1;
     .             }
     .             mul_add(rp[2], ap[2], w, c1);
     .             return c1;
     .           }
     .         
   328 (0.0%)    return c1;
 1,312 (0.0%)  }
     .         
     .         BN_ULONG bn_mul_words(BN_ULONG *rp, const BN_ULONG *ap, size_t num,
   594 (0.0%)                        BN_ULONG w) {
    66 (0.0%)    BN_ULONG c1 = 0;
     .         
   132 (0.0%)    if (num == 0) {
     .             return c1;
     .           }
     .         
 2,244 (0.0%)    while (num & ~3) {
19,008 (0.0%)      mul(rp[0], ap[0], w, c1);
21,120 (0.0%)      mul(rp[1], ap[1], w, c1);
21,120 (0.0%)      mul(rp[2], ap[2], w, c1);
21,120 (0.0%)      mul(rp[3], ap[3], w, c1);
 1,056 (0.0%)      ap += 4;
 1,056 (0.0%)      rp += 4;
 1,056 (0.0%)      num -= 4;
     .           }
   132 (0.0%)    if (num) {
 1,188 (0.0%)      mul(rp[0], ap[0], w, c1);
   198 (0.0%)      if (--num == 0) {
   132 (0.0%)        return c1;
     .             }
     .             mul(rp[1], ap[1], w, c1);
     .             if (--num == 0) {
     .               return c1;
     .             }
     .             mul(rp[2], ap[2], w, c1);
     .           }
     .           return c1;
   264 (0.0%)  }
     .         
     .         void bn_sqr_words(BN_ULONG *r, const BN_ULONG *a, size_t n) {
     .           if (n == 0) {
     .             return;
     .           }
     .         
     .           while (n & ~3) {
     .             sqr(r[0], r[1], a[0]);
-- line 167 ----------------------------------------
-- line 181 ----------------------------------------
     .             if (--n == 0) {
     .               return;
     .             }
     .             sqr(r[4], r[5], a[2]);
     .           }
     .         }
     .         
     .         BN_ULONG bn_add_words(BN_ULONG *rp, const BN_ULONG *ap, const BN_ULONG *bp,
   406 (0.0%)                        size_t n) {
     .           BN_ULONG ret;
    58 (0.0%)    size_t i = 0;
     .         
   116 (0.0%)    if (n == 0) {
     .             return 0;
     .           }
     .         
 3,634 (0.0%)    __asm__ volatile (
     .               "	subq	%0,%0		\n"  // clear carry
     .               "	jmp	1f		\n"
     .               ".p2align 4			\n"
     .               "1:"
     .               "	movq	(%4,%2,8),%0	\n"
     .               "	adcq	(%5,%2,8),%0	\n"
     .               "	movq	%0,(%3,%2,8)	\n"
     .               "	lea	1(%2),%2	\n"
     .               "	dec	%1		\n"
     .               "	jnz	1b		\n"
     .               "	sbbq	%0,%0		\n"
     .               : "=&r"(ret), "+c"(n), "+r"(i)
     .               : "r"(rp), "r"(ap), "r"(bp)
     .               : "cc", "memory");
     .         
   116 (0.0%)    return ret & 1;
   116 (0.0%)  }
     .         
     .         BN_ULONG bn_sub_words(BN_ULONG *rp, const BN_ULONG *ap, const BN_ULONG *bp,
 1,036 (0.0%)                        size_t n) {
     .           BN_ULONG ret;
   148 (0.0%)    size_t i = 0;
     .         
   296 (0.0%)    if (n == 0) {
     .             return 0;
     .           }
     .         
32,716 (0.0%)    __asm__ volatile (
     .               "	subq	%0,%0		\n"  // clear borrow
     .               "	jmp	1f		\n"
     .               ".p2align 4			\n"
     .               "1:"
     .               "	movq	(%4,%2,8),%0	\n"
     .               "	sbbq	(%5,%2,8),%0	\n"
     .               "	movq	%0,(%3,%2,8)	\n"
     .               "	lea	1(%2),%2	\n"
     .               "	dec	%1		\n"
     .               "	jnz	1b		\n"
     .               "	sbbq	%0,%0		\n"
     .               : "=&r"(ret), "+c"(n), "+r"(i)
     .               : "r"(rp), "r"(ap), "r"(bp)
     .               : "cc", "memory");
     .         
   296 (0.0%)    return ret & 1;
   296 (0.0%)  }
     .         
     .         // mul_add_c(a,b,c0,c1,c2)  -- c+=a*b for three word number c=(c2,c1,c0)
     .         // mul_add_c2(a,b,c0,c1,c2) -- c+=2*a*b for three word number c=(c2,c1,c0)
     .         // sqr_add_c(a,i,c0,c1,c2)  -- c+=a[i]^2 for three word number c=(c2,c1,c0)
     .         // sqr_add_c2(a,i,c0,c1,c2) -- c+=2*a[i]*a[j] for three word number c=(c2,c1,c0)
     .         
     .         // Keep in mind that carrying into high part of multiplication result can not
     .         // overflow, because it cannot be all-ones.
-- line 250 ----------------------------------------
-- line 279 ----------------------------------------
     .             __asm__("addq %3,%0; adcq %4,%1; adcq %5,%2"                     \
     .                     : "+r"(c0), "+r"(c1), "+r"(c2)                           \
     .                     : "r"(t1), "r"(t2), "g"(0)                               \
     .                     : "cc");                                                 \
     .           } while (0)
     .         
     .         #define sqr_add_c2(a, i, j, c0, c1, c2) mul_add_c2((a)[i], (a)[j], c0, c1, c2)
     .         
    63 (0.0%)  void bn_mul_comba8(BN_ULONG r[16], const BN_ULONG a[8], const BN_ULONG b[8]) {
     .           BN_ULONG c1, c2, c3;
     .         
     9 (0.0%)    c1 = 0;
     9 (0.0%)    c2 = 0;
     9 (0.0%)    c3 = 0;
   153 (0.0%)    mul_add_c(a[0], b[0], c1, c2, c3);
    27 (0.0%)    r[0] = c1;
     9 (0.0%)    c1 = 0;
   162 (0.0%)    mul_add_c(a[0], b[1], c2, c3, c1);
   162 (0.0%)    mul_add_c(a[1], b[0], c2, c3, c1);
    36 (0.0%)    r[1] = c2;
     9 (0.0%)    c2 = 0;
   162 (0.0%)    mul_add_c(a[2], b[0], c3, c1, c2);
   171 (0.0%)    mul_add_c(a[1], b[1], c3, c1, c2);
   162 (0.0%)    mul_add_c(a[0], b[2], c3, c1, c2);
    36 (0.0%)    r[2] = c3;
     9 (0.0%)    c3 = 0;
   162 (0.0%)    mul_add_c(a[0], b[3], c1, c2, c3);
   171 (0.0%)    mul_add_c(a[1], b[2], c1, c2, c3);
   171 (0.0%)    mul_add_c(a[2], b[1], c1, c2, c3);
   162 (0.0%)    mul_add_c(a[3], b[0], c1, c2, c3);
    36 (0.0%)    r[3] = c1;
     9 (0.0%)    c1 = 0;
   162 (0.0%)    mul_add_c(a[4], b[0], c2, c3, c1);
   171 (0.0%)    mul_add_c(a[3], b[1], c2, c3, c1);
   171 (0.0%)    mul_add_c(a[2], b[2], c2, c3, c1);
   171 (0.0%)    mul_add_c(a[1], b[3], c2, c3, c1);
   162 (0.0%)    mul_add_c(a[0], b[4], c2, c3, c1);
    36 (0.0%)    r[4] = c2;
     9 (0.0%)    c2 = 0;
   162 (0.0%)    mul_add_c(a[0], b[5], c3, c1, c2);
   171 (0.0%)    mul_add_c(a[1], b[4], c3, c1, c2);
   171 (0.0%)    mul_add_c(a[2], b[3], c3, c1, c2);
   171 (0.0%)    mul_add_c(a[3], b[2], c3, c1, c2);
   171 (0.0%)    mul_add_c(a[4], b[1], c3, c1, c2);
   162 (0.0%)    mul_add_c(a[5], b[0], c3, c1, c2);
    36 (0.0%)    r[5] = c3;
     9 (0.0%)    c3 = 0;
   162 (0.0%)    mul_add_c(a[6], b[0], c1, c2, c3);
   171 (0.0%)    mul_add_c(a[5], b[1], c1, c2, c3);
   171 (0.0%)    mul_add_c(a[4], b[2], c1, c2, c3);
   171 (0.0%)    mul_add_c(a[3], b[3], c1, c2, c3);
   171 (0.0%)    mul_add_c(a[2], b[4], c1, c2, c3);
   171 (0.0%)    mul_add_c(a[1], b[5], c1, c2, c3);
   162 (0.0%)    mul_add_c(a[0], b[6], c1, c2, c3);
    36 (0.0%)    r[6] = c1;
     9 (0.0%)    c1 = 0;
   162 (0.0%)    mul_add_c(a[0], b[7], c2, c3, c1);
   171 (0.0%)    mul_add_c(a[1], b[6], c2, c3, c1);
   171 (0.0%)    mul_add_c(a[2], b[5], c2, c3, c1);
   171 (0.0%)    mul_add_c(a[3], b[4], c2, c3, c1);
   171 (0.0%)    mul_add_c(a[4], b[3], c2, c3, c1);
   171 (0.0%)    mul_add_c(a[5], b[2], c2, c3, c1);
   171 (0.0%)    mul_add_c(a[6], b[1], c2, c3, c1);
   162 (0.0%)    mul_add_c(a[7], b[0], c2, c3, c1);
    36 (0.0%)    r[7] = c2;
     9 (0.0%)    c2 = 0;
   171 (0.0%)    mul_add_c(a[7], b[1], c3, c1, c2);
   171 (0.0%)    mul_add_c(a[6], b[2], c3, c1, c2);
   171 (0.0%)    mul_add_c(a[5], b[3], c3, c1, c2);
   171 (0.0%)    mul_add_c(a[4], b[4], c3, c1, c2);
   171 (0.0%)    mul_add_c(a[3], b[5], c3, c1, c2);
   171 (0.0%)    mul_add_c(a[2], b[6], c3, c1, c2);
   171 (0.0%)    mul_add_c(a[1], b[7], c3, c1, c2);
    36 (0.0%)    r[8] = c3;
     9 (0.0%)    c3 = 0;
   171 (0.0%)    mul_add_c(a[2], b[7], c1, c2, c3);
   171 (0.0%)    mul_add_c(a[3], b[6], c1, c2, c3);
   171 (0.0%)    mul_add_c(a[4], b[5], c1, c2, c3);
   171 (0.0%)    mul_add_c(a[5], b[4], c1, c2, c3);
   171 (0.0%)    mul_add_c(a[6], b[3], c1, c2, c3);
   171 (0.0%)    mul_add_c(a[7], b[2], c1, c2, c3);
    36 (0.0%)    r[9] = c1;
     9 (0.0%)    c1 = 0;
   171 (0.0%)    mul_add_c(a[7], b[3], c2, c3, c1);
   171 (0.0%)    mul_add_c(a[6], b[4], c2, c3, c1);
   171 (0.0%)    mul_add_c(a[5], b[5], c2, c3, c1);
   171 (0.0%)    mul_add_c(a[4], b[6], c2, c3, c1);
   171 (0.0%)    mul_add_c(a[3], b[7], c2, c3, c1);
    36 (0.0%)    r[10] = c2;
     9 (0.0%)    c2 = 0;
   171 (0.0%)    mul_add_c(a[4], b[7], c3, c1, c2);
   171 (0.0%)    mul_add_c(a[5], b[6], c3, c1, c2);
   171 (0.0%)    mul_add_c(a[6], b[5], c3, c1, c2);
   171 (0.0%)    mul_add_c(a[7], b[4], c3, c1, c2);
    36 (0.0%)    r[11] = c3;
     9 (0.0%)    c3 = 0;
   171 (0.0%)    mul_add_c(a[7], b[5], c1, c2, c3);
   171 (0.0%)    mul_add_c(a[6], b[6], c1, c2, c3);
   171 (0.0%)    mul_add_c(a[5], b[7], c1, c2, c3);
    36 (0.0%)    r[12] = c1;
     9 (0.0%)    c1 = 0;
   171 (0.0%)    mul_add_c(a[6], b[7], c2, c3, c1);
   171 (0.0%)    mul_add_c(a[7], b[6], c2, c3, c1);
    36 (0.0%)    r[13] = c2;
     9 (0.0%)    c2 = 0;
   171 (0.0%)    mul_add_c(a[7], b[7], c3, c1, c2);
    36 (0.0%)    r[14] = c3;
    36 (0.0%)    r[15] = c1;
    27 (0.0%)  }
     .         
     .         void bn_mul_comba4(BN_ULONG r[8], const BN_ULONG a[4], const BN_ULONG b[4]) {
     .           BN_ULONG c1, c2, c3;
     .         
     .           c1 = 0;
     .           c2 = 0;
     .           c3 = 0;
     .           mul_add_c(a[0], b[0], c1, c2, c3);
-- line 395 ----------------------------------------

--------------------------------------------------------------------------------
-- Annotated source file: /home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S
--------------------------------------------------------------------------------
Ir__________ 

-- line 33 ----------------------------------------
    .         
    .         
    .         .globl	ecp_nistz256_neg
    .         .hidden ecp_nistz256_neg
    .         .type	ecp_nistz256_neg,@function
    .         .align	32
    .         ecp_nistz256_neg:
    .         .cfi_startproc	
  176 (0.0%)  	pushq	%r12
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r12,-16
  176 (0.0%)  	pushq	%r13
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r13,-24
    .         .Lneg_body:
    .         
  176 (0.0%)  	xorq	%r8,%r8
  176 (0.0%)  	xorq	%r9,%r9
  176 (0.0%)  	xorq	%r10,%r10
  176 (0.0%)  	xorq	%r11,%r11
  176 (0.0%)  	xorq	%r13,%r13
    .         
  176 (0.0%)  	subq	0(%rsi),%r8
  176 (0.0%)  	sbbq	8(%rsi),%r9
  176 (0.0%)  	sbbq	16(%rsi),%r10
  176 (0.0%)  	movq	%r8,%rax
  176 (0.0%)  	sbbq	24(%rsi),%r11
  176 (0.0%)  	leaq	.Lpoly(%rip),%rsi
  176 (0.0%)  	movq	%r9,%rdx
  176 (0.0%)  	sbbq	$0,%r13
    .         
  176 (0.0%)  	addq	0(%rsi),%r8
  176 (0.0%)  	movq	%r10,%rcx
  176 (0.0%)  	adcq	8(%rsi),%r9
  176 (0.0%)  	adcq	16(%rsi),%r10
  176 (0.0%)  	movq	%r11,%r12
  176 (0.0%)  	adcq	24(%rsi),%r11
  176 (0.0%)  	testq	%r13,%r13
    .         
  176 (0.0%)  	cmovzq	%rax,%r8
  176 (0.0%)  	cmovzq	%rdx,%r9
  176 (0.0%)  	movq	%r8,0(%rdi)
  176 (0.0%)  	cmovzq	%rcx,%r10
  176 (0.0%)  	movq	%r9,8(%rdi)
  176 (0.0%)  	cmovzq	%r12,%r11
  176 (0.0%)  	movq	%r10,16(%rdi)
  176 (0.0%)  	movq	%r11,24(%rdi)
    .         
  176 (0.0%)  	movq	0(%rsp),%r13
    .         .cfi_restore	%r13
  176 (0.0%)  	movq	8(%rsp),%r12
    .         .cfi_restore	%r12
  352 (0.0%)  	leaq	16(%rsp),%rsp
    .         .cfi_adjust_cfa_offset	-16
    .         .Lneg_epilogue:
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	ecp_nistz256_neg,.-ecp_nistz256_neg
    .         
    .         
    .         
-- line 93 ----------------------------------------
-- line 1188 ----------------------------------------
    .         
    .         
    .         .globl	ecp_nistz256_mul_mont
    .         .hidden ecp_nistz256_mul_mont
    .         .type	ecp_nistz256_mul_mont,@function
    .         .align	32
    .         ecp_nistz256_mul_mont:
    .         .cfi_startproc	
   52 (0.0%)  	leaq	OPENSSL_ia32cap_P(%rip),%rcx
   52 (0.0%)  	movq	8(%rcx),%rcx
   52 (0.0%)  	andl	$0x80100,%ecx
    .         .Lmul_mont:
   52 (0.0%)  	pushq	%rbp
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%rbp,-16
   52 (0.0%)  	pushq	%rbx
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%rbx,-24
   52 (0.0%)  	pushq	%r12
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r12,-32
   52 (0.0%)  	pushq	%r13
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r13,-40
   52 (0.0%)  	pushq	%r14
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r14,-48
   52 (0.0%)  	pushq	%r15
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r15,-56
    .         .Lmul_body:
   52 (0.0%)  	cmpl	$0x80100,%ecx
   52 (0.0%)  	je	.Lmul_montx
   52 (0.0%)  	movq	%rdx,%rbx
   52 (0.0%)  	movq	0(%rdx),%rax
   52 (0.0%)  	movq	0(%rsi),%r9
   52 (0.0%)  	movq	8(%rsi),%r10
   52 (0.0%)  	movq	16(%rsi),%r11
   52 (0.0%)  	movq	24(%rsi),%r12
    .         
   52 (0.0%)  	call	__ecp_nistz256_mul_montq
   52 (0.0%)  	jmp	.Lmul_mont_done
    .         
    .         .align	32
    .         .Lmul_montx:
    .         	movq	%rdx,%rbx
    .         	movq	0(%rdx),%rdx
    .         	movq	0(%rsi),%r9
    .         	movq	8(%rsi),%r10
    .         	movq	16(%rsi),%r11
    .         	movq	24(%rsi),%r12
    .         	leaq	-128(%rsi),%rsi
    .         
    .         	call	__ecp_nistz256_mul_montx
    .         .Lmul_mont_done:
   52 (0.0%)  	movq	0(%rsp),%r15
    .         .cfi_restore	%r15
   52 (0.0%)  	movq	8(%rsp),%r14
    .         .cfi_restore	%r14
   52 (0.0%)  	movq	16(%rsp),%r13
    .         .cfi_restore	%r13
   52 (0.0%)  	movq	24(%rsp),%r12
    .         .cfi_restore	%r12
   52 (0.0%)  	movq	32(%rsp),%rbx
    .         .cfi_restore	%rbx
   52 (0.0%)  	movq	40(%rsp),%rbp
    .         .cfi_restore	%rbp
  104 (0.0%)  	leaq	48(%rsp),%rsp
    .         .cfi_adjust_cfa_offset	-48
    .         .Lmul_epilogue:
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	ecp_nistz256_mul_mont,.-ecp_nistz256_mul_mont
    .         
    .         .type	__ecp_nistz256_mul_montq,@function
    .         .align	32
    .         __ecp_nistz256_mul_montq:
    .         .cfi_startproc	
    .         
    .         
4,124 (0.0%)  	movq	%rax,%rbp
4,124 (0.0%)  	mulq	%r9
4,124 (0.0%)  	movq	.Lpoly+8(%rip),%r14
4,124 (0.0%)  	movq	%rax,%r8
4,124 (0.0%)  	movq	%rbp,%rax
4,124 (0.0%)  	movq	%rdx,%r9
    .         
4,124 (0.0%)  	mulq	%r10
4,124 (0.0%)  	movq	.Lpoly+24(%rip),%r15
4,124 (0.0%)  	addq	%rax,%r9
4,124 (0.0%)  	movq	%rbp,%rax
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	movq	%rdx,%r10
    .         
4,124 (0.0%)  	mulq	%r11
4,124 (0.0%)  	addq	%rax,%r10
4,124 (0.0%)  	movq	%rbp,%rax
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	movq	%rdx,%r11
    .         
4,124 (0.0%)  	mulq	%r12
4,124 (0.0%)  	addq	%rax,%r11
4,124 (0.0%)  	movq	%r8,%rax
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	xorq	%r13,%r13
4,124 (0.0%)  	movq	%rdx,%r12
    .         
    .         
    .         
    .         
    .         
    .         
    .         
    .         
    .         
    .         
4,124 (0.0%)  	movq	%r8,%rbp
4,124 (0.0%)  	shlq	$32,%r8
4,124 (0.0%)  	mulq	%r15
4,124 (0.0%)  	shrq	$32,%rbp
4,124 (0.0%)  	addq	%r8,%r9
4,124 (0.0%)  	adcq	%rbp,%r10
4,124 (0.0%)  	adcq	%rax,%r11
4,124 (0.0%)  	movq	8(%rbx),%rax
4,124 (0.0%)  	adcq	%rdx,%r12
4,124 (0.0%)  	adcq	$0,%r13
4,124 (0.0%)  	xorq	%r8,%r8
    .         
    .         
    .         
4,124 (0.0%)  	movq	%rax,%rbp
4,124 (0.0%)  	mulq	0(%rsi)
4,124 (0.0%)  	addq	%rax,%r9
4,124 (0.0%)  	movq	%rbp,%rax
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	movq	%rdx,%rcx
    .         
4,124 (0.0%)  	mulq	8(%rsi)
4,124 (0.0%)  	addq	%rcx,%r10
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	addq	%rax,%r10
4,124 (0.0%)  	movq	%rbp,%rax
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	movq	%rdx,%rcx
    .         
4,124 (0.0%)  	mulq	16(%rsi)
4,124 (0.0%)  	addq	%rcx,%r11
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	addq	%rax,%r11
4,124 (0.0%)  	movq	%rbp,%rax
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	movq	%rdx,%rcx
    .         
4,124 (0.0%)  	mulq	24(%rsi)
4,124 (0.0%)  	addq	%rcx,%r12
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	addq	%rax,%r12
4,124 (0.0%)  	movq	%r9,%rax
4,124 (0.0%)  	adcq	%rdx,%r13
4,124 (0.0%)  	adcq	$0,%r8
    .         
    .         
    .         
4,124 (0.0%)  	movq	%r9,%rbp
4,124 (0.0%)  	shlq	$32,%r9
4,124 (0.0%)  	mulq	%r15
4,124 (0.0%)  	shrq	$32,%rbp
4,124 (0.0%)  	addq	%r9,%r10
4,124 (0.0%)  	adcq	%rbp,%r11
4,124 (0.0%)  	adcq	%rax,%r12
4,124 (0.0%)  	movq	16(%rbx),%rax
4,124 (0.0%)  	adcq	%rdx,%r13
4,124 (0.0%)  	adcq	$0,%r8
4,124 (0.0%)  	xorq	%r9,%r9
    .         
    .         
    .         
4,124 (0.0%)  	movq	%rax,%rbp
4,124 (0.0%)  	mulq	0(%rsi)
4,124 (0.0%)  	addq	%rax,%r10
4,124 (0.0%)  	movq	%rbp,%rax
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	movq	%rdx,%rcx
    .         
4,124 (0.0%)  	mulq	8(%rsi)
4,124 (0.0%)  	addq	%rcx,%r11
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	addq	%rax,%r11
4,124 (0.0%)  	movq	%rbp,%rax
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	movq	%rdx,%rcx
    .         
4,124 (0.0%)  	mulq	16(%rsi)
4,124 (0.0%)  	addq	%rcx,%r12
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	addq	%rax,%r12
4,124 (0.0%)  	movq	%rbp,%rax
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	movq	%rdx,%rcx
    .         
4,124 (0.0%)  	mulq	24(%rsi)
4,124 (0.0%)  	addq	%rcx,%r13
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	addq	%rax,%r13
4,124 (0.0%)  	movq	%r10,%rax
4,124 (0.0%)  	adcq	%rdx,%r8
4,124 (0.0%)  	adcq	$0,%r9
    .         
    .         
    .         
4,124 (0.0%)  	movq	%r10,%rbp
4,124 (0.0%)  	shlq	$32,%r10
4,124 (0.0%)  	mulq	%r15
4,124 (0.0%)  	shrq	$32,%rbp
4,124 (0.0%)  	addq	%r10,%r11
4,124 (0.0%)  	adcq	%rbp,%r12
4,124 (0.0%)  	adcq	%rax,%r13
4,124 (0.0%)  	movq	24(%rbx),%rax
4,124 (0.0%)  	adcq	%rdx,%r8
4,124 (0.0%)  	adcq	$0,%r9
4,124 (0.0%)  	xorq	%r10,%r10
    .         
    .         
    .         
4,124 (0.0%)  	movq	%rax,%rbp
4,124 (0.0%)  	mulq	0(%rsi)
4,124 (0.0%)  	addq	%rax,%r11
4,124 (0.0%)  	movq	%rbp,%rax
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	movq	%rdx,%rcx
    .         
4,124 (0.0%)  	mulq	8(%rsi)
4,124 (0.0%)  	addq	%rcx,%r12
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	addq	%rax,%r12
4,124 (0.0%)  	movq	%rbp,%rax
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	movq	%rdx,%rcx
    .         
4,124 (0.0%)  	mulq	16(%rsi)
4,124 (0.0%)  	addq	%rcx,%r13
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	addq	%rax,%r13
4,124 (0.0%)  	movq	%rbp,%rax
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	movq	%rdx,%rcx
    .         
4,124 (0.0%)  	mulq	24(%rsi)
4,124 (0.0%)  	addq	%rcx,%r8
4,124 (0.0%)  	adcq	$0,%rdx
4,124 (0.0%)  	addq	%rax,%r8
4,124 (0.0%)  	movq	%r11,%rax
4,124 (0.0%)  	adcq	%rdx,%r9
4,124 (0.0%)  	adcq	$0,%r10
    .         
    .         
    .         
4,124 (0.0%)  	movq	%r11,%rbp
4,124 (0.0%)  	shlq	$32,%r11
4,124 (0.0%)  	mulq	%r15
4,124 (0.0%)  	shrq	$32,%rbp
4,124 (0.0%)  	addq	%r11,%r12
4,124 (0.0%)  	adcq	%rbp,%r13
4,124 (0.0%)  	movq	%r12,%rcx
4,124 (0.0%)  	adcq	%rax,%r8
4,124 (0.0%)  	adcq	%rdx,%r9
4,124 (0.0%)  	movq	%r13,%rbp
4,124 (0.0%)  	adcq	$0,%r10
    .         
    .         
    .         
4,124 (0.0%)  	subq	$-1,%r12
4,124 (0.0%)  	movq	%r8,%rbx
4,124 (0.0%)  	sbbq	%r14,%r13
4,124 (0.0%)  	sbbq	$0,%r8
4,124 (0.0%)  	movq	%r9,%rdx
4,124 (0.0%)  	sbbq	%r15,%r9
4,124 (0.0%)  	sbbq	$0,%r10
    .         
4,124 (0.0%)  	cmovcq	%rcx,%r12
4,124 (0.0%)  	cmovcq	%rbp,%r13
4,124 (0.0%)  	movq	%r12,0(%rdi)
4,124 (0.0%)  	cmovcq	%rbx,%r8
4,124 (0.0%)  	movq	%r13,8(%rdi)
4,124 (0.0%)  	cmovcq	%rdx,%r9
4,124 (0.0%)  	movq	%r8,16(%rdi)
8,248 (0.0%)  	movq	%r9,24(%rdi)
    .         
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	__ecp_nistz256_mul_montq,.-__ecp_nistz256_mul_montq
    .         
    .         
    .         
    .         
-- line 1482 ----------------------------------------
-- line 1485 ----------------------------------------
    .         
    .         
    .         .globl	ecp_nistz256_sqr_mont
    .         .hidden ecp_nistz256_sqr_mont
    .         .type	ecp_nistz256_sqr_mont,@function
    .         .align	32
    .         ecp_nistz256_sqr_mont:
    .         .cfi_startproc	
1,022 (0.0%)  	leaq	OPENSSL_ia32cap_P(%rip),%rcx
1,022 (0.0%)  	movq	8(%rcx),%rcx
1,022 (0.0%)  	andl	$0x80100,%ecx
1,022 (0.0%)  	pushq	%rbp
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%rbp,-16
1,022 (0.0%)  	pushq	%rbx
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%rbx,-24
1,022 (0.0%)  	pushq	%r12
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r12,-32
1,022 (0.0%)  	pushq	%r13
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r13,-40
1,022 (0.0%)  	pushq	%r14
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r14,-48
1,022 (0.0%)  	pushq	%r15
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r15,-56
    .         .Lsqr_body:
1,022 (0.0%)  	cmpl	$0x80100,%ecx
1,022 (0.0%)  	je	.Lsqr_montx
1,022 (0.0%)  	movq	0(%rsi),%rax
1,022 (0.0%)  	movq	8(%rsi),%r14
1,022 (0.0%)  	movq	16(%rsi),%r15
1,022 (0.0%)  	movq	24(%rsi),%r8
    .         
1,022 (0.0%)  	call	__ecp_nistz256_sqr_montq
1,022 (0.0%)  	jmp	.Lsqr_mont_done
    .         
    .         .align	32
    .         .Lsqr_montx:
    .         	movq	0(%rsi),%rdx
    .         	movq	8(%rsi),%r14
    .         	movq	16(%rsi),%r15
    .         	movq	24(%rsi),%r8
    .         	leaq	-128(%rsi),%rsi
    .         
    .         	call	__ecp_nistz256_sqr_montx
    .         .Lsqr_mont_done:
1,022 (0.0%)  	movq	0(%rsp),%r15
    .         .cfi_restore	%r15
1,022 (0.0%)  	movq	8(%rsp),%r14
    .         .cfi_restore	%r14
1,022 (0.0%)  	movq	16(%rsp),%r13
    .         .cfi_restore	%r13
1,022 (0.0%)  	movq	24(%rsp),%r12
    .         .cfi_restore	%r12
1,022 (0.0%)  	movq	32(%rsp),%rbx
    .         .cfi_restore	%rbx
1,022 (0.0%)  	movq	40(%rsp),%rbp
    .         .cfi_restore	%rbp
2,044 (0.0%)  	leaq	48(%rsp),%rsp
    .         .cfi_adjust_cfa_offset	-48
    .         .Lsqr_epilogue:
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	ecp_nistz256_sqr_mont,.-ecp_nistz256_sqr_mont
    .         
    .         .type	__ecp_nistz256_sqr_montq,@function
    .         .align	32
    .         __ecp_nistz256_sqr_montq:
    .         .cfi_startproc	
3,806 (0.0%)  	movq	%rax,%r13
3,806 (0.0%)  	mulq	%r14
3,806 (0.0%)  	movq	%rax,%r9
3,806 (0.0%)  	movq	%r15,%rax
3,806 (0.0%)  	movq	%rdx,%r10
    .         
3,806 (0.0%)  	mulq	%r13
3,806 (0.0%)  	addq	%rax,%r10
3,806 (0.0%)  	movq	%r8,%rax
3,806 (0.0%)  	adcq	$0,%rdx
3,806 (0.0%)  	movq	%rdx,%r11
    .         
3,806 (0.0%)  	mulq	%r13
3,806 (0.0%)  	addq	%rax,%r11
3,806 (0.0%)  	movq	%r15,%rax
3,806 (0.0%)  	adcq	$0,%rdx
3,806 (0.0%)  	movq	%rdx,%r12
    .         
    .         
3,806 (0.0%)  	mulq	%r14
3,806 (0.0%)  	addq	%rax,%r11
3,806 (0.0%)  	movq	%r8,%rax
3,806 (0.0%)  	adcq	$0,%rdx
3,806 (0.0%)  	movq	%rdx,%rbp
    .         
3,806 (0.0%)  	mulq	%r14
3,806 (0.0%)  	addq	%rax,%r12
3,806 (0.0%)  	movq	%r8,%rax
3,806 (0.0%)  	adcq	$0,%rdx
3,806 (0.0%)  	addq	%rbp,%r12
3,806 (0.0%)  	movq	%rdx,%r13
3,806 (0.0%)  	adcq	$0,%r13
    .         
    .         
3,806 (0.0%)  	mulq	%r15
3,806 (0.0%)  	xorq	%r15,%r15
3,806 (0.0%)  	addq	%rax,%r13
3,806 (0.0%)  	movq	0(%rsi),%rax
3,806 (0.0%)  	movq	%rdx,%r14
3,806 (0.0%)  	adcq	$0,%r14
    .         
3,806 (0.0%)  	addq	%r9,%r9
3,806 (0.0%)  	adcq	%r10,%r10
3,806 (0.0%)  	adcq	%r11,%r11
3,806 (0.0%)  	adcq	%r12,%r12
3,806 (0.0%)  	adcq	%r13,%r13
3,806 (0.0%)  	adcq	%r14,%r14
3,806 (0.0%)  	adcq	$0,%r15
    .         
3,806 (0.0%)  	mulq	%rax
3,806 (0.0%)  	movq	%rax,%r8
3,806 (0.0%)  	movq	8(%rsi),%rax
3,806 (0.0%)  	movq	%rdx,%rcx
    .         
3,806 (0.0%)  	mulq	%rax
3,806 (0.0%)  	addq	%rcx,%r9
3,806 (0.0%)  	adcq	%rax,%r10
3,806 (0.0%)  	movq	16(%rsi),%rax
3,806 (0.0%)  	adcq	$0,%rdx
3,806 (0.0%)  	movq	%rdx,%rcx
    .         
3,806 (0.0%)  	mulq	%rax
3,806 (0.0%)  	addq	%rcx,%r11
3,806 (0.0%)  	adcq	%rax,%r12
3,806 (0.0%)  	movq	24(%rsi),%rax
3,806 (0.0%)  	adcq	$0,%rdx
3,806 (0.0%)  	movq	%rdx,%rcx
    .         
3,806 (0.0%)  	mulq	%rax
3,806 (0.0%)  	addq	%rcx,%r13
3,806 (0.0%)  	adcq	%rax,%r14
3,806 (0.0%)  	movq	%r8,%rax
3,806 (0.0%)  	adcq	%rdx,%r15
    .         
3,806 (0.0%)  	movq	.Lpoly+8(%rip),%rsi
3,806 (0.0%)  	movq	.Lpoly+24(%rip),%rbp
    .         
    .         
    .         
    .         
3,806 (0.0%)  	movq	%r8,%rcx
3,806 (0.0%)  	shlq	$32,%r8
3,806 (0.0%)  	mulq	%rbp
3,806 (0.0%)  	shrq	$32,%rcx
3,806 (0.0%)  	addq	%r8,%r9
3,806 (0.0%)  	adcq	%rcx,%r10
3,806 (0.0%)  	adcq	%rax,%r11
3,806 (0.0%)  	movq	%r9,%rax
3,806 (0.0%)  	adcq	$0,%rdx
    .         
    .         
    .         
3,806 (0.0%)  	movq	%r9,%rcx
3,806 (0.0%)  	shlq	$32,%r9
3,806 (0.0%)  	movq	%rdx,%r8
3,806 (0.0%)  	mulq	%rbp
3,806 (0.0%)  	shrq	$32,%rcx
3,806 (0.0%)  	addq	%r9,%r10
3,806 (0.0%)  	adcq	%rcx,%r11
3,806 (0.0%)  	adcq	%rax,%r8
3,806 (0.0%)  	movq	%r10,%rax
3,806 (0.0%)  	adcq	$0,%rdx
    .         
    .         
    .         
3,806 (0.0%)  	movq	%r10,%rcx
3,806 (0.0%)  	shlq	$32,%r10
3,806 (0.0%)  	movq	%rdx,%r9
3,806 (0.0%)  	mulq	%rbp
3,806 (0.0%)  	shrq	$32,%rcx
3,806 (0.0%)  	addq	%r10,%r11
3,806 (0.0%)  	adcq	%rcx,%r8
3,806 (0.0%)  	adcq	%rax,%r9
3,806 (0.0%)  	movq	%r11,%rax
3,806 (0.0%)  	adcq	$0,%rdx
    .         
    .         
    .         
3,806 (0.0%)  	movq	%r11,%rcx
3,806 (0.0%)  	shlq	$32,%r11
3,806 (0.0%)  	movq	%rdx,%r10
3,806 (0.0%)  	mulq	%rbp
3,806 (0.0%)  	shrq	$32,%rcx
3,806 (0.0%)  	addq	%r11,%r8
3,806 (0.0%)  	adcq	%rcx,%r9
3,806 (0.0%)  	adcq	%rax,%r10
3,806 (0.0%)  	adcq	$0,%rdx
3,806 (0.0%)  	xorq	%r11,%r11
    .         
    .         
    .         
3,806 (0.0%)  	addq	%r8,%r12
3,806 (0.0%)  	adcq	%r9,%r13
3,806 (0.0%)  	movq	%r12,%r8
3,806 (0.0%)  	adcq	%r10,%r14
3,806 (0.0%)  	adcq	%rdx,%r15
3,806 (0.0%)  	movq	%r13,%r9
3,806 (0.0%)  	adcq	$0,%r11
    .         
3,806 (0.0%)  	subq	$-1,%r12
3,806 (0.0%)  	movq	%r14,%r10
3,806 (0.0%)  	sbbq	%rsi,%r13
3,806 (0.0%)  	sbbq	$0,%r14
3,806 (0.0%)  	movq	%r15,%rcx
3,806 (0.0%)  	sbbq	%rbp,%r15
3,806 (0.0%)  	sbbq	$0,%r11
    .         
3,806 (0.0%)  	cmovcq	%r8,%r12
3,806 (0.0%)  	cmovcq	%r9,%r13
3,806 (0.0%)  	movq	%r12,0(%rdi)
3,806 (0.0%)  	cmovcq	%r10,%r14
3,806 (0.0%)  	movq	%r13,8(%rdi)
3,806 (0.0%)  	cmovcq	%rcx,%r15
3,806 (0.0%)  	movq	%r14,16(%rdi)
7,612 (0.0%)  	movq	%r15,24(%rdi)
    .         
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	__ecp_nistz256_sqr_montq,.-__ecp_nistz256_sqr_montq
    .         .type	__ecp_nistz256_mul_montx,@function
    .         .align	32
    .         __ecp_nistz256_mul_montx:
    .         .cfi_startproc	
-- line 1720 ----------------------------------------
-- line 2015 ----------------------------------------
    .         
    .         
    .         .globl	ecp_nistz256_select_w5
    .         .hidden ecp_nistz256_select_w5
    .         .type	ecp_nistz256_select_w5,@function
    .         .align	32
    .         ecp_nistz256_select_w5:
    .         .cfi_startproc	
  104 (0.0%)  	leaq	OPENSSL_ia32cap_P(%rip),%rax
  104 (0.0%)  	movq	8(%rax),%rax
  104 (0.0%)  	testl	$32,%eax
  104 (0.0%)  	jnz	.Lavx2_select_w5
    .         	movdqa	.LOne(%rip),%xmm0
    .         	movd	%edx,%xmm1
    .         
    .         	pxor	%xmm2,%xmm2
    .         	pxor	%xmm3,%xmm3
    .         	pxor	%xmm4,%xmm4
    .         	pxor	%xmm5,%xmm5
    .         	pxor	%xmm6,%xmm6
-- line 2034 ----------------------------------------
-- line 2082 ----------------------------------------
    .         
    .         
    .         .globl	ecp_nistz256_select_w7
    .         .hidden ecp_nistz256_select_w7
    .         .type	ecp_nistz256_select_w7,@function
    .         .align	32
    .         ecp_nistz256_select_w7:
    .         .cfi_startproc	
   74 (0.0%)  	leaq	OPENSSL_ia32cap_P(%rip),%rax
   74 (0.0%)  	movq	8(%rax),%rax
   74 (0.0%)  	testl	$32,%eax
   74 (0.0%)  	jnz	.Lavx2_select_w7
    .         	movdqa	.LOne(%rip),%xmm8
    .         	movd	%edx,%xmm1
    .         
    .         	pxor	%xmm2,%xmm2
    .         	pxor	%xmm3,%xmm3
    .         	pxor	%xmm4,%xmm4
    .         	pxor	%xmm5,%xmm5
    .         
-- line 2101 ----------------------------------------
-- line 2136 ----------------------------------------
    .         .size	ecp_nistz256_select_w7,.-ecp_nistz256_select_w7
    .         
    .         
    .         .type	ecp_nistz256_avx2_select_w5,@function
    .         .align	32
    .         ecp_nistz256_avx2_select_w5:
    .         .cfi_startproc	
    .         .Lavx2_select_w5:
  104 (0.0%)  	vzeroupper
  104 (0.0%)  	vmovdqa	.LTwo(%rip),%ymm0
    .         
  104 (0.0%)  	vpxor	%ymm2,%ymm2,%ymm2
  104 (0.0%)  	vpxor	%ymm3,%ymm3,%ymm3
  104 (0.0%)  	vpxor	%ymm4,%ymm4,%ymm4
    .         
  104 (0.0%)  	vmovdqa	.LOne(%rip),%ymm5
  104 (0.0%)  	vmovdqa	.LTwo(%rip),%ymm10
    .         
  104 (0.0%)  	vmovd	%edx,%xmm1
  104 (0.0%)  	vpermd	%ymm1,%ymm2,%ymm1
    .         
  104 (0.0%)  	movq	$8,%rax
    .         .Lselect_loop_avx2_w5:
    .         
  832 (0.0%)  	vmovdqa	0(%rsi),%ymm6
  832 (0.0%)  	vmovdqa	32(%rsi),%ymm7
  832 (0.0%)  	vmovdqa	64(%rsi),%ymm8
    .         
  832 (0.0%)  	vmovdqa	96(%rsi),%ymm11
  832 (0.0%)  	vmovdqa	128(%rsi),%ymm12
  832 (0.0%)  	vmovdqa	160(%rsi),%ymm13
    .         
  832 (0.0%)  	vpcmpeqd	%ymm1,%ymm5,%ymm9
  832 (0.0%)  	vpcmpeqd	%ymm1,%ymm10,%ymm14
    .         
  832 (0.0%)  	vpaddd	%ymm0,%ymm5,%ymm5
  832 (0.0%)  	vpaddd	%ymm0,%ymm10,%ymm10
  832 (0.0%)  	leaq	192(%rsi),%rsi
    .         
  832 (0.0%)  	vpand	%ymm9,%ymm6,%ymm6
  832 (0.0%)  	vpand	%ymm9,%ymm7,%ymm7
  832 (0.0%)  	vpand	%ymm9,%ymm8,%ymm8
  832 (0.0%)  	vpand	%ymm14,%ymm11,%ymm11
  832 (0.0%)  	vpand	%ymm14,%ymm12,%ymm12
  832 (0.0%)  	vpand	%ymm14,%ymm13,%ymm13
    .         
  832 (0.0%)  	vpxor	%ymm6,%ymm2,%ymm2
  832 (0.0%)  	vpxor	%ymm7,%ymm3,%ymm3
  832 (0.0%)  	vpxor	%ymm8,%ymm4,%ymm4
  832 (0.0%)  	vpxor	%ymm11,%ymm2,%ymm2
  832 (0.0%)  	vpxor	%ymm12,%ymm3,%ymm3
  832 (0.0%)  	vpxor	%ymm13,%ymm4,%ymm4
    .         
  832 (0.0%)  	decq	%rax
  832 (0.0%)  	jnz	.Lselect_loop_avx2_w5
    .         
  104 (0.0%)  	vmovdqu	%ymm2,0(%rdi)
  104 (0.0%)  	vmovdqu	%ymm3,32(%rdi)
  104 (0.0%)  	vmovdqu	%ymm4,64(%rdi)
  208 (0.0%)  	vzeroupper
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .LSEH_end_ecp_nistz256_avx2_select_w5:
    .         .size	ecp_nistz256_avx2_select_w5,.-ecp_nistz256_avx2_select_w5
    .         
    .         
    .         
    .         .globl	ecp_nistz256_avx2_select_w7
    .         .hidden ecp_nistz256_avx2_select_w7
    .         .type	ecp_nistz256_avx2_select_w7,@function
    .         .align	32
    .         ecp_nistz256_avx2_select_w7:
    .         .cfi_startproc	
    .         .Lavx2_select_w7:
   74 (0.0%)  	vzeroupper
   74 (0.0%)  	vmovdqa	.LThree(%rip),%ymm0
    .         
   74 (0.0%)  	vpxor	%ymm2,%ymm2,%ymm2
   74 (0.0%)  	vpxor	%ymm3,%ymm3,%ymm3
    .         
   74 (0.0%)  	vmovdqa	.LOne(%rip),%ymm4
   74 (0.0%)  	vmovdqa	.LTwo(%rip),%ymm8
   74 (0.0%)  	vmovdqa	.LThree(%rip),%ymm12
    .         
   74 (0.0%)  	vmovd	%edx,%xmm1
   74 (0.0%)  	vpermd	%ymm1,%ymm2,%ymm1
    .         
    .         
   74 (0.0%)  	movq	$21,%rax
    .         .Lselect_loop_avx2_w7:
    .         
1,554 (0.0%)  	vmovdqa	0(%rsi),%ymm5
1,554 (0.0%)  	vmovdqa	32(%rsi),%ymm6
    .         
1,554 (0.0%)  	vmovdqa	64(%rsi),%ymm9
1,554 (0.0%)  	vmovdqa	96(%rsi),%ymm10
    .         
1,554 (0.0%)  	vmovdqa	128(%rsi),%ymm13
1,554 (0.0%)  	vmovdqa	160(%rsi),%ymm14
    .         
1,554 (0.0%)  	vpcmpeqd	%ymm1,%ymm4,%ymm7
1,554 (0.0%)  	vpcmpeqd	%ymm1,%ymm8,%ymm11
1,554 (0.0%)  	vpcmpeqd	%ymm1,%ymm12,%ymm15
    .         
1,554 (0.0%)  	vpaddd	%ymm0,%ymm4,%ymm4
1,554 (0.0%)  	vpaddd	%ymm0,%ymm8,%ymm8
1,554 (0.0%)  	vpaddd	%ymm0,%ymm12,%ymm12
1,554 (0.0%)  	leaq	192(%rsi),%rsi
    .         
1,554 (0.0%)  	vpand	%ymm7,%ymm5,%ymm5
1,554 (0.0%)  	vpand	%ymm7,%ymm6,%ymm6
1,554 (0.0%)  	vpand	%ymm11,%ymm9,%ymm9
1,554 (0.0%)  	vpand	%ymm11,%ymm10,%ymm10
1,554 (0.0%)  	vpand	%ymm15,%ymm13,%ymm13
1,554 (0.0%)  	vpand	%ymm15,%ymm14,%ymm14
    .         
1,554 (0.0%)  	vpxor	%ymm5,%ymm2,%ymm2
1,554 (0.0%)  	vpxor	%ymm6,%ymm3,%ymm3
1,554 (0.0%)  	vpxor	%ymm9,%ymm2,%ymm2
1,554 (0.0%)  	vpxor	%ymm10,%ymm3,%ymm3
1,554 (0.0%)  	vpxor	%ymm13,%ymm2,%ymm2
1,554 (0.0%)  	vpxor	%ymm14,%ymm3,%ymm3
    .         
1,554 (0.0%)  	decq	%rax
1,554 (0.0%)  	jnz	.Lselect_loop_avx2_w7
    .         
    .         
   74 (0.0%)  	vmovdqa	0(%rsi),%ymm5
   74 (0.0%)  	vmovdqa	32(%rsi),%ymm6
    .         
   74 (0.0%)  	vpcmpeqd	%ymm1,%ymm4,%ymm7
    .         
   74 (0.0%)  	vpand	%ymm7,%ymm5,%ymm5
   74 (0.0%)  	vpand	%ymm7,%ymm6,%ymm6
    .         
   74 (0.0%)  	vpxor	%ymm5,%ymm2,%ymm2
   74 (0.0%)  	vpxor	%ymm6,%ymm3,%ymm3
    .         
   74 (0.0%)  	vmovdqu	%ymm2,0(%rdi)
   74 (0.0%)  	vmovdqu	%ymm3,32(%rdi)
  148 (0.0%)  	vzeroupper
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .LSEH_end_ecp_nistz256_avx2_select_w7:
    .         .size	ecp_nistz256_avx2_select_w7,.-ecp_nistz256_avx2_select_w7
    .         .type	__ecp_nistz256_add_toq,@function
    .         .align	32
    .         __ecp_nistz256_add_toq:
    .         .cfi_startproc	
1,052 (0.0%)  	xorq	%r11,%r11
1,052 (0.0%)  	addq	0(%rbx),%r12
1,052 (0.0%)  	adcq	8(%rbx),%r13
1,052 (0.0%)  	movq	%r12,%rax
1,052 (0.0%)  	adcq	16(%rbx),%r8
1,052 (0.0%)  	adcq	24(%rbx),%r9
1,052 (0.0%)  	movq	%r13,%rbp
1,052 (0.0%)  	adcq	$0,%r11
    .         
1,052 (0.0%)  	subq	$-1,%r12
1,052 (0.0%)  	movq	%r8,%rcx
1,052 (0.0%)  	sbbq	%r14,%r13
1,052 (0.0%)  	sbbq	$0,%r8
1,052 (0.0%)  	movq	%r9,%r10
1,052 (0.0%)  	sbbq	%r15,%r9
1,052 (0.0%)  	sbbq	$0,%r11
    .         
1,052 (0.0%)  	cmovcq	%rax,%r12
1,052 (0.0%)  	cmovcq	%rbp,%r13
1,052 (0.0%)  	movq	%r12,0(%rdi)
1,052 (0.0%)  	cmovcq	%rcx,%r8
1,052 (0.0%)  	movq	%r13,8(%rdi)
1,052 (0.0%)  	cmovcq	%r10,%r9
1,052 (0.0%)  	movq	%r8,16(%rdi)
2,104 (0.0%)  	movq	%r9,24(%rdi)
    .         
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	__ecp_nistz256_add_toq,.-__ecp_nistz256_add_toq
    .         
    .         .type	__ecp_nistz256_sub_fromq,@function
    .         .align	32
    .         __ecp_nistz256_sub_fromq:
    .         .cfi_startproc	
2,330 (0.0%)  	subq	0(%rbx),%r12
2,330 (0.0%)  	sbbq	8(%rbx),%r13
2,330 (0.0%)  	movq	%r12,%rax
2,330 (0.0%)  	sbbq	16(%rbx),%r8
2,330 (0.0%)  	sbbq	24(%rbx),%r9
2,330 (0.0%)  	movq	%r13,%rbp
2,330 (0.0%)  	sbbq	%r11,%r11
    .         
2,330 (0.0%)  	addq	$-1,%r12
2,330 (0.0%)  	movq	%r8,%rcx
2,330 (0.0%)  	adcq	%r14,%r13
2,330 (0.0%)  	adcq	$0,%r8
2,330 (0.0%)  	movq	%r9,%r10
2,330 (0.0%)  	adcq	%r15,%r9
2,330 (0.0%)  	testq	%r11,%r11
    .         
2,330 (0.0%)  	cmovzq	%rax,%r12
2,330 (0.0%)  	cmovzq	%rbp,%r13
2,330 (0.0%)  	movq	%r12,0(%rdi)
2,330 (0.0%)  	cmovzq	%rcx,%r8
2,330 (0.0%)  	movq	%r13,8(%rdi)
2,330 (0.0%)  	cmovzq	%r10,%r9
2,330 (0.0%)  	movq	%r8,16(%rdi)
4,660 (0.0%)  	movq	%r9,24(%rdi)
    .         
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	__ecp_nistz256_sub_fromq,.-__ecp_nistz256_sub_fromq
    .         
    .         .type	__ecp_nistz256_subq,@function
    .         .align	32
    .         __ecp_nistz256_subq:
    .         .cfi_startproc	
  902 (0.0%)  	subq	%r12,%rax
  902 (0.0%)  	sbbq	%r13,%rbp
  902 (0.0%)  	movq	%rax,%r12
  902 (0.0%)  	sbbq	%r8,%rcx
  902 (0.0%)  	sbbq	%r9,%r10
  902 (0.0%)  	movq	%rbp,%r13
  902 (0.0%)  	sbbq	%r11,%r11
    .         
  902 (0.0%)  	addq	$-1,%rax
  902 (0.0%)  	movq	%rcx,%r8
  902 (0.0%)  	adcq	%r14,%rbp
  902 (0.0%)  	adcq	$0,%rcx
  902 (0.0%)  	movq	%r10,%r9
  902 (0.0%)  	adcq	%r15,%r10
  902 (0.0%)  	testq	%r11,%r11
    .         
  902 (0.0%)  	cmovnzq	%rax,%r12
  902 (0.0%)  	cmovnzq	%rbp,%r13
  902 (0.0%)  	cmovnzq	%rcx,%r8
1,804 (0.0%)  	cmovnzq	%r10,%r9
    .         
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	__ecp_nistz256_subq,.-__ecp_nistz256_subq
    .         
    .         .type	__ecp_nistz256_mul_by_2q,@function
    .         .align	32
    .         __ecp_nistz256_mul_by_2q:
    .         .cfi_startproc	
2,104 (0.0%)  	xorq	%r11,%r11
2,104 (0.0%)  	addq	%r12,%r12
2,104 (0.0%)  	adcq	%r13,%r13
2,104 (0.0%)  	movq	%r12,%rax
2,104 (0.0%)  	adcq	%r8,%r8
2,104 (0.0%)  	adcq	%r9,%r9
2,104 (0.0%)  	movq	%r13,%rbp
2,104 (0.0%)  	adcq	$0,%r11
    .         
2,104 (0.0%)  	subq	$-1,%r12
2,104 (0.0%)  	movq	%r8,%rcx
2,104 (0.0%)  	sbbq	%r14,%r13
2,104 (0.0%)  	sbbq	$0,%r8
2,104 (0.0%)  	movq	%r9,%r10
2,104 (0.0%)  	sbbq	%r15,%r9
2,104 (0.0%)  	sbbq	$0,%r11
    .         
2,104 (0.0%)  	cmovcq	%rax,%r12
2,104 (0.0%)  	cmovcq	%rbp,%r13
2,104 (0.0%)  	movq	%r12,0(%rdi)
2,104 (0.0%)  	cmovcq	%rcx,%r8
2,104 (0.0%)  	movq	%r13,8(%rdi)
2,104 (0.0%)  	cmovcq	%r10,%r9
2,104 (0.0%)  	movq	%r8,16(%rdi)
4,208 (0.0%)  	movq	%r9,24(%rdi)
    .         
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	__ecp_nistz256_mul_by_2q,.-__ecp_nistz256_mul_by_2q
    .         .globl	ecp_nistz256_point_double
    .         .hidden ecp_nistz256_point_double
    .         .type	ecp_nistz256_point_double,@function
    .         .align	32
    .         ecp_nistz256_point_double:
    .         .cfi_startproc	
  526 (0.0%)  	leaq	OPENSSL_ia32cap_P(%rip),%rcx
  526 (0.0%)  	movq	8(%rcx),%rcx
  526 (0.0%)  	andl	$0x80100,%ecx
  526 (0.0%)  	cmpl	$0x80100,%ecx
  526 (0.0%)  	je	.Lpoint_doublex
  526 (0.0%)  	pushq	%rbp
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%rbp,-16
  526 (0.0%)  	pushq	%rbx
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%rbx,-24
  526 (0.0%)  	pushq	%r12
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r12,-32
  526 (0.0%)  	pushq	%r13
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r13,-40
  526 (0.0%)  	pushq	%r14
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r14,-48
  526 (0.0%)  	pushq	%r15
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r15,-56
  526 (0.0%)  	subq	$160+8,%rsp
    .         .cfi_adjust_cfa_offset	32*5+8
    .         .Lpoint_doubleq_body:
    .         
    .         .Lpoint_double_shortcutq:
  526 (0.0%)  	movdqu	0(%rsi),%xmm0
  526 (0.0%)  	movq	%rsi,%rbx
  526 (0.0%)  	movdqu	16(%rsi),%xmm1
  526 (0.0%)  	movq	32+0(%rsi),%r12
  526 (0.0%)  	movq	32+8(%rsi),%r13
  526 (0.0%)  	movq	32+16(%rsi),%r8
  526 (0.0%)  	movq	32+24(%rsi),%r9
  526 (0.0%)  	movq	.Lpoly+8(%rip),%r14
  526 (0.0%)  	movq	.Lpoly+24(%rip),%r15
  526 (0.0%)  	movdqa	%xmm0,96(%rsp)
  526 (0.0%)  	movdqa	%xmm1,96+16(%rsp)
  526 (0.0%)  	leaq	32(%rdi),%r10
2,104 (0.0%)  	leaq	64(%rdi),%r11
    .         .byte	102,72,15,110,199
    .         .byte	102,73,15,110,202
    .         .byte	102,73,15,110,211
    .         
  526 (0.0%)  	leaq	0(%rsp),%rdi
  526 (0.0%)  	call	__ecp_nistz256_mul_by_2q
    .         
  526 (0.0%)  	movq	64+0(%rsi),%rax
  526 (0.0%)  	movq	64+8(%rsi),%r14
  526 (0.0%)  	movq	64+16(%rsi),%r15
  526 (0.0%)  	movq	64+24(%rsi),%r8
  526 (0.0%)  	leaq	64-0(%rsi),%rsi
  526 (0.0%)  	leaq	64(%rsp),%rdi
  526 (0.0%)  	call	__ecp_nistz256_sqr_montq
    .         
  526 (0.0%)  	movq	0+0(%rsp),%rax
  526 (0.0%)  	movq	8+0(%rsp),%r14
  526 (0.0%)  	leaq	0+0(%rsp),%rsi
  526 (0.0%)  	movq	16+0(%rsp),%r15
  526 (0.0%)  	movq	24+0(%rsp),%r8
  526 (0.0%)  	leaq	0(%rsp),%rdi
  526 (0.0%)  	call	__ecp_nistz256_sqr_montq
    .         
  526 (0.0%)  	movq	32(%rbx),%rax
  526 (0.0%)  	movq	64+0(%rbx),%r9
  526 (0.0%)  	movq	64+8(%rbx),%r10
  526 (0.0%)  	movq	64+16(%rbx),%r11
  526 (0.0%)  	movq	64+24(%rbx),%r12
  526 (0.0%)  	leaq	64-0(%rbx),%rsi
1,052 (0.0%)  	leaq	32(%rbx),%rbx
    .         .byte	102,72,15,126,215
  526 (0.0%)  	call	__ecp_nistz256_mul_montq
  526 (0.0%)  	call	__ecp_nistz256_mul_by_2q
    .         
  526 (0.0%)  	movq	96+0(%rsp),%r12
  526 (0.0%)  	movq	96+8(%rsp),%r13
  526 (0.0%)  	leaq	64(%rsp),%rbx
  526 (0.0%)  	movq	96+16(%rsp),%r8
  526 (0.0%)  	movq	96+24(%rsp),%r9
  526 (0.0%)  	leaq	32(%rsp),%rdi
  526 (0.0%)  	call	__ecp_nistz256_add_toq
    .         
  526 (0.0%)  	movq	96+0(%rsp),%r12
  526 (0.0%)  	movq	96+8(%rsp),%r13
  526 (0.0%)  	leaq	64(%rsp),%rbx
  526 (0.0%)  	movq	96+16(%rsp),%r8
  526 (0.0%)  	movq	96+24(%rsp),%r9
  526 (0.0%)  	leaq	64(%rsp),%rdi
  526 (0.0%)  	call	__ecp_nistz256_sub_fromq
    .         
  526 (0.0%)  	movq	0+0(%rsp),%rax
  526 (0.0%)  	movq	8+0(%rsp),%r14
  526 (0.0%)  	leaq	0+0(%rsp),%rsi
  526 (0.0%)  	movq	16+0(%rsp),%r15
1,052 (0.0%)  	movq	24+0(%rsp),%r8
    .         .byte	102,72,15,126,207
  526 (0.0%)  	call	__ecp_nistz256_sqr_montq
  526 (0.0%)  	xorq	%r9,%r9
  526 (0.0%)  	movq	%r12,%rax
  526 (0.0%)  	addq	$-1,%r12
  526 (0.0%)  	movq	%r13,%r10
  526 (0.0%)  	adcq	%rsi,%r13
  526 (0.0%)  	movq	%r14,%rcx
  526 (0.0%)  	adcq	$0,%r14
  526 (0.0%)  	movq	%r15,%r8
  526 (0.0%)  	adcq	%rbp,%r15
  526 (0.0%)  	adcq	$0,%r9
  526 (0.0%)  	xorq	%rsi,%rsi
  526 (0.0%)  	testq	$1,%rax
    .         
  526 (0.0%)  	cmovzq	%rax,%r12
  526 (0.0%)  	cmovzq	%r10,%r13
  526 (0.0%)  	cmovzq	%rcx,%r14
  526 (0.0%)  	cmovzq	%r8,%r15
  526 (0.0%)  	cmovzq	%rsi,%r9
    .         
  526 (0.0%)  	movq	%r13,%rax
  526 (0.0%)  	shrq	$1,%r12
  526 (0.0%)  	shlq	$63,%rax
  526 (0.0%)  	movq	%r14,%r10
  526 (0.0%)  	shrq	$1,%r13
  526 (0.0%)  	orq	%rax,%r12
  526 (0.0%)  	shlq	$63,%r10
  526 (0.0%)  	movq	%r15,%rcx
  526 (0.0%)  	shrq	$1,%r14
  526 (0.0%)  	orq	%r10,%r13
  526 (0.0%)  	shlq	$63,%rcx
  526 (0.0%)  	movq	%r12,0(%rdi)
  526 (0.0%)  	shrq	$1,%r15
  526 (0.0%)  	movq	%r13,8(%rdi)
  526 (0.0%)  	shlq	$63,%r9
  526 (0.0%)  	orq	%rcx,%r14
  526 (0.0%)  	orq	%r9,%r15
  526 (0.0%)  	movq	%r14,16(%rdi)
  526 (0.0%)  	movq	%r15,24(%rdi)
  526 (0.0%)  	movq	64(%rsp),%rax
  526 (0.0%)  	leaq	64(%rsp),%rbx
  526 (0.0%)  	movq	0+32(%rsp),%r9
  526 (0.0%)  	movq	8+32(%rsp),%r10
  526 (0.0%)  	leaq	0+32(%rsp),%rsi
  526 (0.0%)  	movq	16+32(%rsp),%r11
  526 (0.0%)  	movq	24+32(%rsp),%r12
  526 (0.0%)  	leaq	32(%rsp),%rdi
  526 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
  526 (0.0%)  	leaq	128(%rsp),%rdi
  526 (0.0%)  	call	__ecp_nistz256_mul_by_2q
    .         
  526 (0.0%)  	leaq	32(%rsp),%rbx
  526 (0.0%)  	leaq	32(%rsp),%rdi
  526 (0.0%)  	call	__ecp_nistz256_add_toq
    .         
  526 (0.0%)  	movq	96(%rsp),%rax
  526 (0.0%)  	leaq	96(%rsp),%rbx
  526 (0.0%)  	movq	0+0(%rsp),%r9
  526 (0.0%)  	movq	8+0(%rsp),%r10
  526 (0.0%)  	leaq	0+0(%rsp),%rsi
  526 (0.0%)  	movq	16+0(%rsp),%r11
  526 (0.0%)  	movq	24+0(%rsp),%r12
  526 (0.0%)  	leaq	0(%rsp),%rdi
  526 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
  526 (0.0%)  	leaq	128(%rsp),%rdi
  526 (0.0%)  	call	__ecp_nistz256_mul_by_2q
    .         
  526 (0.0%)  	movq	0+32(%rsp),%rax
  526 (0.0%)  	movq	8+32(%rsp),%r14
  526 (0.0%)  	leaq	0+32(%rsp),%rsi
  526 (0.0%)  	movq	16+32(%rsp),%r15
1,052 (0.0%)  	movq	24+32(%rsp),%r8
    .         .byte	102,72,15,126,199
  526 (0.0%)  	call	__ecp_nistz256_sqr_montq
    .         
  526 (0.0%)  	leaq	128(%rsp),%rbx
  526 (0.0%)  	movq	%r14,%r8
  526 (0.0%)  	movq	%r15,%r9
  526 (0.0%)  	movq	%rsi,%r14
  526 (0.0%)  	movq	%rbp,%r15
  526 (0.0%)  	call	__ecp_nistz256_sub_fromq
    .         
  526 (0.0%)  	movq	0+0(%rsp),%rax
  526 (0.0%)  	movq	0+8(%rsp),%rbp
  526 (0.0%)  	movq	0+16(%rsp),%rcx
  526 (0.0%)  	movq	0+24(%rsp),%r10
  526 (0.0%)  	leaq	0(%rsp),%rdi
  526 (0.0%)  	call	__ecp_nistz256_subq
    .         
  526 (0.0%)  	movq	32(%rsp),%rax
  526 (0.0%)  	leaq	32(%rsp),%rbx
  526 (0.0%)  	movq	%r12,%r14
  526 (0.0%)  	xorl	%ecx,%ecx
  526 (0.0%)  	movq	%r12,0+0(%rsp)
  526 (0.0%)  	movq	%r13,%r10
  526 (0.0%)  	movq	%r13,0+8(%rsp)
  526 (0.0%)  	cmovzq	%r8,%r11
  526 (0.0%)  	movq	%r8,0+16(%rsp)
  526 (0.0%)  	leaq	0-0(%rsp),%rsi
  526 (0.0%)  	cmovzq	%r9,%r12
  526 (0.0%)  	movq	%r9,0+24(%rsp)
  526 (0.0%)  	movq	%r14,%r9
  526 (0.0%)  	leaq	0(%rsp),%rdi
1,578 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
    .         .byte	102,72,15,126,203
    .         .byte	102,72,15,126,207
  526 (0.0%)  	call	__ecp_nistz256_sub_fromq
    .         
  526 (0.0%)  	leaq	160+56(%rsp),%rsi
    .         .cfi_def_cfa	%rsi,8
  526 (0.0%)  	movq	-48(%rsi),%r15
    .         .cfi_restore	%r15
  526 (0.0%)  	movq	-40(%rsi),%r14
    .         .cfi_restore	%r14
  526 (0.0%)  	movq	-32(%rsi),%r13
    .         .cfi_restore	%r13
  526 (0.0%)  	movq	-24(%rsi),%r12
    .         .cfi_restore	%r12
  526 (0.0%)  	movq	-16(%rsi),%rbx
    .         .cfi_restore	%rbx
  526 (0.0%)  	movq	-8(%rsi),%rbp
    .         .cfi_restore	%rbp
1,052 (0.0%)  	leaq	(%rsi),%rsp
    .         .cfi_def_cfa_register	%rsp
    .         .Lpoint_doubleq_epilogue:
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	ecp_nistz256_point_double,.-ecp_nistz256_point_double
    .         .globl	ecp_nistz256_point_add
    .         .hidden ecp_nistz256_point_add
    .         .type	ecp_nistz256_point_add,@function
    .         .align	32
    .         ecp_nistz256_point_add:
    .         .cfi_startproc	
  116 (0.0%)  	leaq	OPENSSL_ia32cap_P(%rip),%rcx
  116 (0.0%)  	movq	8(%rcx),%rcx
  116 (0.0%)  	andl	$0x80100,%ecx
  116 (0.0%)  	cmpl	$0x80100,%ecx
  116 (0.0%)  	je	.Lpoint_addx
  116 (0.0%)  	pushq	%rbp
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%rbp,-16
  116 (0.0%)  	pushq	%rbx
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%rbx,-24
  116 (0.0%)  	pushq	%r12
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r12,-32
  116 (0.0%)  	pushq	%r13
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r13,-40
  116 (0.0%)  	pushq	%r14
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r14,-48
  116 (0.0%)  	pushq	%r15
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r15,-56
  116 (0.0%)  	subq	$576+8,%rsp
    .         .cfi_adjust_cfa_offset	32*18+8
    .         .Lpoint_addq_body:
    .         
  116 (0.0%)  	movdqu	0(%rsi),%xmm0
  116 (0.0%)  	movdqu	16(%rsi),%xmm1
  116 (0.0%)  	movdqu	32(%rsi),%xmm2
  116 (0.0%)  	movdqu	48(%rsi),%xmm3
  116 (0.0%)  	movdqu	64(%rsi),%xmm4
  116 (0.0%)  	movdqu	80(%rsi),%xmm5
  116 (0.0%)  	movq	%rsi,%rbx
  116 (0.0%)  	movq	%rdx,%rsi
  116 (0.0%)  	movdqa	%xmm0,384(%rsp)
  116 (0.0%)  	movdqa	%xmm1,384+16(%rsp)
  116 (0.0%)  	movdqa	%xmm2,416(%rsp)
  116 (0.0%)  	movdqa	%xmm3,416+16(%rsp)
  116 (0.0%)  	movdqa	%xmm4,448(%rsp)
  116 (0.0%)  	movdqa	%xmm5,448+16(%rsp)
  116 (0.0%)  	por	%xmm4,%xmm5
    .         
  116 (0.0%)  	movdqu	0(%rsi),%xmm0
  116 (0.0%)  	pshufd	$0xb1,%xmm5,%xmm3
  116 (0.0%)  	movdqu	16(%rsi),%xmm1
  116 (0.0%)  	movdqu	32(%rsi),%xmm2
  116 (0.0%)  	por	%xmm3,%xmm5
  116 (0.0%)  	movdqu	48(%rsi),%xmm3
  116 (0.0%)  	movq	64+0(%rsi),%rax
  116 (0.0%)  	movq	64+8(%rsi),%r14
  116 (0.0%)  	movq	64+16(%rsi),%r15
  116 (0.0%)  	movq	64+24(%rsi),%r8
  116 (0.0%)  	movdqa	%xmm0,480(%rsp)
  116 (0.0%)  	pshufd	$0x1e,%xmm5,%xmm4
  116 (0.0%)  	movdqa	%xmm1,480+16(%rsp)
  116 (0.0%)  	movdqu	64(%rsi),%xmm0
  116 (0.0%)  	movdqu	80(%rsi),%xmm1
  116 (0.0%)  	movdqa	%xmm2,512(%rsp)
  116 (0.0%)  	movdqa	%xmm3,512+16(%rsp)
  116 (0.0%)  	por	%xmm4,%xmm5
  116 (0.0%)  	pxor	%xmm4,%xmm4
  232 (0.0%)  	por	%xmm0,%xmm1
    .         .byte	102,72,15,110,199
    .         
  116 (0.0%)  	leaq	64-0(%rsi),%rsi
  116 (0.0%)  	movq	%rax,544+0(%rsp)
  116 (0.0%)  	movq	%r14,544+8(%rsp)
  116 (0.0%)  	movq	%r15,544+16(%rsp)
  116 (0.0%)  	movq	%r8,544+24(%rsp)
  116 (0.0%)  	leaq	96(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_sqr_montq
    .         
  116 (0.0%)  	pcmpeqd	%xmm4,%xmm5
  116 (0.0%)  	pshufd	$0xb1,%xmm1,%xmm4
  116 (0.0%)  	por	%xmm1,%xmm4
  116 (0.0%)  	pshufd	$0,%xmm5,%xmm5
  116 (0.0%)  	pshufd	$0x1e,%xmm4,%xmm3
  116 (0.0%)  	por	%xmm3,%xmm4
  116 (0.0%)  	pxor	%xmm3,%xmm3
  116 (0.0%)  	pcmpeqd	%xmm3,%xmm4
  116 (0.0%)  	pshufd	$0,%xmm4,%xmm4
  116 (0.0%)  	movq	64+0(%rbx),%rax
  116 (0.0%)  	movq	64+8(%rbx),%r14
  116 (0.0%)  	movq	64+16(%rbx),%r15
  232 (0.0%)  	movq	64+24(%rbx),%r8
    .         .byte	102,72,15,110,203
    .         
  116 (0.0%)  	leaq	64-0(%rbx),%rsi
  116 (0.0%)  	leaq	32(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_sqr_montq
    .         
  116 (0.0%)  	movq	544(%rsp),%rax
  116 (0.0%)  	leaq	544(%rsp),%rbx
  116 (0.0%)  	movq	0+96(%rsp),%r9
  116 (0.0%)  	movq	8+96(%rsp),%r10
  116 (0.0%)  	leaq	0+96(%rsp),%rsi
  116 (0.0%)  	movq	16+96(%rsp),%r11
  116 (0.0%)  	movq	24+96(%rsp),%r12
  116 (0.0%)  	leaq	224(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
  116 (0.0%)  	movq	448(%rsp),%rax
  116 (0.0%)  	leaq	448(%rsp),%rbx
  116 (0.0%)  	movq	0+32(%rsp),%r9
  116 (0.0%)  	movq	8+32(%rsp),%r10
  116 (0.0%)  	leaq	0+32(%rsp),%rsi
  116 (0.0%)  	movq	16+32(%rsp),%r11
  116 (0.0%)  	movq	24+32(%rsp),%r12
  116 (0.0%)  	leaq	256(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
  116 (0.0%)  	movq	416(%rsp),%rax
  116 (0.0%)  	leaq	416(%rsp),%rbx
  116 (0.0%)  	movq	0+224(%rsp),%r9
  116 (0.0%)  	movq	8+224(%rsp),%r10
  116 (0.0%)  	leaq	0+224(%rsp),%rsi
  116 (0.0%)  	movq	16+224(%rsp),%r11
  116 (0.0%)  	movq	24+224(%rsp),%r12
  116 (0.0%)  	leaq	224(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
  116 (0.0%)  	movq	512(%rsp),%rax
  116 (0.0%)  	leaq	512(%rsp),%rbx
  116 (0.0%)  	movq	0+256(%rsp),%r9
  116 (0.0%)  	movq	8+256(%rsp),%r10
  116 (0.0%)  	leaq	0+256(%rsp),%rsi
  116 (0.0%)  	movq	16+256(%rsp),%r11
  116 (0.0%)  	movq	24+256(%rsp),%r12
  116 (0.0%)  	leaq	256(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
  116 (0.0%)  	leaq	224(%rsp),%rbx
  116 (0.0%)  	leaq	64(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_sub_fromq
    .         
  116 (0.0%)  	orq	%r13,%r12
  116 (0.0%)  	movdqa	%xmm4,%xmm2
  116 (0.0%)  	orq	%r8,%r12
  116 (0.0%)  	orq	%r9,%r12
  232 (0.0%)  	por	%xmm5,%xmm2
    .         .byte	102,73,15,110,220
    .         
  116 (0.0%)  	movq	384(%rsp),%rax
  116 (0.0%)  	leaq	384(%rsp),%rbx
  116 (0.0%)  	movq	0+96(%rsp),%r9
  116 (0.0%)  	movq	8+96(%rsp),%r10
  116 (0.0%)  	leaq	0+96(%rsp),%rsi
  116 (0.0%)  	movq	16+96(%rsp),%r11
  116 (0.0%)  	movq	24+96(%rsp),%r12
  116 (0.0%)  	leaq	160(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
  116 (0.0%)  	movq	480(%rsp),%rax
  116 (0.0%)  	leaq	480(%rsp),%rbx
  116 (0.0%)  	movq	0+32(%rsp),%r9
  116 (0.0%)  	movq	8+32(%rsp),%r10
  116 (0.0%)  	leaq	0+32(%rsp),%rsi
  116 (0.0%)  	movq	16+32(%rsp),%r11
  116 (0.0%)  	movq	24+32(%rsp),%r12
  116 (0.0%)  	leaq	192(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
  116 (0.0%)  	leaq	160(%rsp),%rbx
  116 (0.0%)  	leaq	0(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_sub_fromq
    .         
  116 (0.0%)  	orq	%r13,%r12
  116 (0.0%)  	orq	%r8,%r12
  348 (0.0%)  	orq	%r9,%r12
    .         
    .         .byte	102,73,15,126,208
    .         .byte	102,73,15,126,217
  232 (0.0%)  	orq	%r8,%r12
    .         .byte	0x3e
    .         	jnz	.Ladd_proceedq
    .         
    .         
    .         
    .         	testq	%r9,%r9
    .         	jz	.Ladd_doubleq
    .         
-- line 2831 ----------------------------------------
-- line 2850 ----------------------------------------
    .         .byte	102,72,15,126,199
    .         	addq	$416,%rsp
    .         .cfi_adjust_cfa_offset	-416
    .         	jmp	.Lpoint_double_shortcutq
    .         .cfi_adjust_cfa_offset	416
    .         
    .         .align	32
    .         .Ladd_proceedq:
  116 (0.0%)  	movq	0+64(%rsp),%rax
  116 (0.0%)  	movq	8+64(%rsp),%r14
  116 (0.0%)  	leaq	0+64(%rsp),%rsi
  116 (0.0%)  	movq	16+64(%rsp),%r15
  116 (0.0%)  	movq	24+64(%rsp),%r8
  116 (0.0%)  	leaq	96(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_sqr_montq
    .         
  116 (0.0%)  	movq	448(%rsp),%rax
  116 (0.0%)  	leaq	448(%rsp),%rbx
  116 (0.0%)  	movq	0+0(%rsp),%r9
  116 (0.0%)  	movq	8+0(%rsp),%r10
  116 (0.0%)  	leaq	0+0(%rsp),%rsi
  116 (0.0%)  	movq	16+0(%rsp),%r11
  116 (0.0%)  	movq	24+0(%rsp),%r12
  116 (0.0%)  	leaq	352(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
  116 (0.0%)  	movq	0+0(%rsp),%rax
  116 (0.0%)  	movq	8+0(%rsp),%r14
  116 (0.0%)  	leaq	0+0(%rsp),%rsi
  116 (0.0%)  	movq	16+0(%rsp),%r15
  116 (0.0%)  	movq	24+0(%rsp),%r8
  116 (0.0%)  	leaq	32(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_sqr_montq
    .         
  116 (0.0%)  	movq	544(%rsp),%rax
  116 (0.0%)  	leaq	544(%rsp),%rbx
  116 (0.0%)  	movq	0+352(%rsp),%r9
  116 (0.0%)  	movq	8+352(%rsp),%r10
  116 (0.0%)  	leaq	0+352(%rsp),%rsi
  116 (0.0%)  	movq	16+352(%rsp),%r11
  116 (0.0%)  	movq	24+352(%rsp),%r12
  116 (0.0%)  	leaq	352(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
  116 (0.0%)  	movq	0(%rsp),%rax
  116 (0.0%)  	leaq	0(%rsp),%rbx
  116 (0.0%)  	movq	0+32(%rsp),%r9
  116 (0.0%)  	movq	8+32(%rsp),%r10
  116 (0.0%)  	leaq	0+32(%rsp),%rsi
  116 (0.0%)  	movq	16+32(%rsp),%r11
  116 (0.0%)  	movq	24+32(%rsp),%r12
  116 (0.0%)  	leaq	128(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
  116 (0.0%)  	movq	160(%rsp),%rax
  116 (0.0%)  	leaq	160(%rsp),%rbx
  116 (0.0%)  	movq	0+32(%rsp),%r9
  116 (0.0%)  	movq	8+32(%rsp),%r10
  116 (0.0%)  	leaq	0+32(%rsp),%rsi
  116 (0.0%)  	movq	16+32(%rsp),%r11
  116 (0.0%)  	movq	24+32(%rsp),%r12
  116 (0.0%)  	leaq	192(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
    .         
    .         
    .         
  116 (0.0%)  	xorq	%r11,%r11
  116 (0.0%)  	addq	%r12,%r12
  116 (0.0%)  	leaq	96(%rsp),%rsi
  116 (0.0%)  	adcq	%r13,%r13
  116 (0.0%)  	movq	%r12,%rax
  116 (0.0%)  	adcq	%r8,%r8
  116 (0.0%)  	adcq	%r9,%r9
  116 (0.0%)  	movq	%r13,%rbp
  116 (0.0%)  	adcq	$0,%r11
    .         
  116 (0.0%)  	subq	$-1,%r12
  116 (0.0%)  	movq	%r8,%rcx
  116 (0.0%)  	sbbq	%r14,%r13
  116 (0.0%)  	sbbq	$0,%r8
  116 (0.0%)  	movq	%r9,%r10
  116 (0.0%)  	sbbq	%r15,%r9
  116 (0.0%)  	sbbq	$0,%r11
    .         
  116 (0.0%)  	cmovcq	%rax,%r12
  116 (0.0%)  	movq	0(%rsi),%rax
  116 (0.0%)  	cmovcq	%rbp,%r13
  116 (0.0%)  	movq	8(%rsi),%rbp
  116 (0.0%)  	cmovcq	%rcx,%r8
  116 (0.0%)  	movq	16(%rsi),%rcx
  116 (0.0%)  	cmovcq	%r10,%r9
  116 (0.0%)  	movq	24(%rsi),%r10
    .         
  116 (0.0%)  	call	__ecp_nistz256_subq
    .         
  116 (0.0%)  	leaq	128(%rsp),%rbx
  116 (0.0%)  	leaq	288(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_sub_fromq
    .         
  116 (0.0%)  	movq	192+0(%rsp),%rax
  116 (0.0%)  	movq	192+8(%rsp),%rbp
  116 (0.0%)  	movq	192+16(%rsp),%rcx
  116 (0.0%)  	movq	192+24(%rsp),%r10
  116 (0.0%)  	leaq	320(%rsp),%rdi
    .         
  116 (0.0%)  	call	__ecp_nistz256_subq
    .         
  116 (0.0%)  	movq	%r12,0(%rdi)
  116 (0.0%)  	movq	%r13,8(%rdi)
  116 (0.0%)  	movq	%r8,16(%rdi)
  116 (0.0%)  	movq	%r9,24(%rdi)
  116 (0.0%)  	movq	128(%rsp),%rax
  116 (0.0%)  	leaq	128(%rsp),%rbx
  116 (0.0%)  	movq	0+224(%rsp),%r9
  116 (0.0%)  	movq	8+224(%rsp),%r10
  116 (0.0%)  	leaq	0+224(%rsp),%rsi
  116 (0.0%)  	movq	16+224(%rsp),%r11
  116 (0.0%)  	movq	24+224(%rsp),%r12
  116 (0.0%)  	leaq	256(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
  116 (0.0%)  	movq	320(%rsp),%rax
  116 (0.0%)  	leaq	320(%rsp),%rbx
  116 (0.0%)  	movq	0+64(%rsp),%r9
  116 (0.0%)  	movq	8+64(%rsp),%r10
  116 (0.0%)  	leaq	0+64(%rsp),%rsi
  116 (0.0%)  	movq	16+64(%rsp),%r11
  116 (0.0%)  	movq	24+64(%rsp),%r12
  116 (0.0%)  	leaq	320(%rsp),%rdi
  116 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
  116 (0.0%)  	leaq	256(%rsp),%rbx
  116 (0.0%)  	leaq	320(%rsp),%rdi
  232 (0.0%)  	call	__ecp_nistz256_sub_fromq
    .         
    .         .byte	102,72,15,126,199
    .         
  116 (0.0%)  	movdqa	%xmm5,%xmm0
  116 (0.0%)  	movdqa	%xmm5,%xmm1
  116 (0.0%)  	pandn	352(%rsp),%xmm0
  116 (0.0%)  	movdqa	%xmm5,%xmm2
  116 (0.0%)  	pandn	352+16(%rsp),%xmm1
  116 (0.0%)  	movdqa	%xmm5,%xmm3
  116 (0.0%)  	pand	544(%rsp),%xmm2
  116 (0.0%)  	pand	544+16(%rsp),%xmm3
  116 (0.0%)  	por	%xmm0,%xmm2
  116 (0.0%)  	por	%xmm1,%xmm3
    .         
  116 (0.0%)  	movdqa	%xmm4,%xmm0
  116 (0.0%)  	movdqa	%xmm4,%xmm1
  116 (0.0%)  	pandn	%xmm2,%xmm0
  116 (0.0%)  	movdqa	%xmm4,%xmm2
  116 (0.0%)  	pandn	%xmm3,%xmm1
  116 (0.0%)  	movdqa	%xmm4,%xmm3
  116 (0.0%)  	pand	448(%rsp),%xmm2
  116 (0.0%)  	pand	448+16(%rsp),%xmm3
  116 (0.0%)  	por	%xmm0,%xmm2
  116 (0.0%)  	por	%xmm1,%xmm3
  116 (0.0%)  	movdqu	%xmm2,64(%rdi)
  116 (0.0%)  	movdqu	%xmm3,80(%rdi)
    .         
  116 (0.0%)  	movdqa	%xmm5,%xmm0
  116 (0.0%)  	movdqa	%xmm5,%xmm1
  116 (0.0%)  	pandn	288(%rsp),%xmm0
  116 (0.0%)  	movdqa	%xmm5,%xmm2
  116 (0.0%)  	pandn	288+16(%rsp),%xmm1
  116 (0.0%)  	movdqa	%xmm5,%xmm3
  116 (0.0%)  	pand	480(%rsp),%xmm2
  116 (0.0%)  	pand	480+16(%rsp),%xmm3
  116 (0.0%)  	por	%xmm0,%xmm2
  116 (0.0%)  	por	%xmm1,%xmm3
    .         
  116 (0.0%)  	movdqa	%xmm4,%xmm0
  116 (0.0%)  	movdqa	%xmm4,%xmm1
  116 (0.0%)  	pandn	%xmm2,%xmm0
  116 (0.0%)  	movdqa	%xmm4,%xmm2
  116 (0.0%)  	pandn	%xmm3,%xmm1
  116 (0.0%)  	movdqa	%xmm4,%xmm3
  116 (0.0%)  	pand	384(%rsp),%xmm2
  116 (0.0%)  	pand	384+16(%rsp),%xmm3
  116 (0.0%)  	por	%xmm0,%xmm2
  116 (0.0%)  	por	%xmm1,%xmm3
  116 (0.0%)  	movdqu	%xmm2,0(%rdi)
  116 (0.0%)  	movdqu	%xmm3,16(%rdi)
    .         
  116 (0.0%)  	movdqa	%xmm5,%xmm0
  116 (0.0%)  	movdqa	%xmm5,%xmm1
  116 (0.0%)  	pandn	320(%rsp),%xmm0
  116 (0.0%)  	movdqa	%xmm5,%xmm2
  116 (0.0%)  	pandn	320+16(%rsp),%xmm1
  116 (0.0%)  	movdqa	%xmm5,%xmm3
  116 (0.0%)  	pand	512(%rsp),%xmm2
  116 (0.0%)  	pand	512+16(%rsp),%xmm3
  116 (0.0%)  	por	%xmm0,%xmm2
  116 (0.0%)  	por	%xmm1,%xmm3
    .         
  116 (0.0%)  	movdqa	%xmm4,%xmm0
  116 (0.0%)  	movdqa	%xmm4,%xmm1
  116 (0.0%)  	pandn	%xmm2,%xmm0
  116 (0.0%)  	movdqa	%xmm4,%xmm2
  116 (0.0%)  	pandn	%xmm3,%xmm1
  116 (0.0%)  	movdqa	%xmm4,%xmm3
  116 (0.0%)  	pand	416(%rsp),%xmm2
  116 (0.0%)  	pand	416+16(%rsp),%xmm3
  116 (0.0%)  	por	%xmm0,%xmm2
  116 (0.0%)  	por	%xmm1,%xmm3
  116 (0.0%)  	movdqu	%xmm2,32(%rdi)
  116 (0.0%)  	movdqu	%xmm3,48(%rdi)
    .         
    .         .Ladd_doneq:
  116 (0.0%)  	leaq	576+56(%rsp),%rsi
    .         .cfi_def_cfa	%rsi,8
  116 (0.0%)  	movq	-48(%rsi),%r15
    .         .cfi_restore	%r15
  116 (0.0%)  	movq	-40(%rsi),%r14
    .         .cfi_restore	%r14
  116 (0.0%)  	movq	-32(%rsi),%r13
    .         .cfi_restore	%r13
  116 (0.0%)  	movq	-24(%rsi),%r12
    .         .cfi_restore	%r12
  116 (0.0%)  	movq	-16(%rsi),%rbx
    .         .cfi_restore	%rbx
  116 (0.0%)  	movq	-8(%rsi),%rbp
    .         .cfi_restore	%rbp
  232 (0.0%)  	leaq	(%rsi),%rsp
    .         .cfi_def_cfa_register	%rsp
    .         .Lpoint_addq_epilogue:
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	ecp_nistz256_point_add,.-ecp_nistz256_point_add
    .         .globl	ecp_nistz256_point_add_affine
    .         .hidden ecp_nistz256_point_add_affine
    .         .type	ecp_nistz256_point_add_affine,@function
    .         .align	32
    .         ecp_nistz256_point_add_affine:
    .         .cfi_startproc	
   72 (0.0%)  	leaq	OPENSSL_ia32cap_P(%rip),%rcx
   72 (0.0%)  	movq	8(%rcx),%rcx
   72 (0.0%)  	andl	$0x80100,%ecx
   72 (0.0%)  	cmpl	$0x80100,%ecx
   72 (0.0%)  	je	.Lpoint_add_affinex
   72 (0.0%)  	pushq	%rbp
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%rbp,-16
   72 (0.0%)  	pushq	%rbx
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%rbx,-24
   72 (0.0%)  	pushq	%r12
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r12,-32
   72 (0.0%)  	pushq	%r13
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r13,-40
   72 (0.0%)  	pushq	%r14
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r14,-48
   72 (0.0%)  	pushq	%r15
    .         .cfi_adjust_cfa_offset	8
    .         .cfi_offset	%r15,-56
   72 (0.0%)  	subq	$480+8,%rsp
    .         .cfi_adjust_cfa_offset	32*15+8
    .         .Ladd_affineq_body:
    .         
   72 (0.0%)  	movdqu	0(%rsi),%xmm0
   72 (0.0%)  	movq	%rdx,%rbx
   72 (0.0%)  	movdqu	16(%rsi),%xmm1
   72 (0.0%)  	movdqu	32(%rsi),%xmm2
   72 (0.0%)  	movdqu	48(%rsi),%xmm3
   72 (0.0%)  	movdqu	64(%rsi),%xmm4
   72 (0.0%)  	movdqu	80(%rsi),%xmm5
   72 (0.0%)  	movq	64+0(%rsi),%rax
   72 (0.0%)  	movq	64+8(%rsi),%r14
   72 (0.0%)  	movq	64+16(%rsi),%r15
   72 (0.0%)  	movq	64+24(%rsi),%r8
   72 (0.0%)  	movdqa	%xmm0,320(%rsp)
   72 (0.0%)  	movdqa	%xmm1,320+16(%rsp)
   72 (0.0%)  	movdqa	%xmm2,352(%rsp)
   72 (0.0%)  	movdqa	%xmm3,352+16(%rsp)
   72 (0.0%)  	movdqa	%xmm4,384(%rsp)
   72 (0.0%)  	movdqa	%xmm5,384+16(%rsp)
   72 (0.0%)  	por	%xmm4,%xmm5
    .         
   72 (0.0%)  	movdqu	0(%rbx),%xmm0
   72 (0.0%)  	pshufd	$0xb1,%xmm5,%xmm3
   72 (0.0%)  	movdqu	16(%rbx),%xmm1
   72 (0.0%)  	movdqu	32(%rbx),%xmm2
   72 (0.0%)  	por	%xmm3,%xmm5
   72 (0.0%)  	movdqu	48(%rbx),%xmm3
   72 (0.0%)  	movdqa	%xmm0,416(%rsp)
   72 (0.0%)  	pshufd	$0x1e,%xmm5,%xmm4
   72 (0.0%)  	movdqa	%xmm1,416+16(%rsp)
  144 (0.0%)  	por	%xmm0,%xmm1
    .         .byte	102,72,15,110,199
   72 (0.0%)  	movdqa	%xmm2,448(%rsp)
   72 (0.0%)  	movdqa	%xmm3,448+16(%rsp)
   72 (0.0%)  	por	%xmm2,%xmm3
   72 (0.0%)  	por	%xmm4,%xmm5
   72 (0.0%)  	pxor	%xmm4,%xmm4
   72 (0.0%)  	por	%xmm1,%xmm3
    .         
   72 (0.0%)  	leaq	64-0(%rsi),%rsi
   72 (0.0%)  	leaq	32(%rsp),%rdi
   72 (0.0%)  	call	__ecp_nistz256_sqr_montq
    .         
   72 (0.0%)  	pcmpeqd	%xmm4,%xmm5
   72 (0.0%)  	pshufd	$0xb1,%xmm3,%xmm4
   72 (0.0%)  	movq	0(%rbx),%rax
    .         
   72 (0.0%)  	movq	%r12,%r9
   72 (0.0%)  	por	%xmm3,%xmm4
   72 (0.0%)  	pshufd	$0,%xmm5,%xmm5
   72 (0.0%)  	pshufd	$0x1e,%xmm4,%xmm3
   72 (0.0%)  	movq	%r13,%r10
   72 (0.0%)  	por	%xmm3,%xmm4
   72 (0.0%)  	pxor	%xmm3,%xmm3
   72 (0.0%)  	movq	%r14,%r11
   72 (0.0%)  	pcmpeqd	%xmm3,%xmm4
   72 (0.0%)  	pshufd	$0,%xmm4,%xmm4
    .         
   72 (0.0%)  	leaq	32-0(%rsp),%rsi
   72 (0.0%)  	movq	%r15,%r12
   72 (0.0%)  	leaq	0(%rsp),%rdi
   72 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
   72 (0.0%)  	leaq	320(%rsp),%rbx
   72 (0.0%)  	leaq	64(%rsp),%rdi
   72 (0.0%)  	call	__ecp_nistz256_sub_fromq
    .         
   72 (0.0%)  	movq	384(%rsp),%rax
   72 (0.0%)  	leaq	384(%rsp),%rbx
   72 (0.0%)  	movq	0+32(%rsp),%r9
   72 (0.0%)  	movq	8+32(%rsp),%r10
   72 (0.0%)  	leaq	0+32(%rsp),%rsi
   72 (0.0%)  	movq	16+32(%rsp),%r11
   72 (0.0%)  	movq	24+32(%rsp),%r12
   72 (0.0%)  	leaq	32(%rsp),%rdi
   72 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
   72 (0.0%)  	movq	384(%rsp),%rax
   72 (0.0%)  	leaq	384(%rsp),%rbx
   72 (0.0%)  	movq	0+64(%rsp),%r9
   72 (0.0%)  	movq	8+64(%rsp),%r10
   72 (0.0%)  	leaq	0+64(%rsp),%rsi
   72 (0.0%)  	movq	16+64(%rsp),%r11
   72 (0.0%)  	movq	24+64(%rsp),%r12
   72 (0.0%)  	leaq	288(%rsp),%rdi
   72 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
   72 (0.0%)  	movq	448(%rsp),%rax
   72 (0.0%)  	leaq	448(%rsp),%rbx
   72 (0.0%)  	movq	0+32(%rsp),%r9
   72 (0.0%)  	movq	8+32(%rsp),%r10
   72 (0.0%)  	leaq	0+32(%rsp),%rsi
   72 (0.0%)  	movq	16+32(%rsp),%r11
   72 (0.0%)  	movq	24+32(%rsp),%r12
   72 (0.0%)  	leaq	32(%rsp),%rdi
   72 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
   72 (0.0%)  	leaq	352(%rsp),%rbx
   72 (0.0%)  	leaq	96(%rsp),%rdi
   72 (0.0%)  	call	__ecp_nistz256_sub_fromq
    .         
   72 (0.0%)  	movq	0+64(%rsp),%rax
   72 (0.0%)  	movq	8+64(%rsp),%r14
   72 (0.0%)  	leaq	0+64(%rsp),%rsi
   72 (0.0%)  	movq	16+64(%rsp),%r15
   72 (0.0%)  	movq	24+64(%rsp),%r8
   72 (0.0%)  	leaq	128(%rsp),%rdi
   72 (0.0%)  	call	__ecp_nistz256_sqr_montq
    .         
   72 (0.0%)  	movq	0+96(%rsp),%rax
   72 (0.0%)  	movq	8+96(%rsp),%r14
   72 (0.0%)  	leaq	0+96(%rsp),%rsi
   72 (0.0%)  	movq	16+96(%rsp),%r15
   72 (0.0%)  	movq	24+96(%rsp),%r8
   72 (0.0%)  	leaq	192(%rsp),%rdi
   72 (0.0%)  	call	__ecp_nistz256_sqr_montq
    .         
   72 (0.0%)  	movq	128(%rsp),%rax
   72 (0.0%)  	leaq	128(%rsp),%rbx
   72 (0.0%)  	movq	0+64(%rsp),%r9
   72 (0.0%)  	movq	8+64(%rsp),%r10
   72 (0.0%)  	leaq	0+64(%rsp),%rsi
   72 (0.0%)  	movq	16+64(%rsp),%r11
   72 (0.0%)  	movq	24+64(%rsp),%r12
   72 (0.0%)  	leaq	160(%rsp),%rdi
   72 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
   72 (0.0%)  	movq	320(%rsp),%rax
   72 (0.0%)  	leaq	320(%rsp),%rbx
   72 (0.0%)  	movq	0+128(%rsp),%r9
   72 (0.0%)  	movq	8+128(%rsp),%r10
   72 (0.0%)  	leaq	0+128(%rsp),%rsi
   72 (0.0%)  	movq	16+128(%rsp),%r11
   72 (0.0%)  	movq	24+128(%rsp),%r12
   72 (0.0%)  	leaq	0(%rsp),%rdi
   72 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
    .         
    .         
    .         
   72 (0.0%)  	xorq	%r11,%r11
   72 (0.0%)  	addq	%r12,%r12
   72 (0.0%)  	leaq	192(%rsp),%rsi
   72 (0.0%)  	adcq	%r13,%r13
   72 (0.0%)  	movq	%r12,%rax
   72 (0.0%)  	adcq	%r8,%r8
   72 (0.0%)  	adcq	%r9,%r9
   72 (0.0%)  	movq	%r13,%rbp
   72 (0.0%)  	adcq	$0,%r11
    .         
   72 (0.0%)  	subq	$-1,%r12
   72 (0.0%)  	movq	%r8,%rcx
   72 (0.0%)  	sbbq	%r14,%r13
   72 (0.0%)  	sbbq	$0,%r8
   72 (0.0%)  	movq	%r9,%r10
   72 (0.0%)  	sbbq	%r15,%r9
   72 (0.0%)  	sbbq	$0,%r11
    .         
   72 (0.0%)  	cmovcq	%rax,%r12
   72 (0.0%)  	movq	0(%rsi),%rax
   72 (0.0%)  	cmovcq	%rbp,%r13
   72 (0.0%)  	movq	8(%rsi),%rbp
   72 (0.0%)  	cmovcq	%rcx,%r8
   72 (0.0%)  	movq	16(%rsi),%rcx
   72 (0.0%)  	cmovcq	%r10,%r9
   72 (0.0%)  	movq	24(%rsi),%r10
    .         
   72 (0.0%)  	call	__ecp_nistz256_subq
    .         
   72 (0.0%)  	leaq	160(%rsp),%rbx
   72 (0.0%)  	leaq	224(%rsp),%rdi
   72 (0.0%)  	call	__ecp_nistz256_sub_fromq
    .         
   72 (0.0%)  	movq	0+0(%rsp),%rax
   72 (0.0%)  	movq	0+8(%rsp),%rbp
   72 (0.0%)  	movq	0+16(%rsp),%rcx
   72 (0.0%)  	movq	0+24(%rsp),%r10
   72 (0.0%)  	leaq	64(%rsp),%rdi
    .         
   72 (0.0%)  	call	__ecp_nistz256_subq
    .         
   72 (0.0%)  	movq	%r12,0(%rdi)
   72 (0.0%)  	movq	%r13,8(%rdi)
   72 (0.0%)  	movq	%r8,16(%rdi)
   72 (0.0%)  	movq	%r9,24(%rdi)
   72 (0.0%)  	movq	352(%rsp),%rax
   72 (0.0%)  	leaq	352(%rsp),%rbx
   72 (0.0%)  	movq	0+160(%rsp),%r9
   72 (0.0%)  	movq	8+160(%rsp),%r10
   72 (0.0%)  	leaq	0+160(%rsp),%rsi
   72 (0.0%)  	movq	16+160(%rsp),%r11
   72 (0.0%)  	movq	24+160(%rsp),%r12
   72 (0.0%)  	leaq	32(%rsp),%rdi
   72 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
   72 (0.0%)  	movq	96(%rsp),%rax
   72 (0.0%)  	leaq	96(%rsp),%rbx
   72 (0.0%)  	movq	0+64(%rsp),%r9
   72 (0.0%)  	movq	8+64(%rsp),%r10
   72 (0.0%)  	leaq	0+64(%rsp),%rsi
   72 (0.0%)  	movq	16+64(%rsp),%r11
   72 (0.0%)  	movq	24+64(%rsp),%r12
   72 (0.0%)  	leaq	64(%rsp),%rdi
   72 (0.0%)  	call	__ecp_nistz256_mul_montq
    .         
   72 (0.0%)  	leaq	32(%rsp),%rbx
   72 (0.0%)  	leaq	256(%rsp),%rdi
  144 (0.0%)  	call	__ecp_nistz256_sub_fromq
    .         
    .         .byte	102,72,15,126,199
    .         
   72 (0.0%)  	movdqa	%xmm5,%xmm0
   72 (0.0%)  	movdqa	%xmm5,%xmm1
   72 (0.0%)  	pandn	288(%rsp),%xmm0
   72 (0.0%)  	movdqa	%xmm5,%xmm2
   72 (0.0%)  	pandn	288+16(%rsp),%xmm1
   72 (0.0%)  	movdqa	%xmm5,%xmm3
   72 (0.0%)  	pand	.LONE_mont(%rip),%xmm2
   72 (0.0%)  	pand	.LONE_mont+16(%rip),%xmm3
   72 (0.0%)  	por	%xmm0,%xmm2
   72 (0.0%)  	por	%xmm1,%xmm3
    .         
   72 (0.0%)  	movdqa	%xmm4,%xmm0
   72 (0.0%)  	movdqa	%xmm4,%xmm1
   72 (0.0%)  	pandn	%xmm2,%xmm0
   72 (0.0%)  	movdqa	%xmm4,%xmm2
   72 (0.0%)  	pandn	%xmm3,%xmm1
   72 (0.0%)  	movdqa	%xmm4,%xmm3
   72 (0.0%)  	pand	384(%rsp),%xmm2
   72 (0.0%)  	pand	384+16(%rsp),%xmm3
   72 (0.0%)  	por	%xmm0,%xmm2
   72 (0.0%)  	por	%xmm1,%xmm3
   72 (0.0%)  	movdqu	%xmm2,64(%rdi)
   72 (0.0%)  	movdqu	%xmm3,80(%rdi)
    .         
   72 (0.0%)  	movdqa	%xmm5,%xmm0
   72 (0.0%)  	movdqa	%xmm5,%xmm1
   72 (0.0%)  	pandn	224(%rsp),%xmm0
   72 (0.0%)  	movdqa	%xmm5,%xmm2
   72 (0.0%)  	pandn	224+16(%rsp),%xmm1
   72 (0.0%)  	movdqa	%xmm5,%xmm3
   72 (0.0%)  	pand	416(%rsp),%xmm2
   72 (0.0%)  	pand	416+16(%rsp),%xmm3
   72 (0.0%)  	por	%xmm0,%xmm2
   72 (0.0%)  	por	%xmm1,%xmm3
    .         
   72 (0.0%)  	movdqa	%xmm4,%xmm0
   72 (0.0%)  	movdqa	%xmm4,%xmm1
   72 (0.0%)  	pandn	%xmm2,%xmm0
   72 (0.0%)  	movdqa	%xmm4,%xmm2
   72 (0.0%)  	pandn	%xmm3,%xmm1
   72 (0.0%)  	movdqa	%xmm4,%xmm3
   72 (0.0%)  	pand	320(%rsp),%xmm2
   72 (0.0%)  	pand	320+16(%rsp),%xmm3
   72 (0.0%)  	por	%xmm0,%xmm2
   72 (0.0%)  	por	%xmm1,%xmm3
   72 (0.0%)  	movdqu	%xmm2,0(%rdi)
   72 (0.0%)  	movdqu	%xmm3,16(%rdi)
    .         
   72 (0.0%)  	movdqa	%xmm5,%xmm0
   72 (0.0%)  	movdqa	%xmm5,%xmm1
   72 (0.0%)  	pandn	256(%rsp),%xmm0
   72 (0.0%)  	movdqa	%xmm5,%xmm2
   72 (0.0%)  	pandn	256+16(%rsp),%xmm1
   72 (0.0%)  	movdqa	%xmm5,%xmm3
   72 (0.0%)  	pand	448(%rsp),%xmm2
   72 (0.0%)  	pand	448+16(%rsp),%xmm3
   72 (0.0%)  	por	%xmm0,%xmm2
   72 (0.0%)  	por	%xmm1,%xmm3
    .         
   72 (0.0%)  	movdqa	%xmm4,%xmm0
   72 (0.0%)  	movdqa	%xmm4,%xmm1
   72 (0.0%)  	pandn	%xmm2,%xmm0
   72 (0.0%)  	movdqa	%xmm4,%xmm2
   72 (0.0%)  	pandn	%xmm3,%xmm1
   72 (0.0%)  	movdqa	%xmm4,%xmm3
   72 (0.0%)  	pand	352(%rsp),%xmm2
   72 (0.0%)  	pand	352+16(%rsp),%xmm3
   72 (0.0%)  	por	%xmm0,%xmm2
   72 (0.0%)  	por	%xmm1,%xmm3
   72 (0.0%)  	movdqu	%xmm2,32(%rdi)
   72 (0.0%)  	movdqu	%xmm3,48(%rdi)
    .         
   72 (0.0%)  	leaq	480+56(%rsp),%rsi
    .         .cfi_def_cfa	%rsi,8
   72 (0.0%)  	movq	-48(%rsi),%r15
    .         .cfi_restore	%r15
   72 (0.0%)  	movq	-40(%rsi),%r14
    .         .cfi_restore	%r14
   72 (0.0%)  	movq	-32(%rsi),%r13
    .         .cfi_restore	%r13
   72 (0.0%)  	movq	-24(%rsi),%r12
    .         .cfi_restore	%r12
   72 (0.0%)  	movq	-16(%rsi),%rbx
    .         .cfi_restore	%rbx
   72 (0.0%)  	movq	-8(%rsi),%rbp
    .         .cfi_restore	%rbp
  144 (0.0%)  	leaq	(%rsi),%rsp
    .         .cfi_def_cfa_register	%rsp
    .         .Ladd_affineq_epilogue:
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	ecp_nistz256_point_add_affine,.-ecp_nistz256_point_add_affine
    .         .type	__ecp_nistz256_add_tox,@function
    .         .align	32
    .         __ecp_nistz256_add_tox:
-- line 3417 ----------------------------------------

--------------------------------------------------------------------------------
-- Annotated source file: /home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/sha256-x86_64.S
--------------------------------------------------------------------------------
Ir__________ 

-- line 9 ----------------------------------------
    .         .extern	OPENSSL_ia32cap_P
    .         .hidden OPENSSL_ia32cap_P
    .         .globl	sha256_block_data_order
    .         .hidden sha256_block_data_order
    .         .type	sha256_block_data_order,@function
    .         .align	16
    .         sha256_block_data_order:
    .         .cfi_startproc	
  262 (0.0%)  	leaq	OPENSSL_ia32cap_P(%rip),%r11
  262 (0.0%)  	movl	0(%r11),%r9d
  262 (0.0%)  	movl	4(%r11),%r10d
  262 (0.0%)  	movl	8(%r11),%r11d
  262 (0.0%)  	testl	$536870912,%r11d
  262 (0.0%)  	jnz	.Lshaext_shortcut
  262 (0.0%)  	andl	$1073741824,%r9d
  262 (0.0%)  	andl	$268435968,%r10d
  262 (0.0%)  	orl	%r9d,%r10d
  262 (0.0%)  	cmpl	$1342177792,%r10d
  262 (0.0%)  	je	.Lavx_shortcut
    .         	testl	$512,%r10d
    .         	jnz	.Lssse3_shortcut
    .         	movq	%rsp,%rax
    .         .cfi_def_cfa_register	%rax
    .         	pushq	%rbx
    .         .cfi_offset	%rbx,-16
    .         	pushq	%rbp
    .         .cfi_offset	%rbp,-24
-- line 35 ----------------------------------------
-- line 3105 ----------------------------------------
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	sha256_block_data_order_ssse3,.-sha256_block_data_order_ssse3
    .         .type	sha256_block_data_order_avx,@function
    .         .align	64
    .         sha256_block_data_order_avx:
    .         .cfi_startproc	
    .         .Lavx_shortcut:
  262 (0.0%)  	movq	%rsp,%rax
    .         .cfi_def_cfa_register	%rax
  262 (0.0%)  	pushq	%rbx
    .         .cfi_offset	%rbx,-16
  262 (0.0%)  	pushq	%rbp
    .         .cfi_offset	%rbp,-24
  262 (0.0%)  	pushq	%r12
    .         .cfi_offset	%r12,-32
  262 (0.0%)  	pushq	%r13
    .         .cfi_offset	%r13,-40
  262 (0.0%)  	pushq	%r14
    .         .cfi_offset	%r14,-48
  262 (0.0%)  	pushq	%r15
    .         .cfi_offset	%r15,-56
  262 (0.0%)  	shlq	$4,%rdx
  262 (0.0%)  	subq	$96,%rsp
  262 (0.0%)  	leaq	(%rsi,%rdx,4),%rdx
  262 (0.0%)  	andq	$-64,%rsp
  262 (0.0%)  	movq	%rdi,64+0(%rsp)
  262 (0.0%)  	movq	%rsi,64+8(%rsp)
  262 (0.0%)  	movq	%rdx,64+16(%rsp)
  262 (0.0%)  	movq	%rax,88(%rsp)
    .         .cfi_escape	0x0f,0x06,0x77,0xd8,0x00,0x06,0x23,0x08
    .         .Lprologue_avx:
    .         
  262 (0.0%)  	vzeroupper
  262 (0.0%)  	movl	0(%rdi),%eax
  262 (0.0%)  	movl	4(%rdi),%ebx
  262 (0.0%)  	movl	8(%rdi),%ecx
  262 (0.0%)  	movl	12(%rdi),%edx
  262 (0.0%)  	movl	16(%rdi),%r8d
  262 (0.0%)  	movl	20(%rdi),%r9d
  262 (0.0%)  	movl	24(%rdi),%r10d
  262 (0.0%)  	movl	28(%rdi),%r11d
  262 (0.0%)  	vmovdqa	K256+512+32(%rip),%xmm8
  262 (0.0%)  	vmovdqa	K256+512+64(%rip),%xmm9
  262 (0.0%)  	jmp	.Lloop_avx
    .         .align	16
    .         .Lloop_avx:
  393 (0.0%)  	vmovdqa	K256+512(%rip),%xmm7
  393 (0.0%)  	vmovdqu	0(%rsi),%xmm0
  393 (0.0%)  	vmovdqu	16(%rsi),%xmm1
  393 (0.0%)  	vmovdqu	32(%rsi),%xmm2
  393 (0.0%)  	vmovdqu	48(%rsi),%xmm3
  393 (0.0%)  	vpshufb	%xmm7,%xmm0,%xmm0
  393 (0.0%)  	leaq	K256(%rip),%rbp
  393 (0.0%)  	vpshufb	%xmm7,%xmm1,%xmm1
  393 (0.0%)  	vpshufb	%xmm7,%xmm2,%xmm2
  393 (0.0%)  	vpaddd	0(%rbp),%xmm0,%xmm4
  393 (0.0%)  	vpshufb	%xmm7,%xmm3,%xmm3
  393 (0.0%)  	vpaddd	32(%rbp),%xmm1,%xmm5
  393 (0.0%)  	vpaddd	64(%rbp),%xmm2,%xmm6
  393 (0.0%)  	vpaddd	96(%rbp),%xmm3,%xmm7
  393 (0.0%)  	vmovdqa	%xmm4,0(%rsp)
  393 (0.0%)  	movl	%eax,%r14d
  393 (0.0%)  	vmovdqa	%xmm5,16(%rsp)
  393 (0.0%)  	movl	%ebx,%edi
  393 (0.0%)  	vmovdqa	%xmm6,32(%rsp)
  393 (0.0%)  	xorl	%ecx,%edi
  393 (0.0%)  	vmovdqa	%xmm7,48(%rsp)
  393 (0.0%)  	movl	%r8d,%r13d
  393 (0.0%)  	jmp	.Lavx_00_47
    .         
    .         .align	16
    .         .Lavx_00_47:
1,179 (0.0%)  	subq	$-128,%rbp
1,179 (0.0%)  	vpalignr	$4,%xmm0,%xmm1,%xmm4
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	movl	%r14d,%eax
1,179 (0.0%)  	movl	%r9d,%r12d
1,179 (0.0%)  	vpalignr	$4,%xmm2,%xmm3,%xmm7
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	xorl	%r8d,%r13d
1,179 (0.0%)  	xorl	%r10d,%r12d
1,179 (0.0%)  	vpsrld	$7,%xmm4,%xmm6
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	xorl	%eax,%r14d
1,179 (0.0%)  	andl	%r8d,%r12d
1,179 (0.0%)  	vpaddd	%xmm7,%xmm0,%xmm0
1,179 (0.0%)  	xorl	%r8d,%r13d
1,179 (0.0%)  	addl	0(%rsp),%r11d
1,179 (0.0%)  	movl	%eax,%r15d
1,179 (0.0%)  	vpsrld	$3,%xmm4,%xmm7
1,179 (0.0%)  	xorl	%r10d,%r12d
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	xorl	%ebx,%r15d
1,179 (0.0%)  	vpslld	$14,%xmm4,%xmm5
1,179 (0.0%)  	addl	%r12d,%r11d
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	andl	%r15d,%edi
1,179 (0.0%)  	vpxor	%xmm6,%xmm7,%xmm4
1,179 (0.0%)  	xorl	%eax,%r14d
1,179 (0.0%)  	addl	%r13d,%r11d
1,179 (0.0%)  	xorl	%ebx,%edi
1,179 (0.0%)  	vpshufd	$250,%xmm3,%xmm7
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	addl	%r11d,%edx
1,179 (0.0%)  	addl	%edi,%r11d
1,179 (0.0%)  	vpsrld	$11,%xmm6,%xmm6
1,179 (0.0%)  	movl	%edx,%r13d
1,179 (0.0%)  	addl	%r11d,%r14d
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	vpxor	%xmm5,%xmm4,%xmm4
1,179 (0.0%)  	movl	%r14d,%r11d
1,179 (0.0%)  	movl	%r8d,%r12d
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	vpslld	$11,%xmm5,%xmm5
1,179 (0.0%)  	xorl	%edx,%r13d
1,179 (0.0%)  	xorl	%r9d,%r12d
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	vpxor	%xmm6,%xmm4,%xmm4
1,179 (0.0%)  	xorl	%r11d,%r14d
1,179 (0.0%)  	andl	%edx,%r12d
1,179 (0.0%)  	xorl	%edx,%r13d
1,179 (0.0%)  	vpsrld	$10,%xmm7,%xmm6
1,179 (0.0%)  	addl	4(%rsp),%r10d
1,179 (0.0%)  	movl	%r11d,%edi
1,179 (0.0%)  	xorl	%r9d,%r12d
1,179 (0.0%)  	vpxor	%xmm5,%xmm4,%xmm4
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	xorl	%eax,%edi
1,179 (0.0%)  	addl	%r12d,%r10d
1,179 (0.0%)  	vpsrlq	$17,%xmm7,%xmm7
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	andl	%edi,%r15d
1,179 (0.0%)  	xorl	%r11d,%r14d
1,179 (0.0%)  	vpaddd	%xmm4,%xmm0,%xmm0
1,179 (0.0%)  	addl	%r13d,%r10d
1,179 (0.0%)  	xorl	%eax,%r15d
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	addl	%r10d,%ecx
1,179 (0.0%)  	addl	%r15d,%r10d
1,179 (0.0%)  	movl	%ecx,%r13d
1,179 (0.0%)  	vpsrlq	$2,%xmm7,%xmm7
1,179 (0.0%)  	addl	%r10d,%r14d
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	movl	%r14d,%r10d
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	movl	%edx,%r12d
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	xorl	%ecx,%r13d
1,179 (0.0%)  	vpshufb	%xmm8,%xmm6,%xmm6
1,179 (0.0%)  	xorl	%r8d,%r12d
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	xorl	%r10d,%r14d
1,179 (0.0%)  	vpaddd	%xmm6,%xmm0,%xmm0
1,179 (0.0%)  	andl	%ecx,%r12d
1,179 (0.0%)  	xorl	%ecx,%r13d
1,179 (0.0%)  	addl	8(%rsp),%r9d
1,179 (0.0%)  	vpshufd	$80,%xmm0,%xmm7
1,179 (0.0%)  	movl	%r10d,%r15d
1,179 (0.0%)  	xorl	%r8d,%r12d
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	vpsrld	$10,%xmm7,%xmm6
1,179 (0.0%)  	xorl	%r11d,%r15d
1,179 (0.0%)  	addl	%r12d,%r9d
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	vpsrlq	$17,%xmm7,%xmm7
1,179 (0.0%)  	andl	%r15d,%edi
1,179 (0.0%)  	xorl	%r10d,%r14d
1,179 (0.0%)  	addl	%r13d,%r9d
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	xorl	%r11d,%edi
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	addl	%r9d,%ebx
1,179 (0.0%)  	vpsrlq	$2,%xmm7,%xmm7
1,179 (0.0%)  	addl	%edi,%r9d
1,179 (0.0%)  	movl	%ebx,%r13d
1,179 (0.0%)  	addl	%r9d,%r14d
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	movl	%r14d,%r9d
1,179 (0.0%)  	movl	%ecx,%r12d
1,179 (0.0%)  	vpshufb	%xmm9,%xmm6,%xmm6
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	xorl	%ebx,%r13d
1,179 (0.0%)  	xorl	%edx,%r12d
1,179 (0.0%)  	vpaddd	%xmm6,%xmm0,%xmm0
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	xorl	%r9d,%r14d
1,179 (0.0%)  	andl	%ebx,%r12d
1,179 (0.0%)  	vpaddd	0(%rbp),%xmm0,%xmm6
1,179 (0.0%)  	xorl	%ebx,%r13d
1,179 (0.0%)  	addl	12(%rsp),%r8d
1,179 (0.0%)  	movl	%r9d,%edi
1,179 (0.0%)  	xorl	%edx,%r12d
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	xorl	%r10d,%edi
1,179 (0.0%)  	addl	%r12d,%r8d
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	andl	%edi,%r15d
1,179 (0.0%)  	xorl	%r9d,%r14d
1,179 (0.0%)  	addl	%r13d,%r8d
1,179 (0.0%)  	xorl	%r10d,%r15d
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	addl	%r8d,%eax
1,179 (0.0%)  	addl	%r15d,%r8d
1,179 (0.0%)  	movl	%eax,%r13d
1,179 (0.0%)  	addl	%r8d,%r14d
1,179 (0.0%)  	vmovdqa	%xmm6,0(%rsp)
1,179 (0.0%)  	vpalignr	$4,%xmm1,%xmm2,%xmm4
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	movl	%r14d,%r8d
1,179 (0.0%)  	movl	%ebx,%r12d
1,179 (0.0%)  	vpalignr	$4,%xmm3,%xmm0,%xmm7
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	xorl	%eax,%r13d
1,179 (0.0%)  	xorl	%ecx,%r12d
1,179 (0.0%)  	vpsrld	$7,%xmm4,%xmm6
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	xorl	%r8d,%r14d
1,179 (0.0%)  	andl	%eax,%r12d
1,179 (0.0%)  	vpaddd	%xmm7,%xmm1,%xmm1
1,179 (0.0%)  	xorl	%eax,%r13d
1,179 (0.0%)  	addl	16(%rsp),%edx
1,179 (0.0%)  	movl	%r8d,%r15d
1,179 (0.0%)  	vpsrld	$3,%xmm4,%xmm7
1,179 (0.0%)  	xorl	%ecx,%r12d
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	xorl	%r9d,%r15d
1,179 (0.0%)  	vpslld	$14,%xmm4,%xmm5
1,179 (0.0%)  	addl	%r12d,%edx
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	andl	%r15d,%edi
1,179 (0.0%)  	vpxor	%xmm6,%xmm7,%xmm4
1,179 (0.0%)  	xorl	%r8d,%r14d
1,179 (0.0%)  	addl	%r13d,%edx
1,179 (0.0%)  	xorl	%r9d,%edi
1,179 (0.0%)  	vpshufd	$250,%xmm0,%xmm7
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	addl	%edx,%r11d
1,179 (0.0%)  	addl	%edi,%edx
1,179 (0.0%)  	vpsrld	$11,%xmm6,%xmm6
1,179 (0.0%)  	movl	%r11d,%r13d
1,179 (0.0%)  	addl	%edx,%r14d
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	vpxor	%xmm5,%xmm4,%xmm4
1,179 (0.0%)  	movl	%r14d,%edx
1,179 (0.0%)  	movl	%eax,%r12d
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	vpslld	$11,%xmm5,%xmm5
1,179 (0.0%)  	xorl	%r11d,%r13d
1,179 (0.0%)  	xorl	%ebx,%r12d
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	vpxor	%xmm6,%xmm4,%xmm4
1,179 (0.0%)  	xorl	%edx,%r14d
1,179 (0.0%)  	andl	%r11d,%r12d
1,179 (0.0%)  	xorl	%r11d,%r13d
1,179 (0.0%)  	vpsrld	$10,%xmm7,%xmm6
1,179 (0.0%)  	addl	20(%rsp),%ecx
1,179 (0.0%)  	movl	%edx,%edi
1,179 (0.0%)  	xorl	%ebx,%r12d
1,179 (0.0%)  	vpxor	%xmm5,%xmm4,%xmm4
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	xorl	%r8d,%edi
1,179 (0.0%)  	addl	%r12d,%ecx
1,179 (0.0%)  	vpsrlq	$17,%xmm7,%xmm7
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	andl	%edi,%r15d
1,179 (0.0%)  	xorl	%edx,%r14d
1,179 (0.0%)  	vpaddd	%xmm4,%xmm1,%xmm1
1,179 (0.0%)  	addl	%r13d,%ecx
1,179 (0.0%)  	xorl	%r8d,%r15d
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	addl	%ecx,%r10d
1,179 (0.0%)  	addl	%r15d,%ecx
1,179 (0.0%)  	movl	%r10d,%r13d
1,179 (0.0%)  	vpsrlq	$2,%xmm7,%xmm7
1,179 (0.0%)  	addl	%ecx,%r14d
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	movl	%r14d,%ecx
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	movl	%r11d,%r12d
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	xorl	%r10d,%r13d
1,179 (0.0%)  	vpshufb	%xmm8,%xmm6,%xmm6
1,179 (0.0%)  	xorl	%eax,%r12d
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	xorl	%ecx,%r14d
1,179 (0.0%)  	vpaddd	%xmm6,%xmm1,%xmm1
1,179 (0.0%)  	andl	%r10d,%r12d
1,179 (0.0%)  	xorl	%r10d,%r13d
1,179 (0.0%)  	addl	24(%rsp),%ebx
1,179 (0.0%)  	vpshufd	$80,%xmm1,%xmm7
1,179 (0.0%)  	movl	%ecx,%r15d
1,179 (0.0%)  	xorl	%eax,%r12d
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	vpsrld	$10,%xmm7,%xmm6
1,179 (0.0%)  	xorl	%edx,%r15d
1,179 (0.0%)  	addl	%r12d,%ebx
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	vpsrlq	$17,%xmm7,%xmm7
1,179 (0.0%)  	andl	%r15d,%edi
1,179 (0.0%)  	xorl	%ecx,%r14d
1,179 (0.0%)  	addl	%r13d,%ebx
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	xorl	%edx,%edi
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	addl	%ebx,%r9d
1,179 (0.0%)  	vpsrlq	$2,%xmm7,%xmm7
1,179 (0.0%)  	addl	%edi,%ebx
1,179 (0.0%)  	movl	%r9d,%r13d
1,179 (0.0%)  	addl	%ebx,%r14d
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	movl	%r14d,%ebx
1,179 (0.0%)  	movl	%r10d,%r12d
1,179 (0.0%)  	vpshufb	%xmm9,%xmm6,%xmm6
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	xorl	%r9d,%r13d
1,179 (0.0%)  	xorl	%r11d,%r12d
1,179 (0.0%)  	vpaddd	%xmm6,%xmm1,%xmm1
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	xorl	%ebx,%r14d
1,179 (0.0%)  	andl	%r9d,%r12d
1,179 (0.0%)  	vpaddd	32(%rbp),%xmm1,%xmm6
1,179 (0.0%)  	xorl	%r9d,%r13d
1,179 (0.0%)  	addl	28(%rsp),%eax
1,179 (0.0%)  	movl	%ebx,%edi
1,179 (0.0%)  	xorl	%r11d,%r12d
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	xorl	%ecx,%edi
1,179 (0.0%)  	addl	%r12d,%eax
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	andl	%edi,%r15d
1,179 (0.0%)  	xorl	%ebx,%r14d
1,179 (0.0%)  	addl	%r13d,%eax
1,179 (0.0%)  	xorl	%ecx,%r15d
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	addl	%eax,%r8d
1,179 (0.0%)  	addl	%r15d,%eax
1,179 (0.0%)  	movl	%r8d,%r13d
1,179 (0.0%)  	addl	%eax,%r14d
1,179 (0.0%)  	vmovdqa	%xmm6,16(%rsp)
1,179 (0.0%)  	vpalignr	$4,%xmm2,%xmm3,%xmm4
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	movl	%r14d,%eax
1,179 (0.0%)  	movl	%r9d,%r12d
1,179 (0.0%)  	vpalignr	$4,%xmm0,%xmm1,%xmm7
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	xorl	%r8d,%r13d
1,179 (0.0%)  	xorl	%r10d,%r12d
1,179 (0.0%)  	vpsrld	$7,%xmm4,%xmm6
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	xorl	%eax,%r14d
1,179 (0.0%)  	andl	%r8d,%r12d
1,179 (0.0%)  	vpaddd	%xmm7,%xmm2,%xmm2
1,179 (0.0%)  	xorl	%r8d,%r13d
1,179 (0.0%)  	addl	32(%rsp),%r11d
1,179 (0.0%)  	movl	%eax,%r15d
1,179 (0.0%)  	vpsrld	$3,%xmm4,%xmm7
1,179 (0.0%)  	xorl	%r10d,%r12d
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	xorl	%ebx,%r15d
1,179 (0.0%)  	vpslld	$14,%xmm4,%xmm5
1,179 (0.0%)  	addl	%r12d,%r11d
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	andl	%r15d,%edi
1,179 (0.0%)  	vpxor	%xmm6,%xmm7,%xmm4
1,179 (0.0%)  	xorl	%eax,%r14d
1,179 (0.0%)  	addl	%r13d,%r11d
1,179 (0.0%)  	xorl	%ebx,%edi
1,179 (0.0%)  	vpshufd	$250,%xmm1,%xmm7
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	addl	%r11d,%edx
1,179 (0.0%)  	addl	%edi,%r11d
1,179 (0.0%)  	vpsrld	$11,%xmm6,%xmm6
1,179 (0.0%)  	movl	%edx,%r13d
1,179 (0.0%)  	addl	%r11d,%r14d
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	vpxor	%xmm5,%xmm4,%xmm4
1,179 (0.0%)  	movl	%r14d,%r11d
1,179 (0.0%)  	movl	%r8d,%r12d
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	vpslld	$11,%xmm5,%xmm5
1,179 (0.0%)  	xorl	%edx,%r13d
1,179 (0.0%)  	xorl	%r9d,%r12d
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	vpxor	%xmm6,%xmm4,%xmm4
1,179 (0.0%)  	xorl	%r11d,%r14d
1,179 (0.0%)  	andl	%edx,%r12d
1,179 (0.0%)  	xorl	%edx,%r13d
1,179 (0.0%)  	vpsrld	$10,%xmm7,%xmm6
1,179 (0.0%)  	addl	36(%rsp),%r10d
1,179 (0.0%)  	movl	%r11d,%edi
1,179 (0.0%)  	xorl	%r9d,%r12d
1,179 (0.0%)  	vpxor	%xmm5,%xmm4,%xmm4
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	xorl	%eax,%edi
1,179 (0.0%)  	addl	%r12d,%r10d
1,179 (0.0%)  	vpsrlq	$17,%xmm7,%xmm7
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	andl	%edi,%r15d
1,179 (0.0%)  	xorl	%r11d,%r14d
1,179 (0.0%)  	vpaddd	%xmm4,%xmm2,%xmm2
1,179 (0.0%)  	addl	%r13d,%r10d
1,179 (0.0%)  	xorl	%eax,%r15d
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	addl	%r10d,%ecx
1,179 (0.0%)  	addl	%r15d,%r10d
1,179 (0.0%)  	movl	%ecx,%r13d
1,179 (0.0%)  	vpsrlq	$2,%xmm7,%xmm7
1,179 (0.0%)  	addl	%r10d,%r14d
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	movl	%r14d,%r10d
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	movl	%edx,%r12d
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	xorl	%ecx,%r13d
1,179 (0.0%)  	vpshufb	%xmm8,%xmm6,%xmm6
1,179 (0.0%)  	xorl	%r8d,%r12d
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	xorl	%r10d,%r14d
1,179 (0.0%)  	vpaddd	%xmm6,%xmm2,%xmm2
1,179 (0.0%)  	andl	%ecx,%r12d
1,179 (0.0%)  	xorl	%ecx,%r13d
1,179 (0.0%)  	addl	40(%rsp),%r9d
1,179 (0.0%)  	vpshufd	$80,%xmm2,%xmm7
1,179 (0.0%)  	movl	%r10d,%r15d
1,179 (0.0%)  	xorl	%r8d,%r12d
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	vpsrld	$10,%xmm7,%xmm6
1,179 (0.0%)  	xorl	%r11d,%r15d
1,179 (0.0%)  	addl	%r12d,%r9d
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	vpsrlq	$17,%xmm7,%xmm7
1,179 (0.0%)  	andl	%r15d,%edi
1,179 (0.0%)  	xorl	%r10d,%r14d
1,179 (0.0%)  	addl	%r13d,%r9d
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	xorl	%r11d,%edi
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	addl	%r9d,%ebx
1,179 (0.0%)  	vpsrlq	$2,%xmm7,%xmm7
1,179 (0.0%)  	addl	%edi,%r9d
1,179 (0.0%)  	movl	%ebx,%r13d
1,179 (0.0%)  	addl	%r9d,%r14d
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	movl	%r14d,%r9d
1,179 (0.0%)  	movl	%ecx,%r12d
1,179 (0.0%)  	vpshufb	%xmm9,%xmm6,%xmm6
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	xorl	%ebx,%r13d
1,179 (0.0%)  	xorl	%edx,%r12d
1,179 (0.0%)  	vpaddd	%xmm6,%xmm2,%xmm2
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	xorl	%r9d,%r14d
1,179 (0.0%)  	andl	%ebx,%r12d
1,179 (0.0%)  	vpaddd	64(%rbp),%xmm2,%xmm6
1,179 (0.0%)  	xorl	%ebx,%r13d
1,179 (0.0%)  	addl	44(%rsp),%r8d
1,179 (0.0%)  	movl	%r9d,%edi
1,179 (0.0%)  	xorl	%edx,%r12d
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	xorl	%r10d,%edi
1,179 (0.0%)  	addl	%r12d,%r8d
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	andl	%edi,%r15d
1,179 (0.0%)  	xorl	%r9d,%r14d
1,179 (0.0%)  	addl	%r13d,%r8d
1,179 (0.0%)  	xorl	%r10d,%r15d
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	addl	%r8d,%eax
1,179 (0.0%)  	addl	%r15d,%r8d
1,179 (0.0%)  	movl	%eax,%r13d
1,179 (0.0%)  	addl	%r8d,%r14d
1,179 (0.0%)  	vmovdqa	%xmm6,32(%rsp)
1,179 (0.0%)  	vpalignr	$4,%xmm3,%xmm0,%xmm4
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	movl	%r14d,%r8d
1,179 (0.0%)  	movl	%ebx,%r12d
1,179 (0.0%)  	vpalignr	$4,%xmm1,%xmm2,%xmm7
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	xorl	%eax,%r13d
1,179 (0.0%)  	xorl	%ecx,%r12d
1,179 (0.0%)  	vpsrld	$7,%xmm4,%xmm6
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	xorl	%r8d,%r14d
1,179 (0.0%)  	andl	%eax,%r12d
1,179 (0.0%)  	vpaddd	%xmm7,%xmm3,%xmm3
1,179 (0.0%)  	xorl	%eax,%r13d
1,179 (0.0%)  	addl	48(%rsp),%edx
1,179 (0.0%)  	movl	%r8d,%r15d
1,179 (0.0%)  	vpsrld	$3,%xmm4,%xmm7
1,179 (0.0%)  	xorl	%ecx,%r12d
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	xorl	%r9d,%r15d
1,179 (0.0%)  	vpslld	$14,%xmm4,%xmm5
1,179 (0.0%)  	addl	%r12d,%edx
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	andl	%r15d,%edi
1,179 (0.0%)  	vpxor	%xmm6,%xmm7,%xmm4
1,179 (0.0%)  	xorl	%r8d,%r14d
1,179 (0.0%)  	addl	%r13d,%edx
1,179 (0.0%)  	xorl	%r9d,%edi
1,179 (0.0%)  	vpshufd	$250,%xmm2,%xmm7
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	addl	%edx,%r11d
1,179 (0.0%)  	addl	%edi,%edx
1,179 (0.0%)  	vpsrld	$11,%xmm6,%xmm6
1,179 (0.0%)  	movl	%r11d,%r13d
1,179 (0.0%)  	addl	%edx,%r14d
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	vpxor	%xmm5,%xmm4,%xmm4
1,179 (0.0%)  	movl	%r14d,%edx
1,179 (0.0%)  	movl	%eax,%r12d
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	vpslld	$11,%xmm5,%xmm5
1,179 (0.0%)  	xorl	%r11d,%r13d
1,179 (0.0%)  	xorl	%ebx,%r12d
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	vpxor	%xmm6,%xmm4,%xmm4
1,179 (0.0%)  	xorl	%edx,%r14d
1,179 (0.0%)  	andl	%r11d,%r12d
1,179 (0.0%)  	xorl	%r11d,%r13d
1,179 (0.0%)  	vpsrld	$10,%xmm7,%xmm6
1,179 (0.0%)  	addl	52(%rsp),%ecx
1,179 (0.0%)  	movl	%edx,%edi
1,179 (0.0%)  	xorl	%ebx,%r12d
1,179 (0.0%)  	vpxor	%xmm5,%xmm4,%xmm4
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	xorl	%r8d,%edi
1,179 (0.0%)  	addl	%r12d,%ecx
1,179 (0.0%)  	vpsrlq	$17,%xmm7,%xmm7
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	andl	%edi,%r15d
1,179 (0.0%)  	xorl	%edx,%r14d
1,179 (0.0%)  	vpaddd	%xmm4,%xmm3,%xmm3
1,179 (0.0%)  	addl	%r13d,%ecx
1,179 (0.0%)  	xorl	%r8d,%r15d
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	addl	%ecx,%r10d
1,179 (0.0%)  	addl	%r15d,%ecx
1,179 (0.0%)  	movl	%r10d,%r13d
1,179 (0.0%)  	vpsrlq	$2,%xmm7,%xmm7
1,179 (0.0%)  	addl	%ecx,%r14d
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	movl	%r14d,%ecx
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	movl	%r11d,%r12d
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	xorl	%r10d,%r13d
1,179 (0.0%)  	vpshufb	%xmm8,%xmm6,%xmm6
1,179 (0.0%)  	xorl	%eax,%r12d
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	xorl	%ecx,%r14d
1,179 (0.0%)  	vpaddd	%xmm6,%xmm3,%xmm3
1,179 (0.0%)  	andl	%r10d,%r12d
1,179 (0.0%)  	xorl	%r10d,%r13d
1,179 (0.0%)  	addl	56(%rsp),%ebx
1,179 (0.0%)  	vpshufd	$80,%xmm3,%xmm7
1,179 (0.0%)  	movl	%ecx,%r15d
1,179 (0.0%)  	xorl	%eax,%r12d
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	vpsrld	$10,%xmm7,%xmm6
1,179 (0.0%)  	xorl	%edx,%r15d
1,179 (0.0%)  	addl	%r12d,%ebx
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	vpsrlq	$17,%xmm7,%xmm7
1,179 (0.0%)  	andl	%r15d,%edi
1,179 (0.0%)  	xorl	%ecx,%r14d
1,179 (0.0%)  	addl	%r13d,%ebx
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	xorl	%edx,%edi
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	addl	%ebx,%r9d
1,179 (0.0%)  	vpsrlq	$2,%xmm7,%xmm7
1,179 (0.0%)  	addl	%edi,%ebx
1,179 (0.0%)  	movl	%r9d,%r13d
1,179 (0.0%)  	addl	%ebx,%r14d
1,179 (0.0%)  	vpxor	%xmm7,%xmm6,%xmm6
1,179 (0.0%)  	shrdl	$14,%r13d,%r13d
1,179 (0.0%)  	movl	%r14d,%ebx
1,179 (0.0%)  	movl	%r10d,%r12d
1,179 (0.0%)  	vpshufb	%xmm9,%xmm6,%xmm6
1,179 (0.0%)  	shrdl	$9,%r14d,%r14d
1,179 (0.0%)  	xorl	%r9d,%r13d
1,179 (0.0%)  	xorl	%r11d,%r12d
1,179 (0.0%)  	vpaddd	%xmm6,%xmm3,%xmm3
1,179 (0.0%)  	shrdl	$5,%r13d,%r13d
1,179 (0.0%)  	xorl	%ebx,%r14d
1,179 (0.0%)  	andl	%r9d,%r12d
1,179 (0.0%)  	vpaddd	96(%rbp),%xmm3,%xmm6
1,179 (0.0%)  	xorl	%r9d,%r13d
1,179 (0.0%)  	addl	60(%rsp),%eax
1,179 (0.0%)  	movl	%ebx,%edi
1,179 (0.0%)  	xorl	%r11d,%r12d
1,179 (0.0%)  	shrdl	$11,%r14d,%r14d
1,179 (0.0%)  	xorl	%ecx,%edi
1,179 (0.0%)  	addl	%r12d,%eax
1,179 (0.0%)  	shrdl	$6,%r13d,%r13d
1,179 (0.0%)  	andl	%edi,%r15d
1,179 (0.0%)  	xorl	%ebx,%r14d
1,179 (0.0%)  	addl	%r13d,%eax
1,179 (0.0%)  	xorl	%ecx,%r15d
1,179 (0.0%)  	shrdl	$2,%r14d,%r14d
1,179 (0.0%)  	addl	%eax,%r8d
1,179 (0.0%)  	addl	%r15d,%eax
1,179 (0.0%)  	movl	%r8d,%r13d
1,179 (0.0%)  	addl	%eax,%r14d
1,179 (0.0%)  	vmovdqa	%xmm6,48(%rsp)
1,179 (0.0%)  	cmpb	$0,131(%rbp)
1,179 (0.0%)  	jne	.Lavx_00_47
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%eax
  393 (0.0%)  	movl	%r9d,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%r8d,%r13d
  393 (0.0%)  	xorl	%r10d,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%eax,%r14d
  393 (0.0%)  	andl	%r8d,%r12d
  393 (0.0%)  	xorl	%r8d,%r13d
  393 (0.0%)  	addl	0(%rsp),%r11d
  393 (0.0%)  	movl	%eax,%r15d
  393 (0.0%)  	xorl	%r10d,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%ebx,%r15d
  393 (0.0%)  	addl	%r12d,%r11d
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%r15d,%edi
  393 (0.0%)  	xorl	%eax,%r14d
  393 (0.0%)  	addl	%r13d,%r11d
  393 (0.0%)  	xorl	%ebx,%edi
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%r11d,%edx
  393 (0.0%)  	addl	%edi,%r11d
  393 (0.0%)  	movl	%edx,%r13d
  393 (0.0%)  	addl	%r11d,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%r11d
  393 (0.0%)  	movl	%r8d,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%edx,%r13d
  393 (0.0%)  	xorl	%r9d,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%r11d,%r14d
  393 (0.0%)  	andl	%edx,%r12d
  393 (0.0%)  	xorl	%edx,%r13d
  393 (0.0%)  	addl	4(%rsp),%r10d
  393 (0.0%)  	movl	%r11d,%edi
  393 (0.0%)  	xorl	%r9d,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%eax,%edi
  393 (0.0%)  	addl	%r12d,%r10d
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%edi,%r15d
  393 (0.0%)  	xorl	%r11d,%r14d
  393 (0.0%)  	addl	%r13d,%r10d
  393 (0.0%)  	xorl	%eax,%r15d
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%r10d,%ecx
  393 (0.0%)  	addl	%r15d,%r10d
  393 (0.0%)  	movl	%ecx,%r13d
  393 (0.0%)  	addl	%r10d,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%r10d
  393 (0.0%)  	movl	%edx,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%ecx,%r13d
  393 (0.0%)  	xorl	%r8d,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%r10d,%r14d
  393 (0.0%)  	andl	%ecx,%r12d
  393 (0.0%)  	xorl	%ecx,%r13d
  393 (0.0%)  	addl	8(%rsp),%r9d
  393 (0.0%)  	movl	%r10d,%r15d
  393 (0.0%)  	xorl	%r8d,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%r11d,%r15d
  393 (0.0%)  	addl	%r12d,%r9d
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%r15d,%edi
  393 (0.0%)  	xorl	%r10d,%r14d
  393 (0.0%)  	addl	%r13d,%r9d
  393 (0.0%)  	xorl	%r11d,%edi
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%r9d,%ebx
  393 (0.0%)  	addl	%edi,%r9d
  393 (0.0%)  	movl	%ebx,%r13d
  393 (0.0%)  	addl	%r9d,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%r9d
  393 (0.0%)  	movl	%ecx,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%ebx,%r13d
  393 (0.0%)  	xorl	%edx,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%r9d,%r14d
  393 (0.0%)  	andl	%ebx,%r12d
  393 (0.0%)  	xorl	%ebx,%r13d
  393 (0.0%)  	addl	12(%rsp),%r8d
  393 (0.0%)  	movl	%r9d,%edi
  393 (0.0%)  	xorl	%edx,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%r10d,%edi
  393 (0.0%)  	addl	%r12d,%r8d
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%edi,%r15d
  393 (0.0%)  	xorl	%r9d,%r14d
  393 (0.0%)  	addl	%r13d,%r8d
  393 (0.0%)  	xorl	%r10d,%r15d
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%r8d,%eax
  393 (0.0%)  	addl	%r15d,%r8d
  393 (0.0%)  	movl	%eax,%r13d
  393 (0.0%)  	addl	%r8d,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%r8d
  393 (0.0%)  	movl	%ebx,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%eax,%r13d
  393 (0.0%)  	xorl	%ecx,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%r8d,%r14d
  393 (0.0%)  	andl	%eax,%r12d
  393 (0.0%)  	xorl	%eax,%r13d
  393 (0.0%)  	addl	16(%rsp),%edx
  393 (0.0%)  	movl	%r8d,%r15d
  393 (0.0%)  	xorl	%ecx,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%r9d,%r15d
  393 (0.0%)  	addl	%r12d,%edx
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%r15d,%edi
  393 (0.0%)  	xorl	%r8d,%r14d
  393 (0.0%)  	addl	%r13d,%edx
  393 (0.0%)  	xorl	%r9d,%edi
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%edx,%r11d
  393 (0.0%)  	addl	%edi,%edx
  393 (0.0%)  	movl	%r11d,%r13d
  393 (0.0%)  	addl	%edx,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%edx
  393 (0.0%)  	movl	%eax,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%r11d,%r13d
  393 (0.0%)  	xorl	%ebx,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%edx,%r14d
  393 (0.0%)  	andl	%r11d,%r12d
  393 (0.0%)  	xorl	%r11d,%r13d
  393 (0.0%)  	addl	20(%rsp),%ecx
  393 (0.0%)  	movl	%edx,%edi
  393 (0.0%)  	xorl	%ebx,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%r8d,%edi
  393 (0.0%)  	addl	%r12d,%ecx
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%edi,%r15d
  393 (0.0%)  	xorl	%edx,%r14d
  393 (0.0%)  	addl	%r13d,%ecx
  393 (0.0%)  	xorl	%r8d,%r15d
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%ecx,%r10d
  393 (0.0%)  	addl	%r15d,%ecx
  393 (0.0%)  	movl	%r10d,%r13d
  393 (0.0%)  	addl	%ecx,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%ecx
  393 (0.0%)  	movl	%r11d,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%r10d,%r13d
  393 (0.0%)  	xorl	%eax,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%ecx,%r14d
  393 (0.0%)  	andl	%r10d,%r12d
  393 (0.0%)  	xorl	%r10d,%r13d
  393 (0.0%)  	addl	24(%rsp),%ebx
  393 (0.0%)  	movl	%ecx,%r15d
  393 (0.0%)  	xorl	%eax,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%edx,%r15d
  393 (0.0%)  	addl	%r12d,%ebx
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%r15d,%edi
  393 (0.0%)  	xorl	%ecx,%r14d
  393 (0.0%)  	addl	%r13d,%ebx
  393 (0.0%)  	xorl	%edx,%edi
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%ebx,%r9d
  393 (0.0%)  	addl	%edi,%ebx
  393 (0.0%)  	movl	%r9d,%r13d
  393 (0.0%)  	addl	%ebx,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%ebx
  393 (0.0%)  	movl	%r10d,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%r9d,%r13d
  393 (0.0%)  	xorl	%r11d,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%ebx,%r14d
  393 (0.0%)  	andl	%r9d,%r12d
  393 (0.0%)  	xorl	%r9d,%r13d
  393 (0.0%)  	addl	28(%rsp),%eax
  393 (0.0%)  	movl	%ebx,%edi
  393 (0.0%)  	xorl	%r11d,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%ecx,%edi
  393 (0.0%)  	addl	%r12d,%eax
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%edi,%r15d
  393 (0.0%)  	xorl	%ebx,%r14d
  393 (0.0%)  	addl	%r13d,%eax
  393 (0.0%)  	xorl	%ecx,%r15d
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%eax,%r8d
  393 (0.0%)  	addl	%r15d,%eax
  393 (0.0%)  	movl	%r8d,%r13d
  393 (0.0%)  	addl	%eax,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%eax
  393 (0.0%)  	movl	%r9d,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%r8d,%r13d
  393 (0.0%)  	xorl	%r10d,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%eax,%r14d
  393 (0.0%)  	andl	%r8d,%r12d
  393 (0.0%)  	xorl	%r8d,%r13d
  393 (0.0%)  	addl	32(%rsp),%r11d
  393 (0.0%)  	movl	%eax,%r15d
  393 (0.0%)  	xorl	%r10d,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%ebx,%r15d
  393 (0.0%)  	addl	%r12d,%r11d
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%r15d,%edi
  393 (0.0%)  	xorl	%eax,%r14d
  393 (0.0%)  	addl	%r13d,%r11d
  393 (0.0%)  	xorl	%ebx,%edi
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%r11d,%edx
  393 (0.0%)  	addl	%edi,%r11d
  393 (0.0%)  	movl	%edx,%r13d
  393 (0.0%)  	addl	%r11d,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%r11d
  393 (0.0%)  	movl	%r8d,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%edx,%r13d
  393 (0.0%)  	xorl	%r9d,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%r11d,%r14d
  393 (0.0%)  	andl	%edx,%r12d
  393 (0.0%)  	xorl	%edx,%r13d
  393 (0.0%)  	addl	36(%rsp),%r10d
  393 (0.0%)  	movl	%r11d,%edi
  393 (0.0%)  	xorl	%r9d,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%eax,%edi
  393 (0.0%)  	addl	%r12d,%r10d
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%edi,%r15d
  393 (0.0%)  	xorl	%r11d,%r14d
  393 (0.0%)  	addl	%r13d,%r10d
  393 (0.0%)  	xorl	%eax,%r15d
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%r10d,%ecx
  393 (0.0%)  	addl	%r15d,%r10d
  393 (0.0%)  	movl	%ecx,%r13d
  393 (0.0%)  	addl	%r10d,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%r10d
  393 (0.0%)  	movl	%edx,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%ecx,%r13d
  393 (0.0%)  	xorl	%r8d,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%r10d,%r14d
  393 (0.0%)  	andl	%ecx,%r12d
  393 (0.0%)  	xorl	%ecx,%r13d
  393 (0.0%)  	addl	40(%rsp),%r9d
  393 (0.0%)  	movl	%r10d,%r15d
  393 (0.0%)  	xorl	%r8d,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%r11d,%r15d
  393 (0.0%)  	addl	%r12d,%r9d
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%r15d,%edi
  393 (0.0%)  	xorl	%r10d,%r14d
  393 (0.0%)  	addl	%r13d,%r9d
  393 (0.0%)  	xorl	%r11d,%edi
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%r9d,%ebx
  393 (0.0%)  	addl	%edi,%r9d
  393 (0.0%)  	movl	%ebx,%r13d
  393 (0.0%)  	addl	%r9d,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%r9d
  393 (0.0%)  	movl	%ecx,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%ebx,%r13d
  393 (0.0%)  	xorl	%edx,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%r9d,%r14d
  393 (0.0%)  	andl	%ebx,%r12d
  393 (0.0%)  	xorl	%ebx,%r13d
  393 (0.0%)  	addl	44(%rsp),%r8d
  393 (0.0%)  	movl	%r9d,%edi
  393 (0.0%)  	xorl	%edx,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%r10d,%edi
  393 (0.0%)  	addl	%r12d,%r8d
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%edi,%r15d
  393 (0.0%)  	xorl	%r9d,%r14d
  393 (0.0%)  	addl	%r13d,%r8d
  393 (0.0%)  	xorl	%r10d,%r15d
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%r8d,%eax
  393 (0.0%)  	addl	%r15d,%r8d
  393 (0.0%)  	movl	%eax,%r13d
  393 (0.0%)  	addl	%r8d,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%r8d
  393 (0.0%)  	movl	%ebx,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%eax,%r13d
  393 (0.0%)  	xorl	%ecx,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%r8d,%r14d
  393 (0.0%)  	andl	%eax,%r12d
  393 (0.0%)  	xorl	%eax,%r13d
  393 (0.0%)  	addl	48(%rsp),%edx
  393 (0.0%)  	movl	%r8d,%r15d
  393 (0.0%)  	xorl	%ecx,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%r9d,%r15d
  393 (0.0%)  	addl	%r12d,%edx
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%r15d,%edi
  393 (0.0%)  	xorl	%r8d,%r14d
  393 (0.0%)  	addl	%r13d,%edx
  393 (0.0%)  	xorl	%r9d,%edi
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%edx,%r11d
  393 (0.0%)  	addl	%edi,%edx
  393 (0.0%)  	movl	%r11d,%r13d
  393 (0.0%)  	addl	%edx,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%edx
  393 (0.0%)  	movl	%eax,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%r11d,%r13d
  393 (0.0%)  	xorl	%ebx,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%edx,%r14d
  393 (0.0%)  	andl	%r11d,%r12d
  393 (0.0%)  	xorl	%r11d,%r13d
  393 (0.0%)  	addl	52(%rsp),%ecx
  393 (0.0%)  	movl	%edx,%edi
  393 (0.0%)  	xorl	%ebx,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%r8d,%edi
  393 (0.0%)  	addl	%r12d,%ecx
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%edi,%r15d
  393 (0.0%)  	xorl	%edx,%r14d
  393 (0.0%)  	addl	%r13d,%ecx
  393 (0.0%)  	xorl	%r8d,%r15d
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%ecx,%r10d
  393 (0.0%)  	addl	%r15d,%ecx
  393 (0.0%)  	movl	%r10d,%r13d
  393 (0.0%)  	addl	%ecx,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%ecx
  393 (0.0%)  	movl	%r11d,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%r10d,%r13d
  393 (0.0%)  	xorl	%eax,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%ecx,%r14d
  393 (0.0%)  	andl	%r10d,%r12d
  393 (0.0%)  	xorl	%r10d,%r13d
  393 (0.0%)  	addl	56(%rsp),%ebx
  393 (0.0%)  	movl	%ecx,%r15d
  393 (0.0%)  	xorl	%eax,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%edx,%r15d
  393 (0.0%)  	addl	%r12d,%ebx
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%r15d,%edi
  393 (0.0%)  	xorl	%ecx,%r14d
  393 (0.0%)  	addl	%r13d,%ebx
  393 (0.0%)  	xorl	%edx,%edi
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%ebx,%r9d
  393 (0.0%)  	addl	%edi,%ebx
  393 (0.0%)  	movl	%r9d,%r13d
  393 (0.0%)  	addl	%ebx,%r14d
  393 (0.0%)  	shrdl	$14,%r13d,%r13d
  393 (0.0%)  	movl	%r14d,%ebx
  393 (0.0%)  	movl	%r10d,%r12d
  393 (0.0%)  	shrdl	$9,%r14d,%r14d
  393 (0.0%)  	xorl	%r9d,%r13d
  393 (0.0%)  	xorl	%r11d,%r12d
  393 (0.0%)  	shrdl	$5,%r13d,%r13d
  393 (0.0%)  	xorl	%ebx,%r14d
  393 (0.0%)  	andl	%r9d,%r12d
  393 (0.0%)  	xorl	%r9d,%r13d
  393 (0.0%)  	addl	60(%rsp),%eax
  393 (0.0%)  	movl	%ebx,%edi
  393 (0.0%)  	xorl	%r11d,%r12d
  393 (0.0%)  	shrdl	$11,%r14d,%r14d
  393 (0.0%)  	xorl	%ecx,%edi
  393 (0.0%)  	addl	%r12d,%eax
  393 (0.0%)  	shrdl	$6,%r13d,%r13d
  393 (0.0%)  	andl	%edi,%r15d
  393 (0.0%)  	xorl	%ebx,%r14d
  393 (0.0%)  	addl	%r13d,%eax
  393 (0.0%)  	xorl	%ecx,%r15d
  393 (0.0%)  	shrdl	$2,%r14d,%r14d
  393 (0.0%)  	addl	%eax,%r8d
  393 (0.0%)  	addl	%r15d,%eax
  393 (0.0%)  	movl	%r8d,%r13d
  393 (0.0%)  	addl	%eax,%r14d
  393 (0.0%)  	movq	64+0(%rsp),%rdi
  393 (0.0%)  	movl	%r14d,%eax
    .         
  393 (0.0%)  	addl	0(%rdi),%eax
  393 (0.0%)  	leaq	64(%rsi),%rsi
  393 (0.0%)  	addl	4(%rdi),%ebx
  393 (0.0%)  	addl	8(%rdi),%ecx
  393 (0.0%)  	addl	12(%rdi),%edx
  393 (0.0%)  	addl	16(%rdi),%r8d
  393 (0.0%)  	addl	20(%rdi),%r9d
  393 (0.0%)  	addl	24(%rdi),%r10d
  393 (0.0%)  	addl	28(%rdi),%r11d
    .         
  393 (0.0%)  	cmpq	64+16(%rsp),%rsi
    .         
  393 (0.0%)  	movl	%eax,0(%rdi)
  393 (0.0%)  	movl	%ebx,4(%rdi)
  393 (0.0%)  	movl	%ecx,8(%rdi)
  393 (0.0%)  	movl	%edx,12(%rdi)
  393 (0.0%)  	movl	%r8d,16(%rdi)
  393 (0.0%)  	movl	%r9d,20(%rdi)
  393 (0.0%)  	movl	%r10d,24(%rdi)
  393 (0.0%)  	movl	%r11d,28(%rdi)
  393 (0.0%)  	jb	.Lloop_avx
    .         
  262 (0.0%)  	movq	88(%rsp),%rsi
    .         .cfi_def_cfa	%rsi,8
  262 (0.0%)  	vzeroupper
  262 (0.0%)  	movq	-48(%rsi),%r15
    .         .cfi_restore	%r15
  262 (0.0%)  	movq	-40(%rsi),%r14
    .         .cfi_restore	%r14
  262 (0.0%)  	movq	-32(%rsi),%r13
    .         .cfi_restore	%r13
  262 (0.0%)  	movq	-24(%rsi),%r12
    .         .cfi_restore	%r12
  262 (0.0%)  	movq	-16(%rsi),%rbp
    .         .cfi_restore	%rbp
  262 (0.0%)  	movq	-8(%rsi),%rbx
    .         .cfi_restore	%rbx
  524 (0.0%)  	leaq	(%rsi),%rsp
    .         .cfi_def_cfa_register	%rsp
    .         .Lepilogue_avx:
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	sha256_block_data_order_avx,.-sha256_block_data_order_avx
    .         #endif

--------------------------------------------------------------------------------
-- Annotated source file: /home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/sha512-x86_64.S
--------------------------------------------------------------------------------
Ir________ 

-- line 9 ----------------------------------------
  .         .extern	OPENSSL_ia32cap_P
  .         .hidden OPENSSL_ia32cap_P
  .         .globl	sha512_block_data_order
  .         .hidden sha512_block_data_order
  .         .type	sha512_block_data_order,@function
  .         .align	16
  .         sha512_block_data_order:
  .         .cfi_startproc	
 14 (0.0%)  	leaq	OPENSSL_ia32cap_P(%rip),%r11
 14 (0.0%)  	movl	0(%r11),%r9d
 14 (0.0%)  	movl	4(%r11),%r10d
 14 (0.0%)  	movl	8(%r11),%r11d
 14 (0.0%)  	andl	$1073741824,%r9d
 14 (0.0%)  	andl	$268435968,%r10d
 14 (0.0%)  	orl	%r9d,%r10d
 14 (0.0%)  	cmpl	$1342177792,%r10d
 14 (0.0%)  	je	.Lavx_shortcut
  .         	movq	%rsp,%rax
  .         .cfi_def_cfa_register	%rax
  .         	pushq	%rbx
  .         .cfi_offset	%rbx,-16
  .         	pushq	%rbp
  .         .cfi_offset	%rbp,-24
  .         	pushq	%r12
  .         .cfi_offset	%r12,-32
-- line 33 ----------------------------------------
-- line 1818 ----------------------------------------
  .         .quad	0x0001020304050607,0x08090a0b0c0d0e0f
  .         .byte	83,72,65,53,49,50,32,98,108,111,99,107,32,116,114,97,110,115,102,111,114,109,32,102,111,114,32,120,56,54,95,54,52,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
  .         .text	
  .         .type	sha512_block_data_order_avx,@function
  .         .align	64
  .         sha512_block_data_order_avx:
  .         .cfi_startproc	
  .         .Lavx_shortcut:
 14 (0.0%)  	movq	%rsp,%rax
  .         .cfi_def_cfa_register	%rax
 14 (0.0%)  	pushq	%rbx
  .         .cfi_offset	%rbx,-16
 14 (0.0%)  	pushq	%rbp
  .         .cfi_offset	%rbp,-24
 14 (0.0%)  	pushq	%r12
  .         .cfi_offset	%r12,-32
 14 (0.0%)  	pushq	%r13
  .         .cfi_offset	%r13,-40
 14 (0.0%)  	pushq	%r14
  .         .cfi_offset	%r14,-48
 14 (0.0%)  	pushq	%r15
  .         .cfi_offset	%r15,-56
 14 (0.0%)  	shlq	$4,%rdx
 14 (0.0%)  	subq	$160,%rsp
 14 (0.0%)  	leaq	(%rsi,%rdx,8),%rdx
 14 (0.0%)  	andq	$-64,%rsp
 14 (0.0%)  	movq	%rdi,128+0(%rsp)
 14 (0.0%)  	movq	%rsi,128+8(%rsp)
 14 (0.0%)  	movq	%rdx,128+16(%rsp)
 14 (0.0%)  	movq	%rax,152(%rsp)
  .         .cfi_escape	0x0f,0x06,0x77,0x98,0x01,0x06,0x23,0x08
  .         .Lprologue_avx:
  .         
 14 (0.0%)  	vzeroupper
 14 (0.0%)  	movq	0(%rdi),%rax
 14 (0.0%)  	movq	8(%rdi),%rbx
 14 (0.0%)  	movq	16(%rdi),%rcx
 14 (0.0%)  	movq	24(%rdi),%rdx
 14 (0.0%)  	movq	32(%rdi),%r8
 14 (0.0%)  	movq	40(%rdi),%r9
 14 (0.0%)  	movq	48(%rdi),%r10
 14 (0.0%)  	movq	56(%rdi),%r11
 14 (0.0%)  	jmp	.Lloop_avx
  .         .align	16
  .         .Lloop_avx:
 36 (0.0%)  	vmovdqa	K512+1280(%rip),%xmm11
 36 (0.0%)  	vmovdqu	0(%rsi),%xmm0
 36 (0.0%)  	leaq	K512+128(%rip),%rbp
 36 (0.0%)  	vmovdqu	16(%rsi),%xmm1
 36 (0.0%)  	vmovdqu	32(%rsi),%xmm2
 36 (0.0%)  	vpshufb	%xmm11,%xmm0,%xmm0
 36 (0.0%)  	vmovdqu	48(%rsi),%xmm3
 36 (0.0%)  	vpshufb	%xmm11,%xmm1,%xmm1
 36 (0.0%)  	vmovdqu	64(%rsi),%xmm4
 36 (0.0%)  	vpshufb	%xmm11,%xmm2,%xmm2
 36 (0.0%)  	vmovdqu	80(%rsi),%xmm5
 36 (0.0%)  	vpshufb	%xmm11,%xmm3,%xmm3
 36 (0.0%)  	vmovdqu	96(%rsi),%xmm6
 36 (0.0%)  	vpshufb	%xmm11,%xmm4,%xmm4
 36 (0.0%)  	vmovdqu	112(%rsi),%xmm7
 36 (0.0%)  	vpshufb	%xmm11,%xmm5,%xmm5
 36 (0.0%)  	vpaddq	-128(%rbp),%xmm0,%xmm8
 36 (0.0%)  	vpshufb	%xmm11,%xmm6,%xmm6
 36 (0.0%)  	vpaddq	-96(%rbp),%xmm1,%xmm9
 36 (0.0%)  	vpshufb	%xmm11,%xmm7,%xmm7
 36 (0.0%)  	vpaddq	-64(%rbp),%xmm2,%xmm10
 36 (0.0%)  	vpaddq	-32(%rbp),%xmm3,%xmm11
 36 (0.0%)  	vmovdqa	%xmm8,0(%rsp)
 36 (0.0%)  	vpaddq	0(%rbp),%xmm4,%xmm8
 36 (0.0%)  	vmovdqa	%xmm9,16(%rsp)
 36 (0.0%)  	vpaddq	32(%rbp),%xmm5,%xmm9
 36 (0.0%)  	vmovdqa	%xmm10,32(%rsp)
 36 (0.0%)  	vpaddq	64(%rbp),%xmm6,%xmm10
 36 (0.0%)  	vmovdqa	%xmm11,48(%rsp)
 36 (0.0%)  	vpaddq	96(%rbp),%xmm7,%xmm11
 36 (0.0%)  	vmovdqa	%xmm8,64(%rsp)
 36 (0.0%)  	movq	%rax,%r14
 36 (0.0%)  	vmovdqa	%xmm9,80(%rsp)
 36 (0.0%)  	movq	%rbx,%rdi
 36 (0.0%)  	vmovdqa	%xmm10,96(%rsp)
 36 (0.0%)  	xorq	%rcx,%rdi
 36 (0.0%)  	vmovdqa	%xmm11,112(%rsp)
 36 (0.0%)  	movq	%r8,%r13
 36 (0.0%)  	jmp	.Lavx_00_47
  .         
  .         .align	16
  .         .Lavx_00_47:
144 (0.0%)  	addq	$256,%rbp
144 (0.0%)  	vpalignr	$8,%xmm0,%xmm1,%xmm8
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%rax
144 (0.0%)  	vpalignr	$8,%xmm4,%xmm5,%xmm11
144 (0.0%)  	movq	%r9,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$1,%xmm8,%xmm10
144 (0.0%)  	xorq	%r8,%r13
144 (0.0%)  	xorq	%r10,%r12
144 (0.0%)  	vpaddq	%xmm11,%xmm0,%xmm0
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%rax,%r14
144 (0.0%)  	vpsrlq	$7,%xmm8,%xmm11
144 (0.0%)  	andq	%r8,%r12
144 (0.0%)  	xorq	%r8,%r13
144 (0.0%)  	vpsllq	$56,%xmm8,%xmm9
144 (0.0%)  	addq	0(%rsp),%r11
144 (0.0%)  	movq	%rax,%r15
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm8
144 (0.0%)  	xorq	%r10,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpsrlq	$7,%xmm10,%xmm10
144 (0.0%)  	xorq	%rbx,%r15
144 (0.0%)  	addq	%r12,%r11
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%r15,%rdi
144 (0.0%)  	vpsllq	$7,%xmm9,%xmm9
144 (0.0%)  	xorq	%rax,%r14
144 (0.0%)  	addq	%r13,%r11
144 (0.0%)  	vpxor	%xmm10,%xmm8,%xmm8
144 (0.0%)  	xorq	%rbx,%rdi
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	vpsrlq	$6,%xmm7,%xmm11
144 (0.0%)  	addq	%r11,%rdx
144 (0.0%)  	addq	%rdi,%r11
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	movq	%rdx,%r13
144 (0.0%)  	addq	%r11,%r14
144 (0.0%)  	vpsllq	$3,%xmm7,%xmm10
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%r11
144 (0.0%)  	vpaddq	%xmm8,%xmm0,%xmm0
144 (0.0%)  	movq	%r8,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$19,%xmm7,%xmm9
144 (0.0%)  	xorq	%rdx,%r13
144 (0.0%)  	xorq	%r9,%r12
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%r11,%r14
144 (0.0%)  	vpsllq	$42,%xmm10,%xmm10
144 (0.0%)  	andq	%rdx,%r12
144 (0.0%)  	xorq	%rdx,%r13
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	addq	8(%rsp),%r10
144 (0.0%)  	movq	%r11,%rdi
144 (0.0%)  	vpsrlq	$42,%xmm9,%xmm9
144 (0.0%)  	xorq	%r9,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	xorq	%rax,%rdi
144 (0.0%)  	addq	%r12,%r10
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%rdi,%r15
144 (0.0%)  	vpaddq	%xmm11,%xmm0,%xmm0
144 (0.0%)  	xorq	%r11,%r14
144 (0.0%)  	addq	%r13,%r10
144 (0.0%)  	vpaddq	-128(%rbp),%xmm0,%xmm10
144 (0.0%)  	xorq	%rax,%r15
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	addq	%r10,%rcx
144 (0.0%)  	addq	%r15,%r10
144 (0.0%)  	movq	%rcx,%r13
144 (0.0%)  	addq	%r10,%r14
144 (0.0%)  	vmovdqa	%xmm10,0(%rsp)
144 (0.0%)  	vpalignr	$8,%xmm1,%xmm2,%xmm8
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%r10
144 (0.0%)  	vpalignr	$8,%xmm5,%xmm6,%xmm11
144 (0.0%)  	movq	%rdx,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$1,%xmm8,%xmm10
144 (0.0%)  	xorq	%rcx,%r13
144 (0.0%)  	xorq	%r8,%r12
144 (0.0%)  	vpaddq	%xmm11,%xmm1,%xmm1
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%r10,%r14
144 (0.0%)  	vpsrlq	$7,%xmm8,%xmm11
144 (0.0%)  	andq	%rcx,%r12
144 (0.0%)  	xorq	%rcx,%r13
144 (0.0%)  	vpsllq	$56,%xmm8,%xmm9
144 (0.0%)  	addq	16(%rsp),%r9
144 (0.0%)  	movq	%r10,%r15
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm8
144 (0.0%)  	xorq	%r8,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpsrlq	$7,%xmm10,%xmm10
144 (0.0%)  	xorq	%r11,%r15
144 (0.0%)  	addq	%r12,%r9
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%r15,%rdi
144 (0.0%)  	vpsllq	$7,%xmm9,%xmm9
144 (0.0%)  	xorq	%r10,%r14
144 (0.0%)  	addq	%r13,%r9
144 (0.0%)  	vpxor	%xmm10,%xmm8,%xmm8
144 (0.0%)  	xorq	%r11,%rdi
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	vpsrlq	$6,%xmm0,%xmm11
144 (0.0%)  	addq	%r9,%rbx
144 (0.0%)  	addq	%rdi,%r9
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	movq	%rbx,%r13
144 (0.0%)  	addq	%r9,%r14
144 (0.0%)  	vpsllq	$3,%xmm0,%xmm10
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%r9
144 (0.0%)  	vpaddq	%xmm8,%xmm1,%xmm1
144 (0.0%)  	movq	%rcx,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$19,%xmm0,%xmm9
144 (0.0%)  	xorq	%rbx,%r13
144 (0.0%)  	xorq	%rdx,%r12
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%r9,%r14
144 (0.0%)  	vpsllq	$42,%xmm10,%xmm10
144 (0.0%)  	andq	%rbx,%r12
144 (0.0%)  	xorq	%rbx,%r13
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	addq	24(%rsp),%r8
144 (0.0%)  	movq	%r9,%rdi
144 (0.0%)  	vpsrlq	$42,%xmm9,%xmm9
144 (0.0%)  	xorq	%rdx,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	xorq	%r10,%rdi
144 (0.0%)  	addq	%r12,%r8
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%rdi,%r15
144 (0.0%)  	vpaddq	%xmm11,%xmm1,%xmm1
144 (0.0%)  	xorq	%r9,%r14
144 (0.0%)  	addq	%r13,%r8
144 (0.0%)  	vpaddq	-96(%rbp),%xmm1,%xmm10
144 (0.0%)  	xorq	%r10,%r15
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	addq	%r8,%rax
144 (0.0%)  	addq	%r15,%r8
144 (0.0%)  	movq	%rax,%r13
144 (0.0%)  	addq	%r8,%r14
144 (0.0%)  	vmovdqa	%xmm10,16(%rsp)
144 (0.0%)  	vpalignr	$8,%xmm2,%xmm3,%xmm8
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%r8
144 (0.0%)  	vpalignr	$8,%xmm6,%xmm7,%xmm11
144 (0.0%)  	movq	%rbx,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$1,%xmm8,%xmm10
144 (0.0%)  	xorq	%rax,%r13
144 (0.0%)  	xorq	%rcx,%r12
144 (0.0%)  	vpaddq	%xmm11,%xmm2,%xmm2
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%r8,%r14
144 (0.0%)  	vpsrlq	$7,%xmm8,%xmm11
144 (0.0%)  	andq	%rax,%r12
144 (0.0%)  	xorq	%rax,%r13
144 (0.0%)  	vpsllq	$56,%xmm8,%xmm9
144 (0.0%)  	addq	32(%rsp),%rdx
144 (0.0%)  	movq	%r8,%r15
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm8
144 (0.0%)  	xorq	%rcx,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpsrlq	$7,%xmm10,%xmm10
144 (0.0%)  	xorq	%r9,%r15
144 (0.0%)  	addq	%r12,%rdx
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%r15,%rdi
144 (0.0%)  	vpsllq	$7,%xmm9,%xmm9
144 (0.0%)  	xorq	%r8,%r14
144 (0.0%)  	addq	%r13,%rdx
144 (0.0%)  	vpxor	%xmm10,%xmm8,%xmm8
144 (0.0%)  	xorq	%r9,%rdi
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	vpsrlq	$6,%xmm1,%xmm11
144 (0.0%)  	addq	%rdx,%r11
144 (0.0%)  	addq	%rdi,%rdx
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	movq	%r11,%r13
144 (0.0%)  	addq	%rdx,%r14
144 (0.0%)  	vpsllq	$3,%xmm1,%xmm10
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%rdx
144 (0.0%)  	vpaddq	%xmm8,%xmm2,%xmm2
144 (0.0%)  	movq	%rax,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$19,%xmm1,%xmm9
144 (0.0%)  	xorq	%r11,%r13
144 (0.0%)  	xorq	%rbx,%r12
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%rdx,%r14
144 (0.0%)  	vpsllq	$42,%xmm10,%xmm10
144 (0.0%)  	andq	%r11,%r12
144 (0.0%)  	xorq	%r11,%r13
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	addq	40(%rsp),%rcx
144 (0.0%)  	movq	%rdx,%rdi
144 (0.0%)  	vpsrlq	$42,%xmm9,%xmm9
144 (0.0%)  	xorq	%rbx,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	xorq	%r8,%rdi
144 (0.0%)  	addq	%r12,%rcx
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%rdi,%r15
144 (0.0%)  	vpaddq	%xmm11,%xmm2,%xmm2
144 (0.0%)  	xorq	%rdx,%r14
144 (0.0%)  	addq	%r13,%rcx
144 (0.0%)  	vpaddq	-64(%rbp),%xmm2,%xmm10
144 (0.0%)  	xorq	%r8,%r15
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	addq	%rcx,%r10
144 (0.0%)  	addq	%r15,%rcx
144 (0.0%)  	movq	%r10,%r13
144 (0.0%)  	addq	%rcx,%r14
144 (0.0%)  	vmovdqa	%xmm10,32(%rsp)
144 (0.0%)  	vpalignr	$8,%xmm3,%xmm4,%xmm8
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%rcx
144 (0.0%)  	vpalignr	$8,%xmm7,%xmm0,%xmm11
144 (0.0%)  	movq	%r11,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$1,%xmm8,%xmm10
144 (0.0%)  	xorq	%r10,%r13
144 (0.0%)  	xorq	%rax,%r12
144 (0.0%)  	vpaddq	%xmm11,%xmm3,%xmm3
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%rcx,%r14
144 (0.0%)  	vpsrlq	$7,%xmm8,%xmm11
144 (0.0%)  	andq	%r10,%r12
144 (0.0%)  	xorq	%r10,%r13
144 (0.0%)  	vpsllq	$56,%xmm8,%xmm9
144 (0.0%)  	addq	48(%rsp),%rbx
144 (0.0%)  	movq	%rcx,%r15
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm8
144 (0.0%)  	xorq	%rax,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpsrlq	$7,%xmm10,%xmm10
144 (0.0%)  	xorq	%rdx,%r15
144 (0.0%)  	addq	%r12,%rbx
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%r15,%rdi
144 (0.0%)  	vpsllq	$7,%xmm9,%xmm9
144 (0.0%)  	xorq	%rcx,%r14
144 (0.0%)  	addq	%r13,%rbx
144 (0.0%)  	vpxor	%xmm10,%xmm8,%xmm8
144 (0.0%)  	xorq	%rdx,%rdi
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	vpsrlq	$6,%xmm2,%xmm11
144 (0.0%)  	addq	%rbx,%r9
144 (0.0%)  	addq	%rdi,%rbx
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	movq	%r9,%r13
144 (0.0%)  	addq	%rbx,%r14
144 (0.0%)  	vpsllq	$3,%xmm2,%xmm10
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%rbx
144 (0.0%)  	vpaddq	%xmm8,%xmm3,%xmm3
144 (0.0%)  	movq	%r10,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$19,%xmm2,%xmm9
144 (0.0%)  	xorq	%r9,%r13
144 (0.0%)  	xorq	%r11,%r12
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%rbx,%r14
144 (0.0%)  	vpsllq	$42,%xmm10,%xmm10
144 (0.0%)  	andq	%r9,%r12
144 (0.0%)  	xorq	%r9,%r13
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	addq	56(%rsp),%rax
144 (0.0%)  	movq	%rbx,%rdi
144 (0.0%)  	vpsrlq	$42,%xmm9,%xmm9
144 (0.0%)  	xorq	%r11,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	xorq	%rcx,%rdi
144 (0.0%)  	addq	%r12,%rax
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%rdi,%r15
144 (0.0%)  	vpaddq	%xmm11,%xmm3,%xmm3
144 (0.0%)  	xorq	%rbx,%r14
144 (0.0%)  	addq	%r13,%rax
144 (0.0%)  	vpaddq	-32(%rbp),%xmm3,%xmm10
144 (0.0%)  	xorq	%rcx,%r15
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	addq	%rax,%r8
144 (0.0%)  	addq	%r15,%rax
144 (0.0%)  	movq	%r8,%r13
144 (0.0%)  	addq	%rax,%r14
144 (0.0%)  	vmovdqa	%xmm10,48(%rsp)
144 (0.0%)  	vpalignr	$8,%xmm4,%xmm5,%xmm8
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%rax
144 (0.0%)  	vpalignr	$8,%xmm0,%xmm1,%xmm11
144 (0.0%)  	movq	%r9,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$1,%xmm8,%xmm10
144 (0.0%)  	xorq	%r8,%r13
144 (0.0%)  	xorq	%r10,%r12
144 (0.0%)  	vpaddq	%xmm11,%xmm4,%xmm4
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%rax,%r14
144 (0.0%)  	vpsrlq	$7,%xmm8,%xmm11
144 (0.0%)  	andq	%r8,%r12
144 (0.0%)  	xorq	%r8,%r13
144 (0.0%)  	vpsllq	$56,%xmm8,%xmm9
144 (0.0%)  	addq	64(%rsp),%r11
144 (0.0%)  	movq	%rax,%r15
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm8
144 (0.0%)  	xorq	%r10,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpsrlq	$7,%xmm10,%xmm10
144 (0.0%)  	xorq	%rbx,%r15
144 (0.0%)  	addq	%r12,%r11
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%r15,%rdi
144 (0.0%)  	vpsllq	$7,%xmm9,%xmm9
144 (0.0%)  	xorq	%rax,%r14
144 (0.0%)  	addq	%r13,%r11
144 (0.0%)  	vpxor	%xmm10,%xmm8,%xmm8
144 (0.0%)  	xorq	%rbx,%rdi
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	vpsrlq	$6,%xmm3,%xmm11
144 (0.0%)  	addq	%r11,%rdx
144 (0.0%)  	addq	%rdi,%r11
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	movq	%rdx,%r13
144 (0.0%)  	addq	%r11,%r14
144 (0.0%)  	vpsllq	$3,%xmm3,%xmm10
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%r11
144 (0.0%)  	vpaddq	%xmm8,%xmm4,%xmm4
144 (0.0%)  	movq	%r8,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$19,%xmm3,%xmm9
144 (0.0%)  	xorq	%rdx,%r13
144 (0.0%)  	xorq	%r9,%r12
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%r11,%r14
144 (0.0%)  	vpsllq	$42,%xmm10,%xmm10
144 (0.0%)  	andq	%rdx,%r12
144 (0.0%)  	xorq	%rdx,%r13
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	addq	72(%rsp),%r10
144 (0.0%)  	movq	%r11,%rdi
144 (0.0%)  	vpsrlq	$42,%xmm9,%xmm9
144 (0.0%)  	xorq	%r9,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	xorq	%rax,%rdi
144 (0.0%)  	addq	%r12,%r10
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%rdi,%r15
144 (0.0%)  	vpaddq	%xmm11,%xmm4,%xmm4
144 (0.0%)  	xorq	%r11,%r14
144 (0.0%)  	addq	%r13,%r10
144 (0.0%)  	vpaddq	0(%rbp),%xmm4,%xmm10
144 (0.0%)  	xorq	%rax,%r15
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	addq	%r10,%rcx
144 (0.0%)  	addq	%r15,%r10
144 (0.0%)  	movq	%rcx,%r13
144 (0.0%)  	addq	%r10,%r14
144 (0.0%)  	vmovdqa	%xmm10,64(%rsp)
144 (0.0%)  	vpalignr	$8,%xmm5,%xmm6,%xmm8
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%r10
144 (0.0%)  	vpalignr	$8,%xmm1,%xmm2,%xmm11
144 (0.0%)  	movq	%rdx,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$1,%xmm8,%xmm10
144 (0.0%)  	xorq	%rcx,%r13
144 (0.0%)  	xorq	%r8,%r12
144 (0.0%)  	vpaddq	%xmm11,%xmm5,%xmm5
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%r10,%r14
144 (0.0%)  	vpsrlq	$7,%xmm8,%xmm11
144 (0.0%)  	andq	%rcx,%r12
144 (0.0%)  	xorq	%rcx,%r13
144 (0.0%)  	vpsllq	$56,%xmm8,%xmm9
144 (0.0%)  	addq	80(%rsp),%r9
144 (0.0%)  	movq	%r10,%r15
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm8
144 (0.0%)  	xorq	%r8,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpsrlq	$7,%xmm10,%xmm10
144 (0.0%)  	xorq	%r11,%r15
144 (0.0%)  	addq	%r12,%r9
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%r15,%rdi
144 (0.0%)  	vpsllq	$7,%xmm9,%xmm9
144 (0.0%)  	xorq	%r10,%r14
144 (0.0%)  	addq	%r13,%r9
144 (0.0%)  	vpxor	%xmm10,%xmm8,%xmm8
144 (0.0%)  	xorq	%r11,%rdi
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	vpsrlq	$6,%xmm4,%xmm11
144 (0.0%)  	addq	%r9,%rbx
144 (0.0%)  	addq	%rdi,%r9
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	movq	%rbx,%r13
144 (0.0%)  	addq	%r9,%r14
144 (0.0%)  	vpsllq	$3,%xmm4,%xmm10
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%r9
144 (0.0%)  	vpaddq	%xmm8,%xmm5,%xmm5
144 (0.0%)  	movq	%rcx,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$19,%xmm4,%xmm9
144 (0.0%)  	xorq	%rbx,%r13
144 (0.0%)  	xorq	%rdx,%r12
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%r9,%r14
144 (0.0%)  	vpsllq	$42,%xmm10,%xmm10
144 (0.0%)  	andq	%rbx,%r12
144 (0.0%)  	xorq	%rbx,%r13
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	addq	88(%rsp),%r8
144 (0.0%)  	movq	%r9,%rdi
144 (0.0%)  	vpsrlq	$42,%xmm9,%xmm9
144 (0.0%)  	xorq	%rdx,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	xorq	%r10,%rdi
144 (0.0%)  	addq	%r12,%r8
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%rdi,%r15
144 (0.0%)  	vpaddq	%xmm11,%xmm5,%xmm5
144 (0.0%)  	xorq	%r9,%r14
144 (0.0%)  	addq	%r13,%r8
144 (0.0%)  	vpaddq	32(%rbp),%xmm5,%xmm10
144 (0.0%)  	xorq	%r10,%r15
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	addq	%r8,%rax
144 (0.0%)  	addq	%r15,%r8
144 (0.0%)  	movq	%rax,%r13
144 (0.0%)  	addq	%r8,%r14
144 (0.0%)  	vmovdqa	%xmm10,80(%rsp)
144 (0.0%)  	vpalignr	$8,%xmm6,%xmm7,%xmm8
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%r8
144 (0.0%)  	vpalignr	$8,%xmm2,%xmm3,%xmm11
144 (0.0%)  	movq	%rbx,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$1,%xmm8,%xmm10
144 (0.0%)  	xorq	%rax,%r13
144 (0.0%)  	xorq	%rcx,%r12
144 (0.0%)  	vpaddq	%xmm11,%xmm6,%xmm6
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%r8,%r14
144 (0.0%)  	vpsrlq	$7,%xmm8,%xmm11
144 (0.0%)  	andq	%rax,%r12
144 (0.0%)  	xorq	%rax,%r13
144 (0.0%)  	vpsllq	$56,%xmm8,%xmm9
144 (0.0%)  	addq	96(%rsp),%rdx
144 (0.0%)  	movq	%r8,%r15
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm8
144 (0.0%)  	xorq	%rcx,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpsrlq	$7,%xmm10,%xmm10
144 (0.0%)  	xorq	%r9,%r15
144 (0.0%)  	addq	%r12,%rdx
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%r15,%rdi
144 (0.0%)  	vpsllq	$7,%xmm9,%xmm9
144 (0.0%)  	xorq	%r8,%r14
144 (0.0%)  	addq	%r13,%rdx
144 (0.0%)  	vpxor	%xmm10,%xmm8,%xmm8
144 (0.0%)  	xorq	%r9,%rdi
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	vpsrlq	$6,%xmm5,%xmm11
144 (0.0%)  	addq	%rdx,%r11
144 (0.0%)  	addq	%rdi,%rdx
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	movq	%r11,%r13
144 (0.0%)  	addq	%rdx,%r14
144 (0.0%)  	vpsllq	$3,%xmm5,%xmm10
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%rdx
144 (0.0%)  	vpaddq	%xmm8,%xmm6,%xmm6
144 (0.0%)  	movq	%rax,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$19,%xmm5,%xmm9
144 (0.0%)  	xorq	%r11,%r13
144 (0.0%)  	xorq	%rbx,%r12
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%rdx,%r14
144 (0.0%)  	vpsllq	$42,%xmm10,%xmm10
144 (0.0%)  	andq	%r11,%r12
144 (0.0%)  	xorq	%r11,%r13
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	addq	104(%rsp),%rcx
144 (0.0%)  	movq	%rdx,%rdi
144 (0.0%)  	vpsrlq	$42,%xmm9,%xmm9
144 (0.0%)  	xorq	%rbx,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	xorq	%r8,%rdi
144 (0.0%)  	addq	%r12,%rcx
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%rdi,%r15
144 (0.0%)  	vpaddq	%xmm11,%xmm6,%xmm6
144 (0.0%)  	xorq	%rdx,%r14
144 (0.0%)  	addq	%r13,%rcx
144 (0.0%)  	vpaddq	64(%rbp),%xmm6,%xmm10
144 (0.0%)  	xorq	%r8,%r15
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	addq	%rcx,%r10
144 (0.0%)  	addq	%r15,%rcx
144 (0.0%)  	movq	%r10,%r13
144 (0.0%)  	addq	%rcx,%r14
144 (0.0%)  	vmovdqa	%xmm10,96(%rsp)
144 (0.0%)  	vpalignr	$8,%xmm7,%xmm0,%xmm8
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%rcx
144 (0.0%)  	vpalignr	$8,%xmm3,%xmm4,%xmm11
144 (0.0%)  	movq	%r11,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$1,%xmm8,%xmm10
144 (0.0%)  	xorq	%r10,%r13
144 (0.0%)  	xorq	%rax,%r12
144 (0.0%)  	vpaddq	%xmm11,%xmm7,%xmm7
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%rcx,%r14
144 (0.0%)  	vpsrlq	$7,%xmm8,%xmm11
144 (0.0%)  	andq	%r10,%r12
144 (0.0%)  	xorq	%r10,%r13
144 (0.0%)  	vpsllq	$56,%xmm8,%xmm9
144 (0.0%)  	addq	112(%rsp),%rbx
144 (0.0%)  	movq	%rcx,%r15
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm8
144 (0.0%)  	xorq	%rax,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpsrlq	$7,%xmm10,%xmm10
144 (0.0%)  	xorq	%rdx,%r15
144 (0.0%)  	addq	%r12,%rbx
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%r15,%rdi
144 (0.0%)  	vpsllq	$7,%xmm9,%xmm9
144 (0.0%)  	xorq	%rcx,%r14
144 (0.0%)  	addq	%r13,%rbx
144 (0.0%)  	vpxor	%xmm10,%xmm8,%xmm8
144 (0.0%)  	xorq	%rdx,%rdi
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	vpsrlq	$6,%xmm6,%xmm11
144 (0.0%)  	addq	%rbx,%r9
144 (0.0%)  	addq	%rdi,%rbx
144 (0.0%)  	vpxor	%xmm9,%xmm8,%xmm8
144 (0.0%)  	movq	%r9,%r13
144 (0.0%)  	addq	%rbx,%r14
144 (0.0%)  	vpsllq	$3,%xmm6,%xmm10
144 (0.0%)  	shrdq	$23,%r13,%r13
144 (0.0%)  	movq	%r14,%rbx
144 (0.0%)  	vpaddq	%xmm8,%xmm7,%xmm7
144 (0.0%)  	movq	%r10,%r12
144 (0.0%)  	shrdq	$5,%r14,%r14
144 (0.0%)  	vpsrlq	$19,%xmm6,%xmm9
144 (0.0%)  	xorq	%r9,%r13
144 (0.0%)  	xorq	%r11,%r12
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	shrdq	$4,%r13,%r13
144 (0.0%)  	xorq	%rbx,%r14
144 (0.0%)  	vpsllq	$42,%xmm10,%xmm10
144 (0.0%)  	andq	%r9,%r12
144 (0.0%)  	xorq	%r9,%r13
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	addq	120(%rsp),%rax
144 (0.0%)  	movq	%rbx,%rdi
144 (0.0%)  	vpsrlq	$42,%xmm9,%xmm9
144 (0.0%)  	xorq	%r11,%r12
144 (0.0%)  	shrdq	$6,%r14,%r14
144 (0.0%)  	vpxor	%xmm10,%xmm11,%xmm11
144 (0.0%)  	xorq	%rcx,%rdi
144 (0.0%)  	addq	%r12,%rax
144 (0.0%)  	vpxor	%xmm9,%xmm11,%xmm11
144 (0.0%)  	shrdq	$14,%r13,%r13
144 (0.0%)  	andq	%rdi,%r15
144 (0.0%)  	vpaddq	%xmm11,%xmm7,%xmm7
144 (0.0%)  	xorq	%rbx,%r14
144 (0.0%)  	addq	%r13,%rax
144 (0.0%)  	vpaddq	96(%rbp),%xmm7,%xmm10
144 (0.0%)  	xorq	%rcx,%r15
144 (0.0%)  	shrdq	$28,%r14,%r14
144 (0.0%)  	addq	%rax,%r8
144 (0.0%)  	addq	%r15,%rax
144 (0.0%)  	movq	%r8,%r13
144 (0.0%)  	addq	%rax,%r14
144 (0.0%)  	vmovdqa	%xmm10,112(%rsp)
144 (0.0%)  	cmpb	$0,135(%rbp)
144 (0.0%)  	jne	.Lavx_00_47
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%rax
 36 (0.0%)  	movq	%r9,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%r8,%r13
 36 (0.0%)  	xorq	%r10,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%rax,%r14
 36 (0.0%)  	andq	%r8,%r12
 36 (0.0%)  	xorq	%r8,%r13
 36 (0.0%)  	addq	0(%rsp),%r11
 36 (0.0%)  	movq	%rax,%r15
 36 (0.0%)  	xorq	%r10,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%rbx,%r15
 36 (0.0%)  	addq	%r12,%r11
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%r15,%rdi
 36 (0.0%)  	xorq	%rax,%r14
 36 (0.0%)  	addq	%r13,%r11
 36 (0.0%)  	xorq	%rbx,%rdi
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%r11,%rdx
 36 (0.0%)  	addq	%rdi,%r11
 36 (0.0%)  	movq	%rdx,%r13
 36 (0.0%)  	addq	%r11,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%r11
 36 (0.0%)  	movq	%r8,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%rdx,%r13
 36 (0.0%)  	xorq	%r9,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%r11,%r14
 36 (0.0%)  	andq	%rdx,%r12
 36 (0.0%)  	xorq	%rdx,%r13
 36 (0.0%)  	addq	8(%rsp),%r10
 36 (0.0%)  	movq	%r11,%rdi
 36 (0.0%)  	xorq	%r9,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%rax,%rdi
 36 (0.0%)  	addq	%r12,%r10
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%rdi,%r15
 36 (0.0%)  	xorq	%r11,%r14
 36 (0.0%)  	addq	%r13,%r10
 36 (0.0%)  	xorq	%rax,%r15
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%r10,%rcx
 36 (0.0%)  	addq	%r15,%r10
 36 (0.0%)  	movq	%rcx,%r13
 36 (0.0%)  	addq	%r10,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%r10
 36 (0.0%)  	movq	%rdx,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%rcx,%r13
 36 (0.0%)  	xorq	%r8,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%r10,%r14
 36 (0.0%)  	andq	%rcx,%r12
 36 (0.0%)  	xorq	%rcx,%r13
 36 (0.0%)  	addq	16(%rsp),%r9
 36 (0.0%)  	movq	%r10,%r15
 36 (0.0%)  	xorq	%r8,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%r11,%r15
 36 (0.0%)  	addq	%r12,%r9
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%r15,%rdi
 36 (0.0%)  	xorq	%r10,%r14
 36 (0.0%)  	addq	%r13,%r9
 36 (0.0%)  	xorq	%r11,%rdi
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%r9,%rbx
 36 (0.0%)  	addq	%rdi,%r9
 36 (0.0%)  	movq	%rbx,%r13
 36 (0.0%)  	addq	%r9,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%r9
 36 (0.0%)  	movq	%rcx,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%rbx,%r13
 36 (0.0%)  	xorq	%rdx,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%r9,%r14
 36 (0.0%)  	andq	%rbx,%r12
 36 (0.0%)  	xorq	%rbx,%r13
 36 (0.0%)  	addq	24(%rsp),%r8
 36 (0.0%)  	movq	%r9,%rdi
 36 (0.0%)  	xorq	%rdx,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%r10,%rdi
 36 (0.0%)  	addq	%r12,%r8
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%rdi,%r15
 36 (0.0%)  	xorq	%r9,%r14
 36 (0.0%)  	addq	%r13,%r8
 36 (0.0%)  	xorq	%r10,%r15
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%r8,%rax
 36 (0.0%)  	addq	%r15,%r8
 36 (0.0%)  	movq	%rax,%r13
 36 (0.0%)  	addq	%r8,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%r8
 36 (0.0%)  	movq	%rbx,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%rax,%r13
 36 (0.0%)  	xorq	%rcx,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%r8,%r14
 36 (0.0%)  	andq	%rax,%r12
 36 (0.0%)  	xorq	%rax,%r13
 36 (0.0%)  	addq	32(%rsp),%rdx
 36 (0.0%)  	movq	%r8,%r15
 36 (0.0%)  	xorq	%rcx,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%r9,%r15
 36 (0.0%)  	addq	%r12,%rdx
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%r15,%rdi
 36 (0.0%)  	xorq	%r8,%r14
 36 (0.0%)  	addq	%r13,%rdx
 36 (0.0%)  	xorq	%r9,%rdi
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%rdx,%r11
 36 (0.0%)  	addq	%rdi,%rdx
 36 (0.0%)  	movq	%r11,%r13
 36 (0.0%)  	addq	%rdx,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%rdx
 36 (0.0%)  	movq	%rax,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%r11,%r13
 36 (0.0%)  	xorq	%rbx,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%rdx,%r14
 36 (0.0%)  	andq	%r11,%r12
 36 (0.0%)  	xorq	%r11,%r13
 36 (0.0%)  	addq	40(%rsp),%rcx
 36 (0.0%)  	movq	%rdx,%rdi
 36 (0.0%)  	xorq	%rbx,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%r8,%rdi
 36 (0.0%)  	addq	%r12,%rcx
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%rdi,%r15
 36 (0.0%)  	xorq	%rdx,%r14
 36 (0.0%)  	addq	%r13,%rcx
 36 (0.0%)  	xorq	%r8,%r15
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%rcx,%r10
 36 (0.0%)  	addq	%r15,%rcx
 36 (0.0%)  	movq	%r10,%r13
 36 (0.0%)  	addq	%rcx,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%rcx
 36 (0.0%)  	movq	%r11,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%r10,%r13
 36 (0.0%)  	xorq	%rax,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%rcx,%r14
 36 (0.0%)  	andq	%r10,%r12
 36 (0.0%)  	xorq	%r10,%r13
 36 (0.0%)  	addq	48(%rsp),%rbx
 36 (0.0%)  	movq	%rcx,%r15
 36 (0.0%)  	xorq	%rax,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%rdx,%r15
 36 (0.0%)  	addq	%r12,%rbx
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%r15,%rdi
 36 (0.0%)  	xorq	%rcx,%r14
 36 (0.0%)  	addq	%r13,%rbx
 36 (0.0%)  	xorq	%rdx,%rdi
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%rbx,%r9
 36 (0.0%)  	addq	%rdi,%rbx
 36 (0.0%)  	movq	%r9,%r13
 36 (0.0%)  	addq	%rbx,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%rbx
 36 (0.0%)  	movq	%r10,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%r9,%r13
 36 (0.0%)  	xorq	%r11,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%rbx,%r14
 36 (0.0%)  	andq	%r9,%r12
 36 (0.0%)  	xorq	%r9,%r13
 36 (0.0%)  	addq	56(%rsp),%rax
 36 (0.0%)  	movq	%rbx,%rdi
 36 (0.0%)  	xorq	%r11,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%rcx,%rdi
 36 (0.0%)  	addq	%r12,%rax
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%rdi,%r15
 36 (0.0%)  	xorq	%rbx,%r14
 36 (0.0%)  	addq	%r13,%rax
 36 (0.0%)  	xorq	%rcx,%r15
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%rax,%r8
 36 (0.0%)  	addq	%r15,%rax
 36 (0.0%)  	movq	%r8,%r13
 36 (0.0%)  	addq	%rax,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%rax
 36 (0.0%)  	movq	%r9,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%r8,%r13
 36 (0.0%)  	xorq	%r10,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%rax,%r14
 36 (0.0%)  	andq	%r8,%r12
 36 (0.0%)  	xorq	%r8,%r13
 36 (0.0%)  	addq	64(%rsp),%r11
 36 (0.0%)  	movq	%rax,%r15
 36 (0.0%)  	xorq	%r10,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%rbx,%r15
 36 (0.0%)  	addq	%r12,%r11
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%r15,%rdi
 36 (0.0%)  	xorq	%rax,%r14
 36 (0.0%)  	addq	%r13,%r11
 36 (0.0%)  	xorq	%rbx,%rdi
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%r11,%rdx
 36 (0.0%)  	addq	%rdi,%r11
 36 (0.0%)  	movq	%rdx,%r13
 36 (0.0%)  	addq	%r11,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%r11
 36 (0.0%)  	movq	%r8,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%rdx,%r13
 36 (0.0%)  	xorq	%r9,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%r11,%r14
 36 (0.0%)  	andq	%rdx,%r12
 36 (0.0%)  	xorq	%rdx,%r13
 36 (0.0%)  	addq	72(%rsp),%r10
 36 (0.0%)  	movq	%r11,%rdi
 36 (0.0%)  	xorq	%r9,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%rax,%rdi
 36 (0.0%)  	addq	%r12,%r10
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%rdi,%r15
 36 (0.0%)  	xorq	%r11,%r14
 36 (0.0%)  	addq	%r13,%r10
 36 (0.0%)  	xorq	%rax,%r15
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%r10,%rcx
 36 (0.0%)  	addq	%r15,%r10
 36 (0.0%)  	movq	%rcx,%r13
 36 (0.0%)  	addq	%r10,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%r10
 36 (0.0%)  	movq	%rdx,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%rcx,%r13
 36 (0.0%)  	xorq	%r8,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%r10,%r14
 36 (0.0%)  	andq	%rcx,%r12
 36 (0.0%)  	xorq	%rcx,%r13
 36 (0.0%)  	addq	80(%rsp),%r9
 36 (0.0%)  	movq	%r10,%r15
 36 (0.0%)  	xorq	%r8,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%r11,%r15
 36 (0.0%)  	addq	%r12,%r9
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%r15,%rdi
 36 (0.0%)  	xorq	%r10,%r14
 36 (0.0%)  	addq	%r13,%r9
 36 (0.0%)  	xorq	%r11,%rdi
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%r9,%rbx
 36 (0.0%)  	addq	%rdi,%r9
 36 (0.0%)  	movq	%rbx,%r13
 36 (0.0%)  	addq	%r9,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%r9
 36 (0.0%)  	movq	%rcx,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%rbx,%r13
 36 (0.0%)  	xorq	%rdx,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%r9,%r14
 36 (0.0%)  	andq	%rbx,%r12
 36 (0.0%)  	xorq	%rbx,%r13
 36 (0.0%)  	addq	88(%rsp),%r8
 36 (0.0%)  	movq	%r9,%rdi
 36 (0.0%)  	xorq	%rdx,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%r10,%rdi
 36 (0.0%)  	addq	%r12,%r8
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%rdi,%r15
 36 (0.0%)  	xorq	%r9,%r14
 36 (0.0%)  	addq	%r13,%r8
 36 (0.0%)  	xorq	%r10,%r15
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%r8,%rax
 36 (0.0%)  	addq	%r15,%r8
 36 (0.0%)  	movq	%rax,%r13
 36 (0.0%)  	addq	%r8,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%r8
 36 (0.0%)  	movq	%rbx,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%rax,%r13
 36 (0.0%)  	xorq	%rcx,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%r8,%r14
 36 (0.0%)  	andq	%rax,%r12
 36 (0.0%)  	xorq	%rax,%r13
 36 (0.0%)  	addq	96(%rsp),%rdx
 36 (0.0%)  	movq	%r8,%r15
 36 (0.0%)  	xorq	%rcx,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%r9,%r15
 36 (0.0%)  	addq	%r12,%rdx
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%r15,%rdi
 36 (0.0%)  	xorq	%r8,%r14
 36 (0.0%)  	addq	%r13,%rdx
 36 (0.0%)  	xorq	%r9,%rdi
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%rdx,%r11
 36 (0.0%)  	addq	%rdi,%rdx
 36 (0.0%)  	movq	%r11,%r13
 36 (0.0%)  	addq	%rdx,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%rdx
 36 (0.0%)  	movq	%rax,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%r11,%r13
 36 (0.0%)  	xorq	%rbx,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%rdx,%r14
 36 (0.0%)  	andq	%r11,%r12
 36 (0.0%)  	xorq	%r11,%r13
 36 (0.0%)  	addq	104(%rsp),%rcx
 36 (0.0%)  	movq	%rdx,%rdi
 36 (0.0%)  	xorq	%rbx,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%r8,%rdi
 36 (0.0%)  	addq	%r12,%rcx
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%rdi,%r15
 36 (0.0%)  	xorq	%rdx,%r14
 36 (0.0%)  	addq	%r13,%rcx
 36 (0.0%)  	xorq	%r8,%r15
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%rcx,%r10
 36 (0.0%)  	addq	%r15,%rcx
 36 (0.0%)  	movq	%r10,%r13
 36 (0.0%)  	addq	%rcx,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%rcx
 36 (0.0%)  	movq	%r11,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%r10,%r13
 36 (0.0%)  	xorq	%rax,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%rcx,%r14
 36 (0.0%)  	andq	%r10,%r12
 36 (0.0%)  	xorq	%r10,%r13
 36 (0.0%)  	addq	112(%rsp),%rbx
 36 (0.0%)  	movq	%rcx,%r15
 36 (0.0%)  	xorq	%rax,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%rdx,%r15
 36 (0.0%)  	addq	%r12,%rbx
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%r15,%rdi
 36 (0.0%)  	xorq	%rcx,%r14
 36 (0.0%)  	addq	%r13,%rbx
 36 (0.0%)  	xorq	%rdx,%rdi
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%rbx,%r9
 36 (0.0%)  	addq	%rdi,%rbx
 36 (0.0%)  	movq	%r9,%r13
 36 (0.0%)  	addq	%rbx,%r14
 36 (0.0%)  	shrdq	$23,%r13,%r13
 36 (0.0%)  	movq	%r14,%rbx
 36 (0.0%)  	movq	%r10,%r12
 36 (0.0%)  	shrdq	$5,%r14,%r14
 36 (0.0%)  	xorq	%r9,%r13
 36 (0.0%)  	xorq	%r11,%r12
 36 (0.0%)  	shrdq	$4,%r13,%r13
 36 (0.0%)  	xorq	%rbx,%r14
 36 (0.0%)  	andq	%r9,%r12
 36 (0.0%)  	xorq	%r9,%r13
 36 (0.0%)  	addq	120(%rsp),%rax
 36 (0.0%)  	movq	%rbx,%rdi
 36 (0.0%)  	xorq	%r11,%r12
 36 (0.0%)  	shrdq	$6,%r14,%r14
 36 (0.0%)  	xorq	%rcx,%rdi
 36 (0.0%)  	addq	%r12,%rax
 36 (0.0%)  	shrdq	$14,%r13,%r13
 36 (0.0%)  	andq	%rdi,%r15
 36 (0.0%)  	xorq	%rbx,%r14
 36 (0.0%)  	addq	%r13,%rax
 36 (0.0%)  	xorq	%rcx,%r15
 36 (0.0%)  	shrdq	$28,%r14,%r14
 36 (0.0%)  	addq	%rax,%r8
 36 (0.0%)  	addq	%r15,%rax
 36 (0.0%)  	movq	%r8,%r13
 36 (0.0%)  	addq	%rax,%r14
 36 (0.0%)  	movq	128+0(%rsp),%rdi
 36 (0.0%)  	movq	%r14,%rax
  .         
 36 (0.0%)  	addq	0(%rdi),%rax
 36 (0.0%)  	leaq	128(%rsi),%rsi
 36 (0.0%)  	addq	8(%rdi),%rbx
 36 (0.0%)  	addq	16(%rdi),%rcx
 36 (0.0%)  	addq	24(%rdi),%rdx
 36 (0.0%)  	addq	32(%rdi),%r8
 36 (0.0%)  	addq	40(%rdi),%r9
 36 (0.0%)  	addq	48(%rdi),%r10
 36 (0.0%)  	addq	56(%rdi),%r11
  .         
 36 (0.0%)  	cmpq	128+16(%rsp),%rsi
  .         
 36 (0.0%)  	movq	%rax,0(%rdi)
 36 (0.0%)  	movq	%rbx,8(%rdi)
 36 (0.0%)  	movq	%rcx,16(%rdi)
 36 (0.0%)  	movq	%rdx,24(%rdi)
 36 (0.0%)  	movq	%r8,32(%rdi)
 36 (0.0%)  	movq	%r9,40(%rdi)
 36 (0.0%)  	movq	%r10,48(%rdi)
 36 (0.0%)  	movq	%r11,56(%rdi)
 36 (0.0%)  	jb	.Lloop_avx
  .         
 14 (0.0%)  	movq	152(%rsp),%rsi
  .         .cfi_def_cfa	%rsi,8
 14 (0.0%)  	vzeroupper
 14 (0.0%)  	movq	-48(%rsi),%r15
  .         .cfi_restore	%r15
 14 (0.0%)  	movq	-40(%rsi),%r14
  .         .cfi_restore	%r14
 14 (0.0%)  	movq	-32(%rsi),%r13
  .         .cfi_restore	%r13
 14 (0.0%)  	movq	-24(%rsi),%r12
  .         .cfi_restore	%r12
 14 (0.0%)  	movq	-16(%rsi),%rbp
  .         .cfi_restore	%rbp
 14 (0.0%)  	movq	-8(%rsi),%rbx
  .         .cfi_restore	%rbx
 28 (0.0%)  	leaq	(%rsi),%rsp
  .         .cfi_def_cfa_register	%rsp
  .         .Lepilogue_avx:
  .         	.byte	0xf3,0xc3
  .         .cfi_endproc	
  .         .size	sha512_block_data_order_avx,.-sha512_block_data_order_avx
  .         #endif

--------------------------------------------------------------------------------
-- Annotated source file: /home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/x86_64-mont.S
--------------------------------------------------------------------------------
Ir__________ 

-- line 10 ----------------------------------------
    .         .hidden OPENSSL_ia32cap_P
    .         
    .         .globl	bn_mul_mont
    .         .hidden bn_mul_mont
    .         .type	bn_mul_mont,@function
    .         .align	16
    .         bn_mul_mont:
    .         .cfi_startproc	
  142 (0.0%)  	movl	%r9d,%r9d
  142 (0.0%)  	movq	%rsp,%rax
    .         .cfi_def_cfa_register	%rax
  142 (0.0%)  	testl	$3,%r9d
  142 (0.0%)  	jnz	.Lmul_enter
  142 (0.0%)  	cmpl	$8,%r9d
  142 (0.0%)  	jb	.Lmul_enter
   76 (0.0%)  	leaq	OPENSSL_ia32cap_P(%rip),%r11
   76 (0.0%)  	movl	8(%r11),%r11d
   76 (0.0%)  	cmpq	%rsi,%rdx
   76 (0.0%)  	jne	.Lmul4x_enter
   64 (0.0%)  	testl	$7,%r9d
   64 (0.0%)  	jz	.Lsqr8x_enter
    .         	jmp	.Lmul4x_enter
    .         
    .         .align	16
    .         .Lmul_enter:
   66 (0.0%)  	pushq	%rbx
    .         .cfi_offset	%rbx,-16
   66 (0.0%)  	pushq	%rbp
    .         .cfi_offset	%rbp,-24
   66 (0.0%)  	pushq	%r12
    .         .cfi_offset	%r12,-32
   66 (0.0%)  	pushq	%r13
    .         .cfi_offset	%r13,-40
   66 (0.0%)  	pushq	%r14
    .         .cfi_offset	%r14,-48
   66 (0.0%)  	pushq	%r15
    .         .cfi_offset	%r15,-56
    .         
   66 (0.0%)  	negq	%r9
   66 (0.0%)  	movq	%rsp,%r11
   66 (0.0%)  	leaq	-16(%rsp,%r9,8),%r10
   66 (0.0%)  	negq	%r9
   66 (0.0%)  	andq	$-1024,%r10
    .         
    .         
    .         
    .         
    .         
    .         
    .         
    .         
    .         
   66 (0.0%)  	subq	%r10,%r11
   66 (0.0%)  	andq	$-4096,%r11
   66 (0.0%)  	leaq	(%r10,%r11,1),%rsp
   66 (0.0%)  	movq	(%rsp),%r11
   66 (0.0%)  	cmpq	%r10,%rsp
   66 (0.0%)  	ja	.Lmul_page_walk
   66 (0.0%)  	jmp	.Lmul_page_walk_done
    .         
    .         .align	16
    .         .Lmul_page_walk:
    .         	leaq	-4096(%rsp),%rsp
    .         	movq	(%rsp),%r11
    .         	cmpq	%r10,%rsp
    .         	ja	.Lmul_page_walk
    .         .Lmul_page_walk_done:
    .         
   66 (0.0%)  	movq	%rax,8(%rsp,%r9,8)
    .         .cfi_escape	0x0f,0x0a,0x77,0x08,0x79,0x00,0x38,0x1e,0x22,0x06,0x23,0x08
    .         .Lmul_body:
   66 (0.0%)  	movq	%rdx,%r12
   66 (0.0%)  	movq	(%r8),%r8
   66 (0.0%)  	movq	(%r12),%rbx
   66 (0.0%)  	movq	(%rsi),%rax
    .         
   66 (0.0%)  	xorq	%r14,%r14
   66 (0.0%)  	xorq	%r15,%r15
    .         
   66 (0.0%)  	movq	%r8,%rbp
   66 (0.0%)  	mulq	%rbx
   66 (0.0%)  	movq	%rax,%r10
   66 (0.0%)  	movq	(%rcx),%rax
    .         
   66 (0.0%)  	imulq	%r10,%rbp
   66 (0.0%)  	movq	%rdx,%r11
    .         
   66 (0.0%)  	mulq	%rbp
   66 (0.0%)  	addq	%rax,%r10
   66 (0.0%)  	movq	8(%rsi),%rax
   66 (0.0%)  	adcq	$0,%rdx
   66 (0.0%)  	movq	%rdx,%r13
    .         
   66 (0.0%)  	leaq	1(%r15),%r15
   66 (0.0%)  	jmp	.L1st_enter
    .         
    .         .align	16
    .         .L1st:
  132 (0.0%)  	addq	%rax,%r13
  132 (0.0%)  	movq	(%rsi,%r15,8),%rax
  132 (0.0%)  	adcq	$0,%rdx
  132 (0.0%)  	addq	%r11,%r13
  132 (0.0%)  	movq	%r10,%r11
  132 (0.0%)  	adcq	$0,%rdx
  132 (0.0%)  	movq	%r13,-16(%rsp,%r15,8)
  132 (0.0%)  	movq	%rdx,%r13
    .         
    .         .L1st_enter:
  198 (0.0%)  	mulq	%rbx
  198 (0.0%)  	addq	%rax,%r11
  198 (0.0%)  	movq	(%rcx,%r15,8),%rax
  198 (0.0%)  	adcq	$0,%rdx
  198 (0.0%)  	leaq	1(%r15),%r15
  198 (0.0%)  	movq	%rdx,%r10
    .         
  198 (0.0%)  	mulq	%rbp
  198 (0.0%)  	cmpq	%r9,%r15
  198 (0.0%)  	jne	.L1st
    .         
   66 (0.0%)  	addq	%rax,%r13
   66 (0.0%)  	movq	(%rsi),%rax
   66 (0.0%)  	adcq	$0,%rdx
   66 (0.0%)  	addq	%r11,%r13
   66 (0.0%)  	adcq	$0,%rdx
   66 (0.0%)  	movq	%r13,-16(%rsp,%r15,8)
   66 (0.0%)  	movq	%rdx,%r13
   66 (0.0%)  	movq	%r10,%r11
    .         
   66 (0.0%)  	xorq	%rdx,%rdx
   66 (0.0%)  	addq	%r11,%r13
   66 (0.0%)  	adcq	$0,%rdx
   66 (0.0%)  	movq	%r13,-8(%rsp,%r9,8)
   66 (0.0%)  	movq	%rdx,(%rsp,%r9,8)
    .         
   66 (0.0%)  	leaq	1(%r14),%r14
   66 (0.0%)  	jmp	.Louter
    .         .align	16
    .         .Louter:
  198 (0.0%)  	movq	(%r12,%r14,8),%rbx
  198 (0.0%)  	xorq	%r15,%r15
  198 (0.0%)  	movq	%r8,%rbp
  198 (0.0%)  	movq	(%rsp),%r10
  198 (0.0%)  	mulq	%rbx
  198 (0.0%)  	addq	%rax,%r10
  198 (0.0%)  	movq	(%rcx),%rax
  198 (0.0%)  	adcq	$0,%rdx
    .         
  198 (0.0%)  	imulq	%r10,%rbp
  198 (0.0%)  	movq	%rdx,%r11
    .         
  198 (0.0%)  	mulq	%rbp
  198 (0.0%)  	addq	%rax,%r10
  198 (0.0%)  	movq	8(%rsi),%rax
  198 (0.0%)  	adcq	$0,%rdx
  198 (0.0%)  	movq	8(%rsp),%r10
  198 (0.0%)  	movq	%rdx,%r13
    .         
  198 (0.0%)  	leaq	1(%r15),%r15
  198 (0.0%)  	jmp	.Linner_enter
    .         
    .         .align	16
    .         .Linner:
  396 (0.0%)  	addq	%rax,%r13
  396 (0.0%)  	movq	(%rsi,%r15,8),%rax
  396 (0.0%)  	adcq	$0,%rdx
  396 (0.0%)  	addq	%r10,%r13
  396 (0.0%)  	movq	(%rsp,%r15,8),%r10
  396 (0.0%)  	adcq	$0,%rdx
  396 (0.0%)  	movq	%r13,-16(%rsp,%r15,8)
  396 (0.0%)  	movq	%rdx,%r13
    .         
    .         .Linner_enter:
  594 (0.0%)  	mulq	%rbx
  594 (0.0%)  	addq	%rax,%r11
  594 (0.0%)  	movq	(%rcx,%r15,8),%rax
  594 (0.0%)  	adcq	$0,%rdx
  594 (0.0%)  	addq	%r11,%r10
  594 (0.0%)  	movq	%rdx,%r11
  594 (0.0%)  	adcq	$0,%r11
  594 (0.0%)  	leaq	1(%r15),%r15
    .         
  594 (0.0%)  	mulq	%rbp
  594 (0.0%)  	cmpq	%r9,%r15
  594 (0.0%)  	jne	.Linner
    .         
  198 (0.0%)  	addq	%rax,%r13
  198 (0.0%)  	movq	(%rsi),%rax
  198 (0.0%)  	adcq	$0,%rdx
  198 (0.0%)  	addq	%r10,%r13
  198 (0.0%)  	movq	(%rsp,%r15,8),%r10
  198 (0.0%)  	adcq	$0,%rdx
  198 (0.0%)  	movq	%r13,-16(%rsp,%r15,8)
  198 (0.0%)  	movq	%rdx,%r13
    .         
  198 (0.0%)  	xorq	%rdx,%rdx
  198 (0.0%)  	addq	%r11,%r13
  198 (0.0%)  	adcq	$0,%rdx
  198 (0.0%)  	addq	%r10,%r13
  198 (0.0%)  	adcq	$0,%rdx
  198 (0.0%)  	movq	%r13,-8(%rsp,%r9,8)
  198 (0.0%)  	movq	%rdx,(%rsp,%r9,8)
    .         
  198 (0.0%)  	leaq	1(%r14),%r14
  198 (0.0%)  	cmpq	%r9,%r14
  198 (0.0%)  	jb	.Louter
    .         
   66 (0.0%)  	xorq	%r14,%r14
   66 (0.0%)  	movq	(%rsp),%rax
   66 (0.0%)  	movq	%r9,%r15
    .         
    .         .align	16
  264 (0.0%)  .Lsub:	sbbq	(%rcx,%r14,8),%rax
  264 (0.0%)  	movq	%rax,(%rdi,%r14,8)
  264 (0.0%)  	movq	8(%rsp,%r14,8),%rax
  264 (0.0%)  	leaq	1(%r14),%r14
  264 (0.0%)  	decq	%r15
  264 (0.0%)  	jnz	.Lsub
    .         
   66 (0.0%)  	sbbq	$0,%rax
   66 (0.0%)  	movq	$-1,%rbx
   66 (0.0%)  	xorq	%rax,%rbx
   66 (0.0%)  	xorq	%r14,%r14
   66 (0.0%)  	movq	%r9,%r15
    .         
    .         .Lcopy:
  264 (0.0%)  	movq	(%rdi,%r14,8),%rcx
  264 (0.0%)  	movq	(%rsp,%r14,8),%rdx
  264 (0.0%)  	andq	%rbx,%rcx
  264 (0.0%)  	andq	%rax,%rdx
  264 (0.0%)  	movq	%r9,(%rsp,%r14,8)
  264 (0.0%)  	orq	%rcx,%rdx
  264 (0.0%)  	movq	%rdx,(%rdi,%r14,8)
  264 (0.0%)  	leaq	1(%r14),%r14
  264 (0.0%)  	subq	$1,%r15
  264 (0.0%)  	jnz	.Lcopy
    .         
   66 (0.0%)  	movq	8(%rsp,%r9,8),%rsi
    .         .cfi_def_cfa	%rsi,8
   66 (0.0%)  	movq	$1,%rax
   66 (0.0%)  	movq	-48(%rsi),%r15
    .         .cfi_restore	%r15
   66 (0.0%)  	movq	-40(%rsi),%r14
    .         .cfi_restore	%r14
   66 (0.0%)  	movq	-32(%rsi),%r13
    .         .cfi_restore	%r13
   66 (0.0%)  	movq	-24(%rsi),%r12
    .         .cfi_restore	%r12
   66 (0.0%)  	movq	-16(%rsi),%rbp
    .         .cfi_restore	%rbp
   66 (0.0%)  	movq	-8(%rsi),%rbx
    .         .cfi_restore	%rbx
  132 (0.0%)  	leaq	(%rsi),%rsp
    .         .cfi_def_cfa_register	%rsp
    .         .Lmul_epilogue:
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	bn_mul_mont,.-bn_mul_mont
    .         .type	bn_mul4x_mont,@function
    .         .align	16
    .         bn_mul4x_mont:
    .         .cfi_startproc	
    .         	movl	%r9d,%r9d
    .         	movq	%rsp,%rax
    .         .cfi_def_cfa_register	%rax
    .         .Lmul4x_enter:
   12 (0.0%)  	andl	$0x80100,%r11d
   12 (0.0%)  	cmpl	$0x80100,%r11d
   12 (0.0%)  	je	.Lmulx4x_enter
   12 (0.0%)  	pushq	%rbx
    .         .cfi_offset	%rbx,-16
   12 (0.0%)  	pushq	%rbp
    .         .cfi_offset	%rbp,-24
   12 (0.0%)  	pushq	%r12
    .         .cfi_offset	%r12,-32
   12 (0.0%)  	pushq	%r13
    .         .cfi_offset	%r13,-40
   12 (0.0%)  	pushq	%r14
    .         .cfi_offset	%r14,-48
   12 (0.0%)  	pushq	%r15
    .         .cfi_offset	%r15,-56
    .         
   12 (0.0%)  	negq	%r9
   12 (0.0%)  	movq	%rsp,%r11
   12 (0.0%)  	leaq	-32(%rsp,%r9,8),%r10
   12 (0.0%)  	negq	%r9
   12 (0.0%)  	andq	$-1024,%r10
    .         
   12 (0.0%)  	subq	%r10,%r11
   12 (0.0%)  	andq	$-4096,%r11
   12 (0.0%)  	leaq	(%r10,%r11,1),%rsp
   12 (0.0%)  	movq	(%rsp),%r11
   12 (0.0%)  	cmpq	%r10,%rsp
   12 (0.0%)  	ja	.Lmul4x_page_walk
   12 (0.0%)  	jmp	.Lmul4x_page_walk_done
    .         
    .         .Lmul4x_page_walk:
    .         	leaq	-4096(%rsp),%rsp
    .         	movq	(%rsp),%r11
    .         	cmpq	%r10,%rsp
    .         	ja	.Lmul4x_page_walk
    .         .Lmul4x_page_walk_done:
    .         
   12 (0.0%)  	movq	%rax,8(%rsp,%r9,8)
    .         .cfi_escape	0x0f,0x0a,0x77,0x08,0x79,0x00,0x38,0x1e,0x22,0x06,0x23,0x08
    .         .Lmul4x_body:
   12 (0.0%)  	movq	%rdi,16(%rsp,%r9,8)
   12 (0.0%)  	movq	%rdx,%r12
   12 (0.0%)  	movq	(%r8),%r8
   12 (0.0%)  	movq	(%r12),%rbx
   12 (0.0%)  	movq	(%rsi),%rax
    .         
   12 (0.0%)  	xorq	%r14,%r14
   12 (0.0%)  	xorq	%r15,%r15
    .         
   12 (0.0%)  	movq	%r8,%rbp
   12 (0.0%)  	mulq	%rbx
   12 (0.0%)  	movq	%rax,%r10
   12 (0.0%)  	movq	(%rcx),%rax
    .         
   12 (0.0%)  	imulq	%r10,%rbp
   12 (0.0%)  	movq	%rdx,%r11
    .         
   12 (0.0%)  	mulq	%rbp
   12 (0.0%)  	addq	%rax,%r10
   12 (0.0%)  	movq	8(%rsi),%rax
   12 (0.0%)  	adcq	$0,%rdx
   12 (0.0%)  	movq	%rdx,%rdi
    .         
   12 (0.0%)  	mulq	%rbx
   12 (0.0%)  	addq	%rax,%r11
   12 (0.0%)  	movq	8(%rcx),%rax
   12 (0.0%)  	adcq	$0,%rdx
   12 (0.0%)  	movq	%rdx,%r10
    .         
   12 (0.0%)  	mulq	%rbp
   12 (0.0%)  	addq	%rax,%rdi
   12 (0.0%)  	movq	16(%rsi),%rax
   12 (0.0%)  	adcq	$0,%rdx
   12 (0.0%)  	addq	%r11,%rdi
   12 (0.0%)  	leaq	4(%r15),%r15
   12 (0.0%)  	adcq	$0,%rdx
   12 (0.0%)  	movq	%rdi,(%rsp)
   12 (0.0%)  	movq	%rdx,%r13
   12 (0.0%)  	jmp	.L1st4x
    .         .align	16
    .         .L1st4x:
  132 (0.0%)  	mulq	%rbx
  132 (0.0%)  	addq	%rax,%r10
  132 (0.0%)  	movq	-16(%rcx,%r15,8),%rax
  132 (0.0%)  	adcq	$0,%rdx
  132 (0.0%)  	movq	%rdx,%r11
    .         
  132 (0.0%)  	mulq	%rbp
  132 (0.0%)  	addq	%rax,%r13
  132 (0.0%)  	movq	-8(%rsi,%r15,8),%rax
  132 (0.0%)  	adcq	$0,%rdx
  132 (0.0%)  	addq	%r10,%r13
  132 (0.0%)  	adcq	$0,%rdx
  132 (0.0%)  	movq	%r13,-24(%rsp,%r15,8)
  132 (0.0%)  	movq	%rdx,%rdi
    .         
  132 (0.0%)  	mulq	%rbx
  132 (0.0%)  	addq	%rax,%r11
  132 (0.0%)  	movq	-8(%rcx,%r15,8),%rax
  132 (0.0%)  	adcq	$0,%rdx
  132 (0.0%)  	movq	%rdx,%r10
    .         
  132 (0.0%)  	mulq	%rbp
  132 (0.0%)  	addq	%rax,%rdi
  132 (0.0%)  	movq	(%rsi,%r15,8),%rax
  132 (0.0%)  	adcq	$0,%rdx
  132 (0.0%)  	addq	%r11,%rdi
  132 (0.0%)  	adcq	$0,%rdx
  132 (0.0%)  	movq	%rdi,-16(%rsp,%r15,8)
  132 (0.0%)  	movq	%rdx,%r13
    .         
  132 (0.0%)  	mulq	%rbx
  132 (0.0%)  	addq	%rax,%r10
  132 (0.0%)  	movq	(%rcx,%r15,8),%rax
  132 (0.0%)  	adcq	$0,%rdx
  132 (0.0%)  	movq	%rdx,%r11
    .         
  132 (0.0%)  	mulq	%rbp
  132 (0.0%)  	addq	%rax,%r13
  132 (0.0%)  	movq	8(%rsi,%r15,8),%rax
  132 (0.0%)  	adcq	$0,%rdx
  132 (0.0%)  	addq	%r10,%r13
  132 (0.0%)  	adcq	$0,%rdx
  132 (0.0%)  	movq	%r13,-8(%rsp,%r15,8)
  132 (0.0%)  	movq	%rdx,%rdi
    .         
  132 (0.0%)  	mulq	%rbx
  132 (0.0%)  	addq	%rax,%r11
  132 (0.0%)  	movq	8(%rcx,%r15,8),%rax
  132 (0.0%)  	adcq	$0,%rdx
  132 (0.0%)  	leaq	4(%r15),%r15
  132 (0.0%)  	movq	%rdx,%r10
    .         
  132 (0.0%)  	mulq	%rbp
  132 (0.0%)  	addq	%rax,%rdi
  132 (0.0%)  	movq	-16(%rsi,%r15,8),%rax
  132 (0.0%)  	adcq	$0,%rdx
  132 (0.0%)  	addq	%r11,%rdi
  132 (0.0%)  	adcq	$0,%rdx
  132 (0.0%)  	movq	%rdi,-32(%rsp,%r15,8)
  132 (0.0%)  	movq	%rdx,%r13
  132 (0.0%)  	cmpq	%r9,%r15
  132 (0.0%)  	jb	.L1st4x
    .         
   12 (0.0%)  	mulq	%rbx
   12 (0.0%)  	addq	%rax,%r10
   12 (0.0%)  	movq	-16(%rcx,%r15,8),%rax
   12 (0.0%)  	adcq	$0,%rdx
   12 (0.0%)  	movq	%rdx,%r11
    .         
   12 (0.0%)  	mulq	%rbp
   12 (0.0%)  	addq	%rax,%r13
   12 (0.0%)  	movq	-8(%rsi,%r15,8),%rax
   12 (0.0%)  	adcq	$0,%rdx
   12 (0.0%)  	addq	%r10,%r13
   12 (0.0%)  	adcq	$0,%rdx
   12 (0.0%)  	movq	%r13,-24(%rsp,%r15,8)
   12 (0.0%)  	movq	%rdx,%rdi
    .         
   12 (0.0%)  	mulq	%rbx
   12 (0.0%)  	addq	%rax,%r11
   12 (0.0%)  	movq	-8(%rcx,%r15,8),%rax
   12 (0.0%)  	adcq	$0,%rdx
   12 (0.0%)  	movq	%rdx,%r10
    .         
   12 (0.0%)  	mulq	%rbp
   12 (0.0%)  	addq	%rax,%rdi
   12 (0.0%)  	movq	(%rsi),%rax
   12 (0.0%)  	adcq	$0,%rdx
   12 (0.0%)  	addq	%r11,%rdi
   12 (0.0%)  	adcq	$0,%rdx
   12 (0.0%)  	movq	%rdi,-16(%rsp,%r15,8)
   12 (0.0%)  	movq	%rdx,%r13
    .         
   12 (0.0%)  	xorq	%rdi,%rdi
   12 (0.0%)  	addq	%r10,%r13
   12 (0.0%)  	adcq	$0,%rdi
   12 (0.0%)  	movq	%r13,-8(%rsp,%r15,8)
   12 (0.0%)  	movq	%rdi,(%rsp,%r15,8)
    .         
   12 (0.0%)  	leaq	1(%r14),%r14
    .         .align	4
    .         .Louter4x:
  564 (0.0%)  	movq	(%r12,%r14,8),%rbx
  564 (0.0%)  	xorq	%r15,%r15
  564 (0.0%)  	movq	(%rsp),%r10
  564 (0.0%)  	movq	%r8,%rbp
  564 (0.0%)  	mulq	%rbx
  564 (0.0%)  	addq	%rax,%r10
  564 (0.0%)  	movq	(%rcx),%rax
  564 (0.0%)  	adcq	$0,%rdx
    .         
  564 (0.0%)  	imulq	%r10,%rbp
  564 (0.0%)  	movq	%rdx,%r11
    .         
  564 (0.0%)  	mulq	%rbp
  564 (0.0%)  	addq	%rax,%r10
  564 (0.0%)  	movq	8(%rsi),%rax
  564 (0.0%)  	adcq	$0,%rdx
  564 (0.0%)  	movq	%rdx,%rdi
    .         
  564 (0.0%)  	mulq	%rbx
  564 (0.0%)  	addq	%rax,%r11
  564 (0.0%)  	movq	8(%rcx),%rax
  564 (0.0%)  	adcq	$0,%rdx
  564 (0.0%)  	addq	8(%rsp),%r11
  564 (0.0%)  	adcq	$0,%rdx
  564 (0.0%)  	movq	%rdx,%r10
    .         
  564 (0.0%)  	mulq	%rbp
  564 (0.0%)  	addq	%rax,%rdi
  564 (0.0%)  	movq	16(%rsi),%rax
  564 (0.0%)  	adcq	$0,%rdx
  564 (0.0%)  	addq	%r11,%rdi
  564 (0.0%)  	leaq	4(%r15),%r15
  564 (0.0%)  	adcq	$0,%rdx
  564 (0.0%)  	movq	%rdi,(%rsp)
  564 (0.0%)  	movq	%rdx,%r13
  564 (0.0%)  	jmp	.Linner4x
    .         .align	16
    .         .Linner4x:
6,972 (0.0%)  	mulq	%rbx
6,972 (0.0%)  	addq	%rax,%r10
6,972 (0.0%)  	movq	-16(%rcx,%r15,8),%rax
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	addq	-16(%rsp,%r15,8),%r10
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	movq	%rdx,%r11
    .         
6,972 (0.0%)  	mulq	%rbp
6,972 (0.0%)  	addq	%rax,%r13
6,972 (0.0%)  	movq	-8(%rsi,%r15,8),%rax
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	addq	%r10,%r13
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	movq	%r13,-24(%rsp,%r15,8)
6,972 (0.0%)  	movq	%rdx,%rdi
    .         
6,972 (0.0%)  	mulq	%rbx
6,972 (0.0%)  	addq	%rax,%r11
6,972 (0.0%)  	movq	-8(%rcx,%r15,8),%rax
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	addq	-8(%rsp,%r15,8),%r11
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	movq	%rdx,%r10
    .         
6,972 (0.0%)  	mulq	%rbp
6,972 (0.0%)  	addq	%rax,%rdi
6,972 (0.0%)  	movq	(%rsi,%r15,8),%rax
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	addq	%r11,%rdi
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	movq	%rdi,-16(%rsp,%r15,8)
6,972 (0.0%)  	movq	%rdx,%r13
    .         
6,972 (0.0%)  	mulq	%rbx
6,972 (0.0%)  	addq	%rax,%r10
6,972 (0.0%)  	movq	(%rcx,%r15,8),%rax
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	addq	(%rsp,%r15,8),%r10
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	movq	%rdx,%r11
    .         
6,972 (0.0%)  	mulq	%rbp
6,972 (0.0%)  	addq	%rax,%r13
6,972 (0.0%)  	movq	8(%rsi,%r15,8),%rax
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	addq	%r10,%r13
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	movq	%r13,-8(%rsp,%r15,8)
6,972 (0.0%)  	movq	%rdx,%rdi
    .         
6,972 (0.0%)  	mulq	%rbx
6,972 (0.0%)  	addq	%rax,%r11
6,972 (0.0%)  	movq	8(%rcx,%r15,8),%rax
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	addq	8(%rsp,%r15,8),%r11
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	leaq	4(%r15),%r15
6,972 (0.0%)  	movq	%rdx,%r10
    .         
6,972 (0.0%)  	mulq	%rbp
6,972 (0.0%)  	addq	%rax,%rdi
6,972 (0.0%)  	movq	-16(%rsi,%r15,8),%rax
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	addq	%r11,%rdi
6,972 (0.0%)  	adcq	$0,%rdx
6,972 (0.0%)  	movq	%rdi,-32(%rsp,%r15,8)
6,972 (0.0%)  	movq	%rdx,%r13
6,972 (0.0%)  	cmpq	%r9,%r15
6,972 (0.0%)  	jb	.Linner4x
    .         
  564 (0.0%)  	mulq	%rbx
  564 (0.0%)  	addq	%rax,%r10
  564 (0.0%)  	movq	-16(%rcx,%r15,8),%rax
  564 (0.0%)  	adcq	$0,%rdx
  564 (0.0%)  	addq	-16(%rsp,%r15,8),%r10
  564 (0.0%)  	adcq	$0,%rdx
  564 (0.0%)  	movq	%rdx,%r11
    .         
  564 (0.0%)  	mulq	%rbp
  564 (0.0%)  	addq	%rax,%r13
  564 (0.0%)  	movq	-8(%rsi,%r15,8),%rax
  564 (0.0%)  	adcq	$0,%rdx
  564 (0.0%)  	addq	%r10,%r13
  564 (0.0%)  	adcq	$0,%rdx
  564 (0.0%)  	movq	%r13,-24(%rsp,%r15,8)
  564 (0.0%)  	movq	%rdx,%rdi
    .         
  564 (0.0%)  	mulq	%rbx
  564 (0.0%)  	addq	%rax,%r11
  564 (0.0%)  	movq	-8(%rcx,%r15,8),%rax
  564 (0.0%)  	adcq	$0,%rdx
  564 (0.0%)  	addq	-8(%rsp,%r15,8),%r11
  564 (0.0%)  	adcq	$0,%rdx
  564 (0.0%)  	leaq	1(%r14),%r14
  564 (0.0%)  	movq	%rdx,%r10
    .         
  564 (0.0%)  	mulq	%rbp
  564 (0.0%)  	addq	%rax,%rdi
  564 (0.0%)  	movq	(%rsi),%rax
  564 (0.0%)  	adcq	$0,%rdx
  564 (0.0%)  	addq	%r11,%rdi
  564 (0.0%)  	adcq	$0,%rdx
  564 (0.0%)  	movq	%rdi,-16(%rsp,%r15,8)
  564 (0.0%)  	movq	%rdx,%r13
    .         
  564 (0.0%)  	xorq	%rdi,%rdi
  564 (0.0%)  	addq	%r10,%r13
  564 (0.0%)  	adcq	$0,%rdi
  564 (0.0%)  	addq	(%rsp,%r9,8),%r13
  564 (0.0%)  	adcq	$0,%rdi
  564 (0.0%)  	movq	%r13,-8(%rsp,%r15,8)
  564 (0.0%)  	movq	%rdi,(%rsp,%r15,8)
    .         
  564 (0.0%)  	cmpq	%r9,%r14
  564 (0.0%)  	jb	.Louter4x
   12 (0.0%)  	movq	16(%rsp,%r9,8),%rdi
   12 (0.0%)  	leaq	-4(%r9),%r15
   12 (0.0%)  	movq	0(%rsp),%rax
   12 (0.0%)  	movq	8(%rsp),%rdx
   12 (0.0%)  	shrq	$2,%r15
   12 (0.0%)  	leaq	(%rsp),%rsi
   12 (0.0%)  	xorq	%r14,%r14
    .         
   12 (0.0%)  	subq	0(%rcx),%rax
   12 (0.0%)  	movq	16(%rsi),%rbx
   12 (0.0%)  	movq	24(%rsi),%rbp
   12 (0.0%)  	sbbq	8(%rcx),%rdx
    .         
    .         .Lsub4x:
  132 (0.0%)  	movq	%rax,0(%rdi,%r14,8)
  132 (0.0%)  	movq	%rdx,8(%rdi,%r14,8)
  132 (0.0%)  	sbbq	16(%rcx,%r14,8),%rbx
  132 (0.0%)  	movq	32(%rsi,%r14,8),%rax
  132 (0.0%)  	movq	40(%rsi,%r14,8),%rdx
  132 (0.0%)  	sbbq	24(%rcx,%r14,8),%rbp
  132 (0.0%)  	movq	%rbx,16(%rdi,%r14,8)
  132 (0.0%)  	movq	%rbp,24(%rdi,%r14,8)
  132 (0.0%)  	sbbq	32(%rcx,%r14,8),%rax
  132 (0.0%)  	movq	48(%rsi,%r14,8),%rbx
  132 (0.0%)  	movq	56(%rsi,%r14,8),%rbp
  132 (0.0%)  	sbbq	40(%rcx,%r14,8),%rdx
  132 (0.0%)  	leaq	4(%r14),%r14
  132 (0.0%)  	decq	%r15
  132 (0.0%)  	jnz	.Lsub4x
    .         
   12 (0.0%)  	movq	%rax,0(%rdi,%r14,8)
   12 (0.0%)  	movq	32(%rsi,%r14,8),%rax
   12 (0.0%)  	sbbq	16(%rcx,%r14,8),%rbx
   12 (0.0%)  	movq	%rdx,8(%rdi,%r14,8)
   12 (0.0%)  	sbbq	24(%rcx,%r14,8),%rbp
   12 (0.0%)  	movq	%rbx,16(%rdi,%r14,8)
    .         
   12 (0.0%)  	sbbq	$0,%rax
   12 (0.0%)  	movq	%rbp,24(%rdi,%r14,8)
   24 (0.0%)  	pxor	%xmm0,%xmm0
    .         .byte	102,72,15,110,224
   12 (0.0%)  	pcmpeqd	%xmm5,%xmm5
   12 (0.0%)  	pshufd	$0,%xmm4,%xmm4
   12 (0.0%)  	movq	%r9,%r15
   12 (0.0%)  	pxor	%xmm4,%xmm5
   12 (0.0%)  	shrq	$2,%r15
   12 (0.0%)  	xorl	%eax,%eax
    .         
   12 (0.0%)  	jmp	.Lcopy4x
    .         .align	16
    .         .Lcopy4x:
  144 (0.0%)  	movdqa	(%rsp,%rax,1),%xmm1
  144 (0.0%)  	movdqu	(%rdi,%rax,1),%xmm2
  144 (0.0%)  	pand	%xmm4,%xmm1
  144 (0.0%)  	pand	%xmm5,%xmm2
  144 (0.0%)  	movdqa	16(%rsp,%rax,1),%xmm3
  144 (0.0%)  	movdqa	%xmm0,(%rsp,%rax,1)
  144 (0.0%)  	por	%xmm2,%xmm1
  144 (0.0%)  	movdqu	16(%rdi,%rax,1),%xmm2
  144 (0.0%)  	movdqu	%xmm1,(%rdi,%rax,1)
  144 (0.0%)  	pand	%xmm4,%xmm3
  144 (0.0%)  	pand	%xmm5,%xmm2
  144 (0.0%)  	movdqa	%xmm0,16(%rsp,%rax,1)
  144 (0.0%)  	por	%xmm2,%xmm3
  144 (0.0%)  	movdqu	%xmm3,16(%rdi,%rax,1)
  144 (0.0%)  	leaq	32(%rax),%rax
  144 (0.0%)  	decq	%r15
  144 (0.0%)  	jnz	.Lcopy4x
   12 (0.0%)  	movq	8(%rsp,%r9,8),%rsi
    .         .cfi_def_cfa	%rsi, 8
   12 (0.0%)  	movq	$1,%rax
   12 (0.0%)  	movq	-48(%rsi),%r15
    .         .cfi_restore	%r15
   12 (0.0%)  	movq	-40(%rsi),%r14
    .         .cfi_restore	%r14
   12 (0.0%)  	movq	-32(%rsi),%r13
    .         .cfi_restore	%r13
   12 (0.0%)  	movq	-24(%rsi),%r12
    .         .cfi_restore	%r12
   12 (0.0%)  	movq	-16(%rsi),%rbp
    .         .cfi_restore	%rbp
   12 (0.0%)  	movq	-8(%rsi),%rbx
    .         .cfi_restore	%rbx
   24 (0.0%)  	leaq	(%rsi),%rsp
    .         .cfi_def_cfa_register	%rsp
    .         .Lmul4x_epilogue:
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	bn_mul4x_mont,.-bn_mul4x_mont
    .         .extern	bn_sqrx8x_internal
    .         .hidden bn_sqrx8x_internal
    .         .extern	bn_sqr8x_internal
-- line 703 ----------------------------------------
-- line 705 ----------------------------------------
    .         
    .         .type	bn_sqr8x_mont,@function
    .         .align	32
    .         bn_sqr8x_mont:
    .         .cfi_startproc	
    .         	movq	%rsp,%rax
    .         .cfi_def_cfa_register	%rax
    .         .Lsqr8x_enter:
   64 (0.0%)  	pushq	%rbx
    .         .cfi_offset	%rbx,-16
   64 (0.0%)  	pushq	%rbp
    .         .cfi_offset	%rbp,-24
   64 (0.0%)  	pushq	%r12
    .         .cfi_offset	%r12,-32
   64 (0.0%)  	pushq	%r13
    .         .cfi_offset	%r13,-40
   64 (0.0%)  	pushq	%r14
    .         .cfi_offset	%r14,-48
   64 (0.0%)  	pushq	%r15
    .         .cfi_offset	%r15,-56
    .         .Lsqr8x_prologue:
    .         
   64 (0.0%)  	movl	%r9d,%r10d
   64 (0.0%)  	shll	$3,%r9d
   64 (0.0%)  	shlq	$3+2,%r10
   64 (0.0%)  	negq	%r9
    .         
    .         
    .         
    .         
    .         
    .         
   64 (0.0%)  	leaq	-64(%rsp,%r9,2),%r11
   64 (0.0%)  	movq	%rsp,%rbp
   64 (0.0%)  	movq	(%r8),%r8
   64 (0.0%)  	subq	%rsi,%r11
   64 (0.0%)  	andq	$4095,%r11
   64 (0.0%)  	cmpq	%r11,%r10
   64 (0.0%)  	jb	.Lsqr8x_sp_alt
   17 (0.0%)  	subq	%r11,%rbp
   17 (0.0%)  	leaq	-64(%rbp,%r9,2),%rbp
   17 (0.0%)  	jmp	.Lsqr8x_sp_done
    .         
    .         .align	32
    .         .Lsqr8x_sp_alt:
   47 (0.0%)  	leaq	4096-64(,%r9,2),%r10
   47 (0.0%)  	leaq	-64(%rbp,%r9,2),%rbp
   47 (0.0%)  	subq	%r10,%r11
   47 (0.0%)  	movq	$0,%r10
   47 (0.0%)  	cmovcq	%r10,%r11
   47 (0.0%)  	subq	%r11,%rbp
    .         .Lsqr8x_sp_done:
   64 (0.0%)  	andq	$-64,%rbp
   64 (0.0%)  	movq	%rsp,%r11
   64 (0.0%)  	subq	%rbp,%r11
   64 (0.0%)  	andq	$-4096,%r11
   64 (0.0%)  	leaq	(%r11,%rbp,1),%rsp
   64 (0.0%)  	movq	(%rsp),%r10
   64 (0.0%)  	cmpq	%rbp,%rsp
   64 (0.0%)  	ja	.Lsqr8x_page_walk
   64 (0.0%)  	jmp	.Lsqr8x_page_walk_done
    .         
    .         .align	16
    .         .Lsqr8x_page_walk:
    .         	leaq	-4096(%rsp),%rsp
    .         	movq	(%rsp),%r10
    .         	cmpq	%rbp,%rsp
    .         	ja	.Lsqr8x_page_walk
    .         .Lsqr8x_page_walk_done:
    .         
   64 (0.0%)  	movq	%r9,%r10
   64 (0.0%)  	negq	%r9
    .         
   64 (0.0%)  	movq	%r8,32(%rsp)
  128 (0.0%)  	movq	%rax,40(%rsp)
    .         .cfi_escape	0x0f,0x05,0x77,0x28,0x06,0x23,0x08
    .         .Lsqr8x_body:
    .         
    .         .byte	102,72,15,110,209
  192 (0.0%)  	pxor	%xmm0,%xmm0
    .         .byte	102,72,15,110,207
    .         .byte	102,73,15,110,218
   64 (0.0%)  	leaq	OPENSSL_ia32cap_P(%rip),%rax
   64 (0.0%)  	movl	8(%rax),%eax
   64 (0.0%)  	andl	$0x80100,%eax
   64 (0.0%)  	cmpl	$0x80100,%eax
   64 (0.0%)  	jne	.Lsqr8x_nox
    .         
    .         	call	bn_sqrx8x_internal
    .         
    .         
    .         
    .         
    .         	leaq	(%r8,%rcx,1),%rbx
    .         	movq	%rcx,%r9
    .         	movq	%rcx,%rdx
    .         .byte	102,72,15,126,207
    .         	sarq	$3+2,%rcx
    .         	jmp	.Lsqr8x_sub
    .         
    .         .align	32
    .         .Lsqr8x_nox:
   64 (0.0%)  	call	bn_sqr8x_internal
    .         
    .         
    .         
    .         
   64 (0.0%)  	leaq	(%rdi,%r9,1),%rbx
   64 (0.0%)  	movq	%r9,%rcx
  128 (0.0%)  	movq	%r9,%rdx
    .         .byte	102,72,15,126,207
   64 (0.0%)  	sarq	$3+2,%rcx
   64 (0.0%)  	jmp	.Lsqr8x_sub
    .         
    .         .align	32
    .         .Lsqr8x_sub:
  784 (0.0%)  	movq	0(%rbx),%r12
  784 (0.0%)  	movq	8(%rbx),%r13
  784 (0.0%)  	movq	16(%rbx),%r14
  784 (0.0%)  	movq	24(%rbx),%r15
  784 (0.0%)  	leaq	32(%rbx),%rbx
  784 (0.0%)  	sbbq	0(%rbp),%r12
  784 (0.0%)  	sbbq	8(%rbp),%r13
  784 (0.0%)  	sbbq	16(%rbp),%r14
  784 (0.0%)  	sbbq	24(%rbp),%r15
  784 (0.0%)  	leaq	32(%rbp),%rbp
  784 (0.0%)  	movq	%r12,0(%rdi)
  784 (0.0%)  	movq	%r13,8(%rdi)
  784 (0.0%)  	movq	%r14,16(%rdi)
  784 (0.0%)  	movq	%r15,24(%rdi)
  784 (0.0%)  	leaq	32(%rdi),%rdi
  784 (0.0%)  	incq	%rcx
  784 (0.0%)  	jnz	.Lsqr8x_sub
    .         
   64 (0.0%)  	sbbq	$0,%rax
   64 (0.0%)  	leaq	(%rbx,%r9,1),%rbx
  128 (0.0%)  	leaq	(%rdi,%r9,1),%rdi
    .         
    .         .byte	102,72,15,110,200
   64 (0.0%)  	pxor	%xmm0,%xmm0
   64 (0.0%)  	pshufd	$0,%xmm1,%xmm1
   64 (0.0%)  	movq	40(%rsp),%rsi
    .         .cfi_def_cfa	%rsi,8
   64 (0.0%)  	jmp	.Lsqr8x_cond_copy
    .         
    .         .align	32
    .         .Lsqr8x_cond_copy:
  784 (0.0%)  	movdqa	0(%rbx),%xmm2
  784 (0.0%)  	movdqa	16(%rbx),%xmm3
  784 (0.0%)  	leaq	32(%rbx),%rbx
  784 (0.0%)  	movdqu	0(%rdi),%xmm4
  784 (0.0%)  	movdqu	16(%rdi),%xmm5
  784 (0.0%)  	leaq	32(%rdi),%rdi
  784 (0.0%)  	movdqa	%xmm0,-32(%rbx)
  784 (0.0%)  	movdqa	%xmm0,-16(%rbx)
  784 (0.0%)  	movdqa	%xmm0,-32(%rbx,%rdx,1)
  784 (0.0%)  	movdqa	%xmm0,-16(%rbx,%rdx,1)
  784 (0.0%)  	pcmpeqd	%xmm1,%xmm0
  784 (0.0%)  	pand	%xmm1,%xmm2
  784 (0.0%)  	pand	%xmm1,%xmm3
  784 (0.0%)  	pand	%xmm0,%xmm4
  784 (0.0%)  	pand	%xmm0,%xmm5
  784 (0.0%)  	pxor	%xmm0,%xmm0
  784 (0.0%)  	por	%xmm2,%xmm4
  784 (0.0%)  	por	%xmm3,%xmm5
  784 (0.0%)  	movdqu	%xmm4,-32(%rdi)
  784 (0.0%)  	movdqu	%xmm5,-16(%rdi)
  784 (0.0%)  	addq	$32,%r9
  784 (0.0%)  	jnz	.Lsqr8x_cond_copy
    .         
   64 (0.0%)  	movq	$1,%rax
   64 (0.0%)  	movq	-48(%rsi),%r15
    .         .cfi_restore	%r15
   64 (0.0%)  	movq	-40(%rsi),%r14
    .         .cfi_restore	%r14
   64 (0.0%)  	movq	-32(%rsi),%r13
    .         .cfi_restore	%r13
   64 (0.0%)  	movq	-24(%rsi),%r12
    .         .cfi_restore	%r12
   64 (0.0%)  	movq	-16(%rsi),%rbp
    .         .cfi_restore	%rbp
   64 (0.0%)  	movq	-8(%rsi),%rbx
    .         .cfi_restore	%rbx
  128 (0.0%)  	leaq	(%rsi),%rsp
    .         .cfi_def_cfa_register	%rsp
    .         .Lsqr8x_epilogue:
    .         	.byte	0xf3,0xc3
    .         .cfi_endproc	
    .         .size	bn_sqr8x_mont,.-bn_sqr8x_mont
    .         .type	bn_mulx4x_mont,@function
    .         .align	32
    .         bn_mulx4x_mont:
-- line 896 ----------------------------------------

--------------------------------------------------------------------------------
-- Annotated source file: /home/ubuntu/.cargo/registry/src/index.crates.io-6f17d22bba15001f/aws-lc-sys-0.19.0/aws-lc/generated-src/linux-x86_64/crypto/fipsmodule/x86_64-mont5.S
--------------------------------------------------------------------------------
Ir____________ 

-- line 10 ----------------------------------------
      .         .hidden OPENSSL_ia32cap_P
      .         
      .         .globl	bn_mul_mont_gather5
      .         .hidden bn_mul_mont_gather5
      .         .type	bn_mul_mont_gather5,@function
      .         .align	64
      .         bn_mul_mont_gather5:
      .         .cfi_startproc	
     30 (0.0%)  	movl	%r9d,%r9d
     30 (0.0%)  	movq	%rsp,%rax
      .         .cfi_def_cfa_register	%rax
     30 (0.0%)  	testl	$7,%r9d
     30 (0.0%)  	jnz	.Lmul_enter
     30 (0.0%)  	leaq	OPENSSL_ia32cap_P(%rip),%r11
     30 (0.0%)  	movl	8(%r11),%r11d
     30 (0.0%)  	jmp	.Lmul4x_enter
      .         
      .         .align	16
      .         .Lmul_enter:
      .         	movd	8(%rsp),%xmm5
      .         	pushq	%rbx
      .         .cfi_offset	%rbx,-16
      .         	pushq	%rbp
      .         .cfi_offset	%rbp,-24
-- line 33 ----------------------------------------
-- line 455 ----------------------------------------
      .         .type	bn_mul4x_mont_gather5,@function
      .         .align	32
      .         bn_mul4x_mont_gather5:
      .         .cfi_startproc	
      .         .byte	0x67
      .         	movq	%rsp,%rax
      .         .cfi_def_cfa_register	%rax
      .         .Lmul4x_enter:
     30 (0.0%)  	andl	$0x80108,%r11d
     30 (0.0%)  	cmpl	$0x80108,%r11d
     30 (0.0%)  	je	.Lmulx4x_enter
     30 (0.0%)  	pushq	%rbx
      .         .cfi_offset	%rbx,-16
     30 (0.0%)  	pushq	%rbp
      .         .cfi_offset	%rbp,-24
     30 (0.0%)  	pushq	%r12
      .         .cfi_offset	%r12,-32
     30 (0.0%)  	pushq	%r13
      .         .cfi_offset	%r13,-40
     30 (0.0%)  	pushq	%r14
      .         .cfi_offset	%r14,-48
     60 (0.0%)  	pushq	%r15
      .         .cfi_offset	%r15,-56
      .         .Lmul4x_prologue:
      .         
      .         .byte	0x67
      .         	shll	$3,%r9d
     30 (0.0%)  	leaq	(%r9,%r9,2),%r10
     30 (0.0%)  	negq	%r9
      .         
      .         
      .         
      .         
      .         
      .         
      .         
      .         
      .         
      .         
     30 (0.0%)  	leaq	-320(%rsp,%r9,2),%r11
     30 (0.0%)  	movq	%rsp,%rbp
     30 (0.0%)  	subq	%rdi,%r11
     30 (0.0%)  	andq	$4095,%r11
     30 (0.0%)  	cmpq	%r11,%r10
     30 (0.0%)  	jb	.Lmul4xsp_alt
      .         	subq	%r11,%rbp
      .         	leaq	-320(%rbp,%r9,2),%rbp
      .         	jmp	.Lmul4xsp_done
      .         
      .         .align	32
      .         .Lmul4xsp_alt:
     30 (0.0%)  	leaq	4096-320(,%r9,2),%r10
     30 (0.0%)  	leaq	-320(%rbp,%r9,2),%rbp
     30 (0.0%)  	subq	%r10,%r11
     30 (0.0%)  	movq	$0,%r10
     30 (0.0%)  	cmovcq	%r10,%r11
     30 (0.0%)  	subq	%r11,%rbp
      .         .Lmul4xsp_done:
     30 (0.0%)  	andq	$-64,%rbp
     30 (0.0%)  	movq	%rsp,%r11
     30 (0.0%)  	subq	%rbp,%r11
     30 (0.0%)  	andq	$-4096,%r11
     30 (0.0%)  	leaq	(%r11,%rbp,1),%rsp
     30 (0.0%)  	movq	(%rsp),%r10
     30 (0.0%)  	cmpq	%rbp,%rsp
     30 (0.0%)  	ja	.Lmul4x_page_walk
     30 (0.0%)  	jmp	.Lmul4x_page_walk_done
      .         
      .         .Lmul4x_page_walk:
      .         	leaq	-4096(%rsp),%rsp
      .         	movq	(%rsp),%r10
      .         	cmpq	%rbp,%rsp
      .         	ja	.Lmul4x_page_walk
      .         .Lmul4x_page_walk_done:
      .         
     30 (0.0%)  	negq	%r9
      .         
     30 (0.0%)  	movq	%rax,40(%rsp)
      .         .cfi_escape	0x0f,0x05,0x77,0x28,0x06,0x23,0x08
      .         .Lmul4x_body:
      .         
     30 (0.0%)  	call	mul4x_internal
      .         
     30 (0.0%)  	movq	40(%rsp),%rsi
      .         .cfi_def_cfa	%rsi,8
     30 (0.0%)  	movq	$1,%rax
      .         
     30 (0.0%)  	movq	-48(%rsi),%r15
      .         .cfi_restore	%r15
     30 (0.0%)  	movq	-40(%rsi),%r14
      .         .cfi_restore	%r14
     30 (0.0%)  	movq	-32(%rsi),%r13
      .         .cfi_restore	%r13
     30 (0.0%)  	movq	-24(%rsi),%r12
      .         .cfi_restore	%r12
     30 (0.0%)  	movq	-16(%rsi),%rbp
      .         .cfi_restore	%rbp
     30 (0.0%)  	movq	-8(%rsi),%rbx
      .         .cfi_restore	%rbx
     60 (0.0%)  	leaq	(%rsi),%rsp
      .         .cfi_def_cfa_register	%rsp
      .         .Lmul4x_epilogue:
      .         	.byte	0xf3,0xc3
      .         .cfi_endproc	
      .         .size	bn_mul4x_mont_gather5,.-bn_mul4x_mont_gather5
      .         
      .         .type	mul4x_internal,@function
      .         .align	32
      .         mul4x_internal:
      .         .cfi_startproc	
    848 (0.0%)  	shlq	$5,%r9
    848 (0.0%)  	movd	8(%rax),%xmm5
    848 (0.0%)  	leaq	.Linc(%rip),%rax
    848 (0.0%)  	leaq	128(%rdx,%r9,1),%r13
    848 (0.0%)  	shrq	$5,%r9
    848 (0.0%)  	movdqa	0(%rax),%xmm0
    848 (0.0%)  	movdqa	16(%rax),%xmm1
    848 (0.0%)  	leaq	88-112(%rsp,%r9,1),%r10
    848 (0.0%)  	leaq	128(%rdx),%r12
      .         
    848 (0.0%)  	pshufd	$0,%xmm5,%xmm5
  1,696 (0.0%)  	movdqa	%xmm1,%xmm4
      .         .byte	0x67,0x67
      .         	movdqa	%xmm1,%xmm2
    848 (0.0%)  	paddd	%xmm0,%xmm1
  1,696 (0.0%)  	pcmpeqd	%xmm5,%xmm0
      .         .byte	0x67
      .         	movdqa	%xmm4,%xmm3
    848 (0.0%)  	paddd	%xmm1,%xmm2
    848 (0.0%)  	pcmpeqd	%xmm5,%xmm1
    848 (0.0%)  	movdqa	%xmm0,112(%r10)
    848 (0.0%)  	movdqa	%xmm4,%xmm0
      .         
    848 (0.0%)  	paddd	%xmm2,%xmm3
    848 (0.0%)  	pcmpeqd	%xmm5,%xmm2
    848 (0.0%)  	movdqa	%xmm1,128(%r10)
    848 (0.0%)  	movdqa	%xmm4,%xmm1
      .         
    848 (0.0%)  	paddd	%xmm3,%xmm0
    848 (0.0%)  	pcmpeqd	%xmm5,%xmm3
    848 (0.0%)  	movdqa	%xmm2,144(%r10)
    848 (0.0%)  	movdqa	%xmm4,%xmm2
      .         
    848 (0.0%)  	paddd	%xmm0,%xmm1
    848 (0.0%)  	pcmpeqd	%xmm5,%xmm0
    848 (0.0%)  	movdqa	%xmm3,160(%r10)
    848 (0.0%)  	movdqa	%xmm4,%xmm3
    848 (0.0%)  	paddd	%xmm1,%xmm2
    848 (0.0%)  	pcmpeqd	%xmm5,%xmm1
    848 (0.0%)  	movdqa	%xmm0,176(%r10)
    848 (0.0%)  	movdqa	%xmm4,%xmm0
      .         
    848 (0.0%)  	paddd	%xmm2,%xmm3
    848 (0.0%)  	pcmpeqd	%xmm5,%xmm2
    848 (0.0%)  	movdqa	%xmm1,192(%r10)
    848 (0.0%)  	movdqa	%xmm4,%xmm1
      .         
    848 (0.0%)  	paddd	%xmm3,%xmm0
    848 (0.0%)  	pcmpeqd	%xmm5,%xmm3
    848 (0.0%)  	movdqa	%xmm2,208(%r10)
    848 (0.0%)  	movdqa	%xmm4,%xmm2
      .         
    848 (0.0%)  	paddd	%xmm0,%xmm1
    848 (0.0%)  	pcmpeqd	%xmm5,%xmm0
    848 (0.0%)  	movdqa	%xmm3,224(%r10)
    848 (0.0%)  	movdqa	%xmm4,%xmm3
    848 (0.0%)  	paddd	%xmm1,%xmm2
    848 (0.0%)  	pcmpeqd	%xmm5,%xmm1
    848 (0.0%)  	movdqa	%xmm0,240(%r10)
    848 (0.0%)  	movdqa	%xmm4,%xmm0
      .         
    848 (0.0%)  	paddd	%xmm2,%xmm3
    848 (0.0%)  	pcmpeqd	%xmm5,%xmm2
    848 (0.0%)  	movdqa	%xmm1,256(%r10)
    848 (0.0%)  	movdqa	%xmm4,%xmm1
      .         
    848 (0.0%)  	paddd	%xmm3,%xmm0
    848 (0.0%)  	pcmpeqd	%xmm5,%xmm3
    848 (0.0%)  	movdqa	%xmm2,272(%r10)
    848 (0.0%)  	movdqa	%xmm4,%xmm2
      .         
    848 (0.0%)  	paddd	%xmm0,%xmm1
    848 (0.0%)  	pcmpeqd	%xmm5,%xmm0
    848 (0.0%)  	movdqa	%xmm3,288(%r10)
    848 (0.0%)  	movdqa	%xmm4,%xmm3
    848 (0.0%)  	paddd	%xmm1,%xmm2
    848 (0.0%)  	pcmpeqd	%xmm5,%xmm1
    848 (0.0%)  	movdqa	%xmm0,304(%r10)
      .         
  1,696 (0.0%)  	paddd	%xmm2,%xmm3
      .         .byte	0x67
      .         	pcmpeqd	%xmm5,%xmm2
    848 (0.0%)  	movdqa	%xmm1,320(%r10)
      .         
    848 (0.0%)  	pcmpeqd	%xmm5,%xmm3
    848 (0.0%)  	movdqa	%xmm2,336(%r10)
    848 (0.0%)  	pand	64(%r12),%xmm0
      .         
    848 (0.0%)  	pand	80(%r12),%xmm1
    848 (0.0%)  	pand	96(%r12),%xmm2
    848 (0.0%)  	movdqa	%xmm3,352(%r10)
    848 (0.0%)  	pand	112(%r12),%xmm3
    848 (0.0%)  	por	%xmm2,%xmm0
    848 (0.0%)  	por	%xmm3,%xmm1
    848 (0.0%)  	movdqa	-128(%r12),%xmm4
    848 (0.0%)  	movdqa	-112(%r12),%xmm5
    848 (0.0%)  	movdqa	-96(%r12),%xmm2
    848 (0.0%)  	pand	112(%r10),%xmm4
    848 (0.0%)  	movdqa	-80(%r12),%xmm3
    848 (0.0%)  	pand	128(%r10),%xmm5
    848 (0.0%)  	por	%xmm4,%xmm0
    848 (0.0%)  	pand	144(%r10),%xmm2
    848 (0.0%)  	por	%xmm5,%xmm1
    848 (0.0%)  	pand	160(%r10),%xmm3
    848 (0.0%)  	por	%xmm2,%xmm0
    848 (0.0%)  	por	%xmm3,%xmm1
    848 (0.0%)  	movdqa	-64(%r12),%xmm4
    848 (0.0%)  	movdqa	-48(%r12),%xmm5
    848 (0.0%)  	movdqa	-32(%r12),%xmm2
    848 (0.0%)  	pand	176(%r10),%xmm4
    848 (0.0%)  	movdqa	-16(%r12),%xmm3
    848 (0.0%)  	pand	192(%r10),%xmm5
    848 (0.0%)  	por	%xmm4,%xmm0
    848 (0.0%)  	pand	208(%r10),%xmm2
    848 (0.0%)  	por	%xmm5,%xmm1
    848 (0.0%)  	pand	224(%r10),%xmm3
    848 (0.0%)  	por	%xmm2,%xmm0
    848 (0.0%)  	por	%xmm3,%xmm1
    848 (0.0%)  	movdqa	0(%r12),%xmm4
    848 (0.0%)  	movdqa	16(%r12),%xmm5
    848 (0.0%)  	movdqa	32(%r12),%xmm2
    848 (0.0%)  	pand	240(%r10),%xmm4
    848 (0.0%)  	movdqa	48(%r12),%xmm3
    848 (0.0%)  	pand	256(%r10),%xmm5
    848 (0.0%)  	por	%xmm4,%xmm0
    848 (0.0%)  	pand	272(%r10),%xmm2
    848 (0.0%)  	por	%xmm5,%xmm1
    848 (0.0%)  	pand	288(%r10),%xmm3
    848 (0.0%)  	por	%xmm2,%xmm0
    848 (0.0%)  	por	%xmm3,%xmm1
    848 (0.0%)  	por	%xmm1,%xmm0
      .         
    848 (0.0%)  	pshufd	$0x4e,%xmm0,%xmm1
    848 (0.0%)  	por	%xmm1,%xmm0
  1,696 (0.0%)  	leaq	256(%r12),%r12
      .         .byte	102,72,15,126,195
      .         
    848 (0.0%)  	movq	%r13,16+8(%rsp)
    848 (0.0%)  	movq	%rdi,56+8(%rsp)
      .         
    848 (0.0%)  	movq	(%r8),%r8
    848 (0.0%)  	movq	(%rsi),%rax
    848 (0.0%)  	leaq	(%rsi,%r9,1),%rsi
    848 (0.0%)  	negq	%r9
      .         
    848 (0.0%)  	movq	%r8,%rbp
    848 (0.0%)  	mulq	%rbx
    848 (0.0%)  	movq	%rax,%r10
    848 (0.0%)  	movq	(%rcx),%rax
      .         
    848 (0.0%)  	imulq	%r10,%rbp
    848 (0.0%)  	leaq	64+8(%rsp),%r14
    848 (0.0%)  	movq	%rdx,%r11
      .         
    848 (0.0%)  	mulq	%rbp
    848 (0.0%)  	addq	%rax,%r10
    848 (0.0%)  	movq	8(%rsi,%r9,1),%rax
    848 (0.0%)  	adcq	$0,%rdx
    848 (0.0%)  	movq	%rdx,%rdi
      .         
    848 (0.0%)  	mulq	%rbx
    848 (0.0%)  	addq	%rax,%r11
    848 (0.0%)  	movq	8(%rcx),%rax
    848 (0.0%)  	adcq	$0,%rdx
    848 (0.0%)  	movq	%rdx,%r10
      .         
    848 (0.0%)  	mulq	%rbp
    848 (0.0%)  	addq	%rax,%rdi
    848 (0.0%)  	movq	16(%rsi,%r9,1),%rax
    848 (0.0%)  	adcq	$0,%rdx
    848 (0.0%)  	addq	%r11,%rdi
    848 (0.0%)  	leaq	32(%r9),%r15
    848 (0.0%)  	leaq	32(%rcx),%rcx
    848 (0.0%)  	adcq	$0,%rdx
    848 (0.0%)  	movq	%rdi,(%r14)
    848 (0.0%)  	movq	%rdx,%r13
    848 (0.0%)  	jmp	.L1st4x
      .         
      .         .align	32
      .         .L1st4x:
  5,936 (0.0%)  	mulq	%rbx
  5,936 (0.0%)  	addq	%rax,%r10
  5,936 (0.0%)  	movq	-16(%rcx),%rax
  5,936 (0.0%)  	leaq	32(%r14),%r14
  5,936 (0.0%)  	adcq	$0,%rdx
  5,936 (0.0%)  	movq	%rdx,%r11
      .         
  5,936 (0.0%)  	mulq	%rbp
  5,936 (0.0%)  	addq	%rax,%r13
  5,936 (0.0%)  	movq	-8(%rsi,%r15,1),%rax
  5,936 (0.0%)  	adcq	$0,%rdx
  5,936 (0.0%)  	addq	%r10,%r13
  5,936 (0.0%)  	adcq	$0,%rdx
  5,936 (0.0%)  	movq	%r13,-24(%r14)
  5,936 (0.0%)  	movq	%rdx,%rdi
      .         
  5,936 (0.0%)  	mulq	%rbx
  5,936 (0.0%)  	addq	%rax,%r11
  5,936 (0.0%)  	movq	-8(%rcx),%rax
  5,936 (0.0%)  	adcq	$0,%rdx
  5,936 (0.0%)  	movq	%rdx,%r10
      .         
  5,936 (0.0%)  	mulq	%rbp
  5,936 (0.0%)  	addq	%rax,%rdi
  5,936 (0.0%)  	movq	(%rsi,%r15,1),%rax
  5,936 (0.0%)  	adcq	$0,%rdx
  5,936 (0.0%)  	addq	%r11,%rdi
  5,936 (0.0%)  	adcq	$0,%rdx
  5,936 (0.0%)  	movq	%rdi,-16(%r14)
  5,936 (0.0%)  	movq	%rdx,%r13
      .         
  5,936 (0.0%)  	mulq	%rbx
  5,936 (0.0%)  	addq	%rax,%r10
  5,936 (0.0%)  	movq	0(%rcx),%rax
  5,936 (0.0%)  	adcq	$0,%rdx
  5,936 (0.0%)  	movq	%rdx,%r11
      .         
  5,936 (0.0%)  	mulq	%rbp
  5,936 (0.0%)  	addq	%rax,%r13
  5,936 (0.0%)  	movq	8(%rsi,%r15,1),%rax
  5,936 (0.0%)  	adcq	$0,%rdx
  5,936 (0.0%)  	addq	%r10,%r13
  5,936 (0.0%)  	adcq	$0,%rdx
  5,936 (0.0%)  	movq	%r13,-8(%r14)
  5,936 (0.0%)  	movq	%rdx,%rdi
      .         
  5,936 (0.0%)  	mulq	%rbx
  5,936 (0.0%)  	addq	%rax,%r11
  5,936 (0.0%)  	movq	8(%rcx),%rax
  5,936 (0.0%)  	adcq	$0,%rdx
  5,936 (0.0%)  	movq	%rdx,%r10
      .         
  5,936 (0.0%)  	mulq	%rbp
  5,936 (0.0%)  	addq	%rax,%rdi
  5,936 (0.0%)  	movq	16(%rsi,%r15,1),%rax
  5,936 (0.0%)  	adcq	$0,%rdx
  5,936 (0.0%)  	addq	%r11,%rdi
  5,936 (0.0%)  	leaq	32(%rcx),%rcx
  5,936 (0.0%)  	adcq	$0,%rdx
  5,936 (0.0%)  	movq	%rdi,(%r14)
  5,936 (0.0%)  	movq	%rdx,%r13
      .         
  5,936 (0.0%)  	addq	$32,%r15
  5,936 (0.0%)  	jnz	.L1st4x
      .         
    848 (0.0%)  	mulq	%rbx
    848 (0.0%)  	addq	%rax,%r10
    848 (0.0%)  	movq	-16(%rcx),%rax
    848 (0.0%)  	leaq	32(%r14),%r14
    848 (0.0%)  	adcq	$0,%rdx
    848 (0.0%)  	movq	%rdx,%r11
      .         
    848 (0.0%)  	mulq	%rbp
    848 (0.0%)  	addq	%rax,%r13
    848 (0.0%)  	movq	-8(%rsi),%rax
    848 (0.0%)  	adcq	$0,%rdx
    848 (0.0%)  	addq	%r10,%r13
    848 (0.0%)  	adcq	$0,%rdx
    848 (0.0%)  	movq	%r13,-24(%r14)
    848 (0.0%)  	movq	%rdx,%rdi
      .         
    848 (0.0%)  	mulq	%rbx
    848 (0.0%)  	addq	%rax,%r11
    848 (0.0%)  	movq	-8(%rcx),%rax
    848 (0.0%)  	adcq	$0,%rdx
    848 (0.0%)  	movq	%rdx,%r10
      .         
    848 (0.0%)  	mulq	%rbp
    848 (0.0%)  	addq	%rax,%rdi
    848 (0.0%)  	movq	(%rsi,%r9,1),%rax
    848 (0.0%)  	adcq	$0,%rdx
    848 (0.0%)  	addq	%r11,%rdi
    848 (0.0%)  	adcq	$0,%rdx
    848 (0.0%)  	movq	%rdi,-16(%r14)
    848 (0.0%)  	movq	%rdx,%r13
      .         
    848 (0.0%)  	leaq	(%rcx,%r9,1),%rcx
      .         
    848 (0.0%)  	xorq	%rdi,%rdi
    848 (0.0%)  	addq	%r10,%r13
    848 (0.0%)  	adcq	$0,%rdi
    848 (0.0%)  	movq	%r13,-8(%r14)
      .         
    848 (0.0%)  	jmp	.Louter4x
      .         
      .         .align	32
      .         .Louter4x:
 26,288 (0.0%)  	leaq	16+128(%r14),%rdx
 26,288 (0.0%)  	pxor	%xmm4,%xmm4
 26,288 (0.0%)  	pxor	%xmm5,%xmm5
 26,288 (0.0%)  	movdqa	-128(%r12),%xmm0
 26,288 (0.0%)  	movdqa	-112(%r12),%xmm1
 26,288 (0.0%)  	movdqa	-96(%r12),%xmm2
 26,288 (0.0%)  	movdqa	-80(%r12),%xmm3
 26,288 (0.0%)  	pand	-128(%rdx),%xmm0
 26,288 (0.0%)  	pand	-112(%rdx),%xmm1
 26,288 (0.0%)  	por	%xmm0,%xmm4
 26,288 (0.0%)  	pand	-96(%rdx),%xmm2
 26,288 (0.0%)  	por	%xmm1,%xmm5
 26,288 (0.0%)  	pand	-80(%rdx),%xmm3
 26,288 (0.0%)  	por	%xmm2,%xmm4
 26,288 (0.0%)  	por	%xmm3,%xmm5
 26,288 (0.0%)  	movdqa	-64(%r12),%xmm0
 26,288 (0.0%)  	movdqa	-48(%r12),%xmm1
 26,288 (0.0%)  	movdqa	-32(%r12),%xmm2
 26,288 (0.0%)  	movdqa	-16(%r12),%xmm3
 26,288 (0.0%)  	pand	-64(%rdx),%xmm0
 26,288 (0.0%)  	pand	-48(%rdx),%xmm1
 26,288 (0.0%)  	por	%xmm0,%xmm4
 26,288 (0.0%)  	pand	-32(%rdx),%xmm2
 26,288 (0.0%)  	por	%xmm1,%xmm5
 26,288 (0.0%)  	pand	-16(%rdx),%xmm3
 26,288 (0.0%)  	por	%xmm2,%xmm4
 26,288 (0.0%)  	por	%xmm3,%xmm5
 26,288 (0.0%)  	movdqa	0(%r12),%xmm0
 26,288 (0.0%)  	movdqa	16(%r12),%xmm1
 26,288 (0.0%)  	movdqa	32(%r12),%xmm2
 26,288 (0.0%)  	movdqa	48(%r12),%xmm3
 26,288 (0.0%)  	pand	0(%rdx),%xmm0
 26,288 (0.0%)  	pand	16(%rdx),%xmm1
 26,288 (0.0%)  	por	%xmm0,%xmm4
 26,288 (0.0%)  	pand	32(%rdx),%xmm2
 26,288 (0.0%)  	por	%xmm1,%xmm5
 26,288 (0.0%)  	pand	48(%rdx),%xmm3
 26,288 (0.0%)  	por	%xmm2,%xmm4
 26,288 (0.0%)  	por	%xmm3,%xmm5
 26,288 (0.0%)  	movdqa	64(%r12),%xmm0
 26,288 (0.0%)  	movdqa	80(%r12),%xmm1
 26,288 (0.0%)  	movdqa	96(%r12),%xmm2
 26,288 (0.0%)  	movdqa	112(%r12),%xmm3
 26,288 (0.0%)  	pand	64(%rdx),%xmm0
 26,288 (0.0%)  	pand	80(%rdx),%xmm1
 26,288 (0.0%)  	por	%xmm0,%xmm4
 26,288 (0.0%)  	pand	96(%rdx),%xmm2
 26,288 (0.0%)  	por	%xmm1,%xmm5
 26,288 (0.0%)  	pand	112(%rdx),%xmm3
 26,288 (0.0%)  	por	%xmm2,%xmm4
 26,288 (0.0%)  	por	%xmm3,%xmm5
 26,288 (0.0%)  	por	%xmm5,%xmm4
      .         
 26,288 (0.0%)  	pshufd	$0x4e,%xmm4,%xmm0
 26,288 (0.0%)  	por	%xmm4,%xmm0
 52,576 (0.1%)  	leaq	256(%r12),%r12
      .         .byte	102,72,15,126,195
      .         
 26,288 (0.0%)  	movq	(%r14,%r9,1),%r10
 26,288 (0.0%)  	movq	%r8,%rbp
 26,288 (0.0%)  	mulq	%rbx
 26,288 (0.0%)  	addq	%rax,%r10
 26,288 (0.0%)  	movq	(%rcx),%rax
 26,288 (0.0%)  	adcq	$0,%rdx
      .         
 26,288 (0.0%)  	imulq	%r10,%rbp
 26,288 (0.0%)  	movq	%rdx,%r11
 26,288 (0.0%)  	movq	%rdi,(%r14)
      .         
 26,288 (0.0%)  	leaq	(%r14,%r9,1),%r14
      .         
 26,288 (0.0%)  	mulq	%rbp
 26,288 (0.0%)  	addq	%rax,%r10
 26,288 (0.0%)  	movq	8(%rsi,%r9,1),%rax
 26,288 (0.0%)  	adcq	$0,%rdx
 26,288 (0.0%)  	movq	%rdx,%rdi
      .         
 26,288 (0.0%)  	mulq	%rbx
 26,288 (0.0%)  	addq	%rax,%r11
 26,288 (0.0%)  	movq	8(%rcx),%rax
 26,288 (0.0%)  	adcq	$0,%rdx
 26,288 (0.0%)  	addq	8(%r14),%r11
 26,288 (0.0%)  	adcq	$0,%rdx
 26,288 (0.0%)  	movq	%rdx,%r10
      .         
 26,288 (0.0%)  	mulq	%rbp
 26,288 (0.0%)  	addq	%rax,%rdi
 26,288 (0.0%)  	movq	16(%rsi,%r9,1),%rax
 26,288 (0.0%)  	adcq	$0,%rdx
 26,288 (0.0%)  	addq	%r11,%rdi
 26,288 (0.0%)  	leaq	32(%r9),%r15
 26,288 (0.0%)  	leaq	32(%rcx),%rcx
 26,288 (0.0%)  	adcq	$0,%rdx
 26,288 (0.0%)  	movq	%rdx,%r13
 26,288 (0.0%)  	jmp	.Linner4x
      .         
      .         .align	32
      .         .Linner4x:
184,016 (0.2%)  	mulq	%rbx
184,016 (0.2%)  	addq	%rax,%r10
184,016 (0.2%)  	movq	-16(%rcx),%rax
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	addq	16(%r14),%r10
184,016 (0.2%)  	leaq	32(%r14),%r14
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	movq	%rdx,%r11
      .         
184,016 (0.2%)  	mulq	%rbp
184,016 (0.2%)  	addq	%rax,%r13
184,016 (0.2%)  	movq	-8(%rsi,%r15,1),%rax
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	addq	%r10,%r13
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	movq	%rdi,-32(%r14)
184,016 (0.2%)  	movq	%rdx,%rdi
      .         
184,016 (0.2%)  	mulq	%rbx
184,016 (0.2%)  	addq	%rax,%r11
184,016 (0.2%)  	movq	-8(%rcx),%rax
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	addq	-8(%r14),%r11
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	movq	%rdx,%r10
      .         
184,016 (0.2%)  	mulq	%rbp
184,016 (0.2%)  	addq	%rax,%rdi
184,016 (0.2%)  	movq	(%rsi,%r15,1),%rax
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	addq	%r11,%rdi
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	movq	%r13,-24(%r14)
184,016 (0.2%)  	movq	%rdx,%r13
      .         
184,016 (0.2%)  	mulq	%rbx
184,016 (0.2%)  	addq	%rax,%r10
184,016 (0.2%)  	movq	0(%rcx),%rax
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	addq	(%r14),%r10
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	movq	%rdx,%r11
      .         
184,016 (0.2%)  	mulq	%rbp
184,016 (0.2%)  	addq	%rax,%r13
184,016 (0.2%)  	movq	8(%rsi,%r15,1),%rax
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	addq	%r10,%r13
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	movq	%rdi,-16(%r14)
184,016 (0.2%)  	movq	%rdx,%rdi
      .         
184,016 (0.2%)  	mulq	%rbx
184,016 (0.2%)  	addq	%rax,%r11
184,016 (0.2%)  	movq	8(%rcx),%rax
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	addq	8(%r14),%r11
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	movq	%rdx,%r10
      .         
184,016 (0.2%)  	mulq	%rbp
184,016 (0.2%)  	addq	%rax,%rdi
184,016 (0.2%)  	movq	16(%rsi,%r15,1),%rax
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	addq	%r11,%rdi
184,016 (0.2%)  	leaq	32(%rcx),%rcx
184,016 (0.2%)  	adcq	$0,%rdx
184,016 (0.2%)  	movq	%r13,-8(%r14)
184,016 (0.2%)  	movq	%rdx,%r13
      .         
184,016 (0.2%)  	addq	$32,%r15
184,016 (0.2%)  	jnz	.Linner4x
      .         
 26,288 (0.0%)  	mulq	%rbx
 26,288 (0.0%)  	addq	%rax,%r10
 26,288 (0.0%)  	movq	-16(%rcx),%rax
 26,288 (0.0%)  	adcq	$0,%rdx
 26,288 (0.0%)  	addq	16(%r14),%r10
 26,288 (0.0%)  	leaq	32(%r14),%r14
 26,288 (0.0%)  	adcq	$0,%rdx
 26,288 (0.0%)  	movq	%rdx,%r11
      .         
 26,288 (0.0%)  	mulq	%rbp
 26,288 (0.0%)  	addq	%rax,%r13
 26,288 (0.0%)  	movq	-8(%rsi),%rax
 26,288 (0.0%)  	adcq	$0,%rdx
 26,288 (0.0%)  	addq	%r10,%r13
 26,288 (0.0%)  	adcq	$0,%rdx
 26,288 (0.0%)  	movq	%rdi,-32(%r14)
 26,288 (0.0%)  	movq	%rdx,%rdi
      .         
 26,288 (0.0%)  	mulq	%rbx
 26,288 (0.0%)  	addq	%rax,%r11
 26,288 (0.0%)  	movq	%rbp,%rax
 26,288 (0.0%)  	movq	-8(%rcx),%rbp
 26,288 (0.0%)  	adcq	$0,%rdx
 26,288 (0.0%)  	addq	-8(%r14),%r11
 26,288 (0.0%)  	adcq	$0,%rdx
 26,288 (0.0%)  	movq	%rdx,%r10
      .         
 26,288 (0.0%)  	mulq	%rbp
 26,288 (0.0%)  	addq	%rax,%rdi
 26,288 (0.0%)  	movq	(%rsi,%r9,1),%rax
 26,288 (0.0%)  	adcq	$0,%rdx
 26,288 (0.0%)  	addq	%r11,%rdi
 26,288 (0.0%)  	adcq	$0,%rdx
 26,288 (0.0%)  	movq	%r13,-24(%r14)
 26,288 (0.0%)  	movq	%rdx,%r13
      .         
 26,288 (0.0%)  	movq	%rdi,-16(%r14)
 26,288 (0.0%)  	leaq	(%rcx,%r9,1),%rcx
      .         
 26,288 (0.0%)  	xorq	%rdi,%rdi
 26,288 (0.0%)  	addq	%r10,%r13
 26,288 (0.0%)  	adcq	$0,%rdi
 26,288 (0.0%)  	addq	(%r14),%r13
 26,288 (0.0%)  	adcq	$0,%rdi
 26,288 (0.0%)  	movq	%r13,-8(%r14)
      .         
 26,288 (0.0%)  	cmpq	16+8(%rsp),%r12
 26,288 (0.0%)  	jb	.Louter4x
    848 (0.0%)  	xorq	%rax,%rax
    848 (0.0%)  	subq	%r13,%rbp
    848 (0.0%)  	adcq	%r15,%r15
    848 (0.0%)  	orq	%r15,%rdi
    848 (0.0%)  	subq	%rdi,%rax
    848 (0.0%)  	leaq	(%r14,%r9,1),%rbx
    848 (0.0%)  	movq	(%rcx),%r12
    848 (0.0%)  	leaq	(%rcx),%rbp
    848 (0.0%)  	movq	%r9,%rcx
    848 (0.0%)  	sarq	$3+2,%rcx
    848 (0.0%)  	movq	56+8(%rsp),%rdi
    848 (0.0%)  	decq	%r12
    848 (0.0%)  	xorq	%r10,%r10
    848 (0.0%)  	movq	8(%rbp),%r13
    848 (0.0%)  	movq	16(%rbp),%r14
    848 (0.0%)  	movq	24(%rbp),%r15
    848 (0.0%)  	jmp	.Lsqr4x_sub_entry
      .         .cfi_endproc	
      .         .size	mul4x_internal,.-mul4x_internal
      .         .globl	bn_power5
      .         .hidden bn_power5
      .         .type	bn_power5,@function
      .         .align	32
      .         bn_power5:
      .         .cfi_startproc	
    818 (0.0%)  	movq	%rsp,%rax
      .         .cfi_def_cfa_register	%rax
    818 (0.0%)  	leaq	OPENSSL_ia32cap_P(%rip),%r11
    818 (0.0%)  	movl	8(%r11),%r11d
    818 (0.0%)  	andl	$0x80108,%r11d
    818 (0.0%)  	cmpl	$0x80108,%r11d
    818 (0.0%)  	je	.Lpowerx5_enter
    818 (0.0%)  	pushq	%rbx
      .         .cfi_offset	%rbx,-16
    818 (0.0%)  	pushq	%rbp
      .         .cfi_offset	%rbp,-24
    818 (0.0%)  	pushq	%r12
      .         .cfi_offset	%r12,-32
    818 (0.0%)  	pushq	%r13
      .         .cfi_offset	%r13,-40
    818 (0.0%)  	pushq	%r14
      .         .cfi_offset	%r14,-48
    818 (0.0%)  	pushq	%r15
      .         .cfi_offset	%r15,-56
      .         .Lpower5_prologue:
      .         
    818 (0.0%)  	shll	$3,%r9d
    818 (0.0%)  	leal	(%r9,%r9,2),%r10d
    818 (0.0%)  	negq	%r9
    818 (0.0%)  	movq	(%r8),%r8
      .         
      .         
      .         
      .         
      .         
      .         
      .         
      .         
    818 (0.0%)  	leaq	-320(%rsp,%r9,2),%r11
    818 (0.0%)  	movq	%rsp,%rbp
    818 (0.0%)  	subq	%rdi,%r11
    818 (0.0%)  	andq	$4095,%r11
    818 (0.0%)  	cmpq	%r11,%r10
    818 (0.0%)  	jb	.Lpwr_sp_alt
      .         	subq	%r11,%rbp
      .         	leaq	-320(%rbp,%r9,2),%rbp
      .         	jmp	.Lpwr_sp_done
      .         
      .         .align	32
      .         .Lpwr_sp_alt:
    818 (0.0%)  	leaq	4096-320(,%r9,2),%r10
    818 (0.0%)  	leaq	-320(%rbp,%r9,2),%rbp
    818 (0.0%)  	subq	%r10,%r11
    818 (0.0%)  	movq	$0,%r10
    818 (0.0%)  	cmovcq	%r10,%r11
    818 (0.0%)  	subq	%r11,%rbp
      .         .Lpwr_sp_done:
    818 (0.0%)  	andq	$-64,%rbp
    818 (0.0%)  	movq	%rsp,%r11
    818 (0.0%)  	subq	%rbp,%r11
    818 (0.0%)  	andq	$-4096,%r11
    818 (0.0%)  	leaq	(%r11,%rbp,1),%rsp
    818 (0.0%)  	movq	(%rsp),%r10
    818 (0.0%)  	cmpq	%rbp,%rsp
    818 (0.0%)  	ja	.Lpwr_page_walk
    818 (0.0%)  	jmp	.Lpwr_page_walk_done
      .         
      .         .Lpwr_page_walk:
      .         	leaq	-4096(%rsp),%rsp
      .         	movq	(%rsp),%r10
      .         	cmpq	%rbp,%rsp
      .         	ja	.Lpwr_page_walk
      .         .Lpwr_page_walk_done:
      .         
    818 (0.0%)  	movq	%r9,%r10
    818 (0.0%)  	negq	%r9
      .         
      .         
      .         
      .         
      .         
      .         
      .         
      .         
      .         
      .         
    818 (0.0%)  	movq	%r8,32(%rsp)
  4,090 (0.0%)  	movq	%rax,40(%rsp)
      .         .cfi_escape	0x0f,0x05,0x77,0x28,0x06,0x23,0x08
      .         .Lpower5_body:
      .         .byte	102,72,15,110,207
      .         .byte	102,72,15,110,209
      .         .byte	102,73,15,110,218
      .         .byte	102,72,15,110,226
      .         
    818 (0.0%)  	call	__bn_sqr8x_internal
    818 (0.0%)  	call	__bn_post4x_internal
    818 (0.0%)  	call	__bn_sqr8x_internal
    818 (0.0%)  	call	__bn_post4x_internal
    818 (0.0%)  	call	__bn_sqr8x_internal
    818 (0.0%)  	call	__bn_post4x_internal
    818 (0.0%)  	call	__bn_sqr8x_internal
    818 (0.0%)  	call	__bn_post4x_internal
    818 (0.0%)  	call	__bn_sqr8x_internal
  2,454 (0.0%)  	call	__bn_post4x_internal
      .         
      .         .byte	102,72,15,126,209
      .         .byte	102,72,15,126,226
    818 (0.0%)  	movq	%rsi,%rdi
    818 (0.0%)  	movq	40(%rsp),%rax
    818 (0.0%)  	leaq	32(%rsp),%r8
      .         
    818 (0.0%)  	call	mul4x_internal
      .         
    818 (0.0%)  	movq	40(%rsp),%rsi
      .         .cfi_def_cfa	%rsi,8
    818 (0.0%)  	movq	$1,%rax
    818 (0.0%)  	movq	-48(%rsi),%r15
      .         .cfi_restore	%r15
    818 (0.0%)  	movq	-40(%rsi),%r14
      .         .cfi_restore	%r14
    818 (0.0%)  	movq	-32(%rsi),%r13
      .         .cfi_restore	%r13
    818 (0.0%)  	movq	-24(%rsi),%r12
      .         .cfi_restore	%r12
    818 (0.0%)  	movq	-16(%rsi),%rbp
      .         .cfi_restore	%rbp
    818 (0.0%)  	movq	-8(%rsi),%rbx
      .         .cfi_restore	%rbx
  1,636 (0.0%)  	leaq	(%rsi),%rsp
      .         .cfi_def_cfa_register	%rsp
      .         .Lpower5_epilogue:
      .         	.byte	0xf3,0xc3
      .         .cfi_endproc	
      .         .size	bn_power5,.-bn_power5
      .         
      .         .globl	bn_sqr8x_internal
      .         .hidden bn_sqr8x_internal
-- line 1228 ----------------------------------------
-- line 1300 ----------------------------------------
      .         
      .         
      .         
      .         
      .         
      .         
      .         
      .         
  4,154 (0.0%)  	leaq	32(%r10),%rbp
  4,154 (0.0%)  	leaq	(%rsi,%r9,1),%rsi
      .         
  4,154 (0.0%)  	movq	%r9,%rcx
      .         
      .         
  4,154 (0.0%)  	movq	-32(%rsi,%rbp,1),%r14
  4,154 (0.0%)  	leaq	48+8(%rsp,%r9,2),%rdi
  4,154 (0.0%)  	movq	-24(%rsi,%rbp,1),%rax
  4,154 (0.0%)  	leaq	-32(%rdi,%rbp,1),%rdi
  4,154 (0.0%)  	movq	-16(%rsi,%rbp,1),%rbx
  4,154 (0.0%)  	movq	%rax,%r15
      .         
  4,154 (0.0%)  	mulq	%r14
  4,154 (0.0%)  	movq	%rax,%r10
  4,154 (0.0%)  	movq	%rbx,%rax
  4,154 (0.0%)  	movq	%rdx,%r11
  4,154 (0.0%)  	movq	%r10,-24(%rdi,%rbp,1)
      .         
  4,154 (0.0%)  	mulq	%r14
  4,154 (0.0%)  	addq	%rax,%r11
  4,154 (0.0%)  	movq	%rbx,%rax
  4,154 (0.0%)  	adcq	$0,%rdx
  4,154 (0.0%)  	movq	%r11,-16(%rdi,%rbp,1)
  4,154 (0.0%)  	movq	%rdx,%r10
      .         
      .         
  4,154 (0.0%)  	movq	-8(%rsi,%rbp,1),%rbx
  4,154 (0.0%)  	mulq	%r15
  4,154 (0.0%)  	movq	%rax,%r12
  4,154 (0.0%)  	movq	%rbx,%rax
  4,154 (0.0%)  	movq	%rdx,%r13
      .         
  4,154 (0.0%)  	leaq	(%rbp),%rcx
  4,154 (0.0%)  	mulq	%r14
  4,154 (0.0%)  	addq	%rax,%r10
  4,154 (0.0%)  	movq	%rbx,%rax
  4,154 (0.0%)  	movq	%rdx,%r11
  4,154 (0.0%)  	adcq	$0,%r11
  4,154 (0.0%)  	addq	%r12,%r10
  4,154 (0.0%)  	adcq	$0,%r11
  4,154 (0.0%)  	movq	%r10,-8(%rdi,%rcx,1)
  4,154 (0.0%)  	jmp	.Lsqr4x_1st
      .         
      .         .align	32
      .         .Lsqr4x_1st:
 29,350 (0.0%)  	movq	(%rsi,%rcx,1),%rbx
 29,350 (0.0%)  	mulq	%r15
 29,350 (0.0%)  	addq	%rax,%r13
 29,350 (0.0%)  	movq	%rbx,%rax
 29,350 (0.0%)  	movq	%rdx,%r12
 29,350 (0.0%)  	adcq	$0,%r12
      .         
 29,350 (0.0%)  	mulq	%r14
 29,350 (0.0%)  	addq	%rax,%r11
 29,350 (0.0%)  	movq	%rbx,%rax
 29,350 (0.0%)  	movq	8(%rsi,%rcx,1),%rbx
 29,350 (0.0%)  	movq	%rdx,%r10
 29,350 (0.0%)  	adcq	$0,%r10
 29,350 (0.0%)  	addq	%r13,%r11
 29,350 (0.0%)  	adcq	$0,%r10
      .         
      .         
 29,350 (0.0%)  	mulq	%r15
 29,350 (0.0%)  	addq	%rax,%r12
 29,350 (0.0%)  	movq	%rbx,%rax
 29,350 (0.0%)  	movq	%r11,(%rdi,%rcx,1)
 29,350 (0.0%)  	movq	%rdx,%r13
 29,350 (0.0%)  	adcq	$0,%r13
      .         
 29,350 (0.0%)  	mulq	%r14
 29,350 (0.0%)  	addq	%rax,%r10
 29,350 (0.0%)  	movq	%rbx,%rax
 29,350 (0.0%)  	movq	16(%rsi,%rcx,1),%rbx
 29,350 (0.0%)  	movq	%rdx,%r11
 29,350 (0.0%)  	adcq	$0,%r11
 29,350 (0.0%)  	addq	%r12,%r10
 29,350 (0.0%)  	adcq	$0,%r11
      .         
 29,350 (0.0%)  	mulq	%r15
 29,350 (0.0%)  	addq	%rax,%r13
 29,350 (0.0%)  	movq	%rbx,%rax
 29,350 (0.0%)  	movq	%r10,8(%rdi,%rcx,1)
 29,350 (0.0%)  	movq	%rdx,%r12
 29,350 (0.0%)  	adcq	$0,%r12
      .         
 29,350 (0.0%)  	mulq	%r14
 29,350 (0.0%)  	addq	%rax,%r11
 29,350 (0.0%)  	movq	%rbx,%rax
 29,350 (0.0%)  	movq	24(%rsi,%rcx,1),%rbx
 29,350 (0.0%)  	movq	%rdx,%r10
 29,350 (0.0%)  	adcq	$0,%r10
 29,350 (0.0%)  	addq	%r13,%r11
 29,350 (0.0%)  	adcq	$0,%r10
      .         
      .         
 29,350 (0.0%)  	mulq	%r15
 29,350 (0.0%)  	addq	%rax,%r12
 29,350 (0.0%)  	movq	%rbx,%rax
 29,350 (0.0%)  	movq	%r11,16(%rdi,%rcx,1)
 29,350 (0.0%)  	movq	%rdx,%r13
 29,350 (0.0%)  	adcq	$0,%r13
 29,350 (0.0%)  	leaq	32(%rcx),%rcx
      .         
 29,350 (0.0%)  	mulq	%r14
 29,350 (0.0%)  	addq	%rax,%r10
 29,350 (0.0%)  	movq	%rbx,%rax
 29,350 (0.0%)  	movq	%rdx,%r11
 29,350 (0.0%)  	adcq	$0,%r11
 29,350 (0.0%)  	addq	%r12,%r10
 29,350 (0.0%)  	adcq	$0,%r11
 29,350 (0.0%)  	movq	%r10,-8(%rdi,%rcx,1)
      .         
 29,350 (0.0%)  	cmpq	$0,%rcx
 29,350 (0.0%)  	jne	.Lsqr4x_1st
      .         
  4,154 (0.0%)  	mulq	%r15
  4,154 (0.0%)  	addq	%rax,%r13
  4,154 (0.0%)  	leaq	16(%rbp),%rbp
  4,154 (0.0%)  	adcq	$0,%rdx
  4,154 (0.0%)  	addq	%r11,%r13
  4,154 (0.0%)  	adcq	$0,%rdx
      .         
  4,154 (0.0%)  	movq	%r13,(%rdi)
  4,154 (0.0%)  	movq	%rdx,%r12
  4,154 (0.0%)  	movq	%rdx,8(%rdi)
  4,154 (0.0%)  	jmp	.Lsqr4x_outer
      .         
      .         .align	32
      .         .Lsqr4x_outer:
 54,546 (0.1%)  	movq	-32(%rsi,%rbp,1),%r14
 54,546 (0.1%)  	leaq	48+8(%rsp,%r9,2),%rdi
 54,546 (0.1%)  	movq	-24(%rsi,%rbp,1),%rax
 54,546 (0.1%)  	leaq	-32(%rdi,%rbp,1),%rdi
 54,546 (0.1%)  	movq	-16(%rsi,%rbp,1),%rbx
 54,546 (0.1%)  	movq	%rax,%r15
      .         
 54,546 (0.1%)  	mulq	%r14
 54,546 (0.1%)  	movq	-24(%rdi,%rbp,1),%r10
 54,546 (0.1%)  	addq	%rax,%r10
 54,546 (0.1%)  	movq	%rbx,%rax
 54,546 (0.1%)  	adcq	$0,%rdx
 54,546 (0.1%)  	movq	%r10,-24(%rdi,%rbp,1)
 54,546 (0.1%)  	movq	%rdx,%r11
      .         
 54,546 (0.1%)  	mulq	%r14
 54,546 (0.1%)  	addq	%rax,%r11
 54,546 (0.1%)  	movq	%rbx,%rax
 54,546 (0.1%)  	adcq	$0,%rdx
 54,546 (0.1%)  	addq	-16(%rdi,%rbp,1),%r11
 54,546 (0.1%)  	movq	%rdx,%r10
 54,546 (0.1%)  	adcq	$0,%r10
 54,546 (0.1%)  	movq	%r11,-16(%rdi,%rbp,1)
      .         
 54,546 (0.1%)  	xorq	%r12,%r12
      .         
 54,546 (0.1%)  	movq	-8(%rsi,%rbp,1),%rbx
 54,546 (0.1%)  	mulq	%r15
 54,546 (0.1%)  	addq	%rax,%r12
 54,546 (0.1%)  	movq	%rbx,%rax
 54,546 (0.1%)  	adcq	$0,%rdx
 54,546 (0.1%)  	addq	-8(%rdi,%rbp,1),%r12
 54,546 (0.1%)  	movq	%rdx,%r13
 54,546 (0.1%)  	adcq	$0,%r13
      .         
 54,546 (0.1%)  	mulq	%r14
 54,546 (0.1%)  	addq	%rax,%r10
 54,546 (0.1%)  	movq	%rbx,%rax
 54,546 (0.1%)  	adcq	$0,%rdx
 54,546 (0.1%)  	addq	%r12,%r10
 54,546 (0.1%)  	movq	%rdx,%r11
 54,546 (0.1%)  	adcq	$0,%r11
 54,546 (0.1%)  	movq	%r10,-8(%rdi,%rbp,1)
      .         
 54,546 (0.1%)  	leaq	(%rbp),%rcx
 54,546 (0.1%)  	jmp	.Lsqr4x_inner
      .         
      .         .align	32
      .         .Lsqr4x_inner:
389,710 (0.5%)  	movq	(%rsi,%rcx,1),%rbx
389,710 (0.5%)  	mulq	%r15
389,710 (0.5%)  	addq	%rax,%r13
389,710 (0.5%)  	movq	%rbx,%rax
389,710 (0.5%)  	movq	%rdx,%r12
389,710 (0.5%)  	adcq	$0,%r12
389,710 (0.5%)  	addq	(%rdi,%rcx,1),%r13
779,420 (1.0%)  	adcq	$0,%r12
      .         
      .         .byte	0x67
      .         	mulq	%r14
389,710 (0.5%)  	addq	%rax,%r11
389,710 (0.5%)  	movq	%rbx,%rax
389,710 (0.5%)  	movq	8(%rsi,%rcx,1),%rbx
389,710 (0.5%)  	movq	%rdx,%r10
389,710 (0.5%)  	adcq	$0,%r10
389,710 (0.5%)  	addq	%r13,%r11
389,710 (0.5%)  	adcq	$0,%r10
      .         
389,710 (0.5%)  	mulq	%r15
389,710 (0.5%)  	addq	%rax,%r12
389,710 (0.5%)  	movq	%r11,(%rdi,%rcx,1)
389,710 (0.5%)  	movq	%rbx,%rax
389,710 (0.5%)  	movq	%rdx,%r13
389,710 (0.5%)  	adcq	$0,%r13
389,710 (0.5%)  	addq	8(%rdi,%rcx,1),%r12
389,710 (0.5%)  	leaq	16(%rcx),%rcx
389,710 (0.5%)  	adcq	$0,%r13
      .         
389,710 (0.5%)  	mulq	%r14
389,710 (0.5%)  	addq	%rax,%r10
389,710 (0.5%)  	movq	%rbx,%rax
389,710 (0.5%)  	adcq	$0,%rdx
389,710 (0.5%)  	addq	%r12,%r10
389,710 (0.5%)  	movq	%rdx,%r11
389,710 (0.5%)  	adcq	$0,%r11
389,710 (0.5%)  	movq	%r10,-8(%rdi,%rcx,1)
      .         
389,710 (0.5%)  	cmpq	$0,%rcx
444,256 (0.6%)  	jne	.Lsqr4x_inner
      .         
      .         .byte	0x67
      .         	mulq	%r15
 54,546 (0.1%)  	addq	%rax,%r13
 54,546 (0.1%)  	adcq	$0,%rdx
 54,546 (0.1%)  	addq	%r11,%r13
 54,546 (0.1%)  	adcq	$0,%rdx
      .         
 54,546 (0.1%)  	movq	%r13,(%rdi)
 54,546 (0.1%)  	movq	%rdx,%r12
 54,546 (0.1%)  	movq	%rdx,8(%rdi)
      .         
 54,546 (0.1%)  	addq	$16,%rbp
 54,546 (0.1%)  	jnz	.Lsqr4x_outer
      .         
      .         
  4,154 (0.0%)  	movq	-32(%rsi),%r14
  4,154 (0.0%)  	leaq	48+8(%rsp,%r9,2),%rdi
  4,154 (0.0%)  	movq	-24(%rsi),%rax
  4,154 (0.0%)  	leaq	-32(%rdi,%rbp,1),%rdi
  4,154 (0.0%)  	movq	-16(%rsi),%rbx
  4,154 (0.0%)  	movq	%rax,%r15
      .         
  4,154 (0.0%)  	mulq	%r14
  4,154 (0.0%)  	addq	%rax,%r10
  4,154 (0.0%)  	movq	%rbx,%rax
  4,154 (0.0%)  	movq	%rdx,%r11
  4,154 (0.0%)  	adcq	$0,%r11
      .         
  4,154 (0.0%)  	mulq	%r14
  4,154 (0.0%)  	addq	%rax,%r11
  4,154 (0.0%)  	movq	%rbx,%rax
  4,154 (0.0%)  	movq	%r10,-24(%rdi)
  4,154 (0.0%)  	movq	%rdx,%r10
  4,154 (0.0%)  	adcq	$0,%r10
  4,154 (0.0%)  	addq	%r13,%r11
  4,154 (0.0%)  	movq	-8(%rsi),%rbx
  4,154 (0.0%)  	adcq	$0,%r10
      .         
  4,154 (0.0%)  	mulq	%r15
  4,154 (0.0%)  	addq	%rax,%r12
  4,154 (0.0%)  	movq	%rbx,%rax
  4,154 (0.0%)  	movq	%r11,-16(%rdi)
  4,154 (0.0%)  	movq	%rdx,%r13
  4,154 (0.0%)  	adcq	$0,%r13
      .         
  4,154 (0.0%)  	mulq	%r14
  4,154 (0.0%)  	addq	%rax,%r10
  4,154 (0.0%)  	movq	%rbx,%rax
  4,154 (0.0%)  	movq	%rdx,%r11
  4,154 (0.0%)  	adcq	$0,%r11
  4,154 (0.0%)  	addq	%r12,%r10
  4,154 (0.0%)  	adcq	$0,%r11
  4,154 (0.0%)  	movq	%r10,-8(%rdi)
      .         
  4,154 (0.0%)  	mulq	%r15
  4,154 (0.0%)  	addq	%rax,%r13
  4,154 (0.0%)  	movq	-16(%rsi),%rax
  4,154 (0.0%)  	adcq	$0,%rdx
  4,154 (0.0%)  	addq	%r11,%r13
  4,154 (0.0%)  	adcq	$0,%rdx
      .         
  4,154 (0.0%)  	movq	%r13,(%rdi)
  4,154 (0.0%)  	movq	%rdx,%r12
  4,154 (0.0%)  	movq	%rdx,8(%rdi)
      .         
  4,154 (0.0%)  	mulq	%rbx
  4,154 (0.0%)  	addq	$16,%rbp
  4,154 (0.0%)  	xorq	%r14,%r14
  4,154 (0.0%)  	subq	%r9,%rbp
  4,154 (0.0%)  	xorq	%r15,%r15
      .         
  4,154 (0.0%)  	addq	%r12,%rax
  4,154 (0.0%)  	adcq	$0,%rdx
  4,154 (0.0%)  	movq	%rax,8(%rdi)
  4,154 (0.0%)  	movq	%rdx,16(%rdi)
  4,154 (0.0%)  	movq	%r15,24(%rdi)
      .         
  4,154 (0.0%)  	movq	-16(%rsi,%rbp,1),%rax
  4,154 (0.0%)  	leaq	48+8(%rsp),%rdi
  4,154 (0.0%)  	xorq	%r10,%r10
  4,154 (0.0%)  	movq	8(%rdi),%r11
      .         
  4,154 (0.0%)  	leaq	(%r14,%r10,2),%r12
  4,154 (0.0%)  	shrq	$63,%r10
  4,154 (0.0%)  	leaq	(%rcx,%r11,2),%r13
  4,154 (0.0%)  	shrq	$63,%r11
  4,154 (0.0%)  	orq	%r10,%r13
  4,154 (0.0%)  	movq	16(%rdi),%r10
  4,154 (0.0%)  	movq	%r11,%r14
  4,154 (0.0%)  	mulq	%rax
  4,154 (0.0%)  	negq	%r15
  4,154 (0.0%)  	movq	24(%rdi),%r11
  4,154 (0.0%)  	adcq	%rax,%r12
  4,154 (0.0%)  	movq	-8(%rsi,%rbp,1),%rax
  4,154 (0.0%)  	movq	%r12,(%rdi)
  4,154 (0.0%)  	adcq	%rdx,%r13
      .         
  4,154 (0.0%)  	leaq	(%r14,%r10,2),%rbx
  4,154 (0.0%)  	movq	%r13,8(%rdi)
  4,154 (0.0%)  	sbbq	%r15,%r15
  4,154 (0.0%)  	shrq	$63,%r10
  4,154 (0.0%)  	leaq	(%rcx,%r11,2),%r8
  4,154 (0.0%)  	shrq	$63,%r11
  4,154 (0.0%)  	orq	%r10,%r8
  4,154 (0.0%)  	movq	32(%rdi),%r10
  4,154 (0.0%)  	movq	%r11,%r14
  4,154 (0.0%)  	mulq	%rax
  4,154 (0.0%)  	negq	%r15
  4,154 (0.0%)  	movq	40(%rdi),%r11
  4,154 (0.0%)  	adcq	%rax,%rbx
  4,154 (0.0%)  	movq	0(%rsi,%rbp,1),%rax
  4,154 (0.0%)  	movq	%rbx,16(%rdi)
  4,154 (0.0%)  	adcq	%rdx,%r8
  4,154 (0.0%)  	leaq	16(%rbp),%rbp
  4,154 (0.0%)  	movq	%r8,24(%rdi)
  4,154 (0.0%)  	sbbq	%r15,%r15
  4,154 (0.0%)  	leaq	64(%rdi),%rdi
  4,154 (0.0%)  	jmp	.Lsqr4x_shift_n_add
      .         
      .         .align	32
      .         .Lsqr4x_shift_n_add:
 29,350 (0.0%)  	leaq	(%r14,%r10,2),%r12
 29,350 (0.0%)  	shrq	$63,%r10
 29,350 (0.0%)  	leaq	(%rcx,%r11,2),%r13
 29,350 (0.0%)  	shrq	$63,%r11
 29,350 (0.0%)  	orq	%r10,%r13
 29,350 (0.0%)  	movq	-16(%rdi),%r10
 29,350 (0.0%)  	movq	%r11,%r14
 29,350 (0.0%)  	mulq	%rax
 29,350 (0.0%)  	negq	%r15
 29,350 (0.0%)  	movq	-8(%rdi),%r11
 29,350 (0.0%)  	adcq	%rax,%r12
 29,350 (0.0%)  	movq	-8(%rsi,%rbp,1),%rax
 29,350 (0.0%)  	movq	%r12,-32(%rdi)
 29,350 (0.0%)  	adcq	%rdx,%r13
      .         
 29,350 (0.0%)  	leaq	(%r14,%r10,2),%rbx
 29,350 (0.0%)  	movq	%r13,-24(%rdi)
 29,350 (0.0%)  	sbbq	%r15,%r15
 29,350 (0.0%)  	shrq	$63,%r10
 29,350 (0.0%)  	leaq	(%rcx,%r11,2),%r8
 29,350 (0.0%)  	shrq	$63,%r11
 29,350 (0.0%)  	orq	%r10,%r8
 29,350 (0.0%)  	movq	0(%rdi),%r10
 29,350 (0.0%)  	movq	%r11,%r14
 29,350 (0.0%)  	mulq	%rax
 29,350 (0.0%)  	negq	%r15
 29,350 (0.0%)  	movq	8(%rdi),%r11
 29,350 (0.0%)  	adcq	%rax,%rbx
 29,350 (0.0%)  	movq	0(%rsi,%rbp,1),%rax
 29,350 (0.0%)  	movq	%rbx,-16(%rdi)
 29,350 (0.0%)  	adcq	%rdx,%r8
      .         
 29,350 (0.0%)  	leaq	(%r14,%r10,2),%r12
 29,350 (0.0%)  	movq	%r8,-8(%rdi)
 29,350 (0.0%)  	sbbq	%r15,%r15
 29,350 (0.0%)  	shrq	$63,%r10
 29,350 (0.0%)  	leaq	(%rcx,%r11,2),%r13
 29,350 (0.0%)  	shrq	$63,%r11
 29,350 (0.0%)  	orq	%r10,%r13
 29,350 (0.0%)  	movq	16(%rdi),%r10
 29,350 (0.0%)  	movq	%r11,%r14
 29,350 (0.0%)  	mulq	%rax
 29,350 (0.0%)  	negq	%r15
 29,350 (0.0%)  	movq	24(%rdi),%r11
 29,350 (0.0%)  	adcq	%rax,%r12
 29,350 (0.0%)  	movq	8(%rsi,%rbp,1),%rax
 29,350 (0.0%)  	movq	%r12,0(%rdi)
 29,350 (0.0%)  	adcq	%rdx,%r13
      .         
 29,350 (0.0%)  	leaq	(%r14,%r10,2),%rbx
 29,350 (0.0%)  	movq	%r13,8(%rdi)
 29,350 (0.0%)  	sbbq	%r15,%r15
 29,350 (0.0%)  	shrq	$63,%r10
 29,350 (0.0%)  	leaq	(%rcx,%r11,2),%r8
 29,350 (0.0%)  	shrq	$63,%r11
 29,350 (0.0%)  	orq	%r10,%r8
 29,350 (0.0%)  	movq	32(%rdi),%r10
 29,350 (0.0%)  	movq	%r11,%r14
 29,350 (0.0%)  	mulq	%rax
 29,350 (0.0%)  	negq	%r15
 29,350 (0.0%)  	movq	40(%rdi),%r11
 29,350 (0.0%)  	adcq	%rax,%rbx
 29,350 (0.0%)  	movq	16(%rsi,%rbp,1),%rax
 29,350 (0.0%)  	movq	%rbx,16(%rdi)
 29,350 (0.0%)  	adcq	%rdx,%r8
 29,350 (0.0%)  	movq	%r8,24(%rdi)
 29,350 (0.0%)  	sbbq	%r15,%r15
 29,350 (0.0%)  	leaq	64(%rdi),%rdi
 29,350 (0.0%)  	addq	$32,%rbp
 29,350 (0.0%)  	jnz	.Lsqr4x_shift_n_add
      .         
  8,308 (0.0%)  	leaq	(%r14,%r10,2),%r12
      .         .byte	0x67
      .         	shrq	$63,%r10
  4,154 (0.0%)  	leaq	(%rcx,%r11,2),%r13
  4,154 (0.0%)  	shrq	$63,%r11
  4,154 (0.0%)  	orq	%r10,%r13
  4,154 (0.0%)  	movq	-16(%rdi),%r10
  4,154 (0.0%)  	movq	%r11,%r14
  4,154 (0.0%)  	mulq	%rax
  4,154 (0.0%)  	negq	%r15
  4,154 (0.0%)  	movq	-8(%rdi),%r11
  4,154 (0.0%)  	adcq	%rax,%r12
  4,154 (0.0%)  	movq	-8(%rsi),%rax
  4,154 (0.0%)  	movq	%r12,-32(%rdi)
  4,154 (0.0%)  	adcq	%rdx,%r13
      .         
  4,154 (0.0%)  	leaq	(%r14,%r10,2),%rbx
  4,154 (0.0%)  	movq	%r13,-24(%rdi)
  4,154 (0.0%)  	sbbq	%r15,%r15
  4,154 (0.0%)  	shrq	$63,%r10
  4,154 (0.0%)  	leaq	(%rcx,%r11,2),%r8
  4,154 (0.0%)  	shrq	$63,%r11
  4,154 (0.0%)  	orq	%r10,%r8
  4,154 (0.0%)  	mulq	%rax
  4,154 (0.0%)  	negq	%r15
  4,154 (0.0%)  	adcq	%rax,%rbx
  4,154 (0.0%)  	adcq	%rdx,%r8
  4,154 (0.0%)  	movq	%rbx,-16(%rdi)
  8,308 (0.0%)  	movq	%r8,-8(%rdi)
      .         .byte	102,72,15,126,213
      .         __bn_sqr8x_reduction:
  4,154 (0.0%)  	xorq	%rax,%rax
  4,154 (0.0%)  	leaq	(%r9,%rbp,1),%rcx
  4,154 (0.0%)  	leaq	48+8(%rsp,%r9,2),%rdx
  4,154 (0.0%)  	movq	%rcx,0+8(%rsp)
  4,154 (0.0%)  	leaq	48+8(%rsp,%r9,1),%rdi
  4,154 (0.0%)  	movq	%rdx,8+8(%rsp)
  4,154 (0.0%)  	negq	%r9
  4,154 (0.0%)  	jmp	.L8x_reduction_loop
      .         
      .         .align	32
      .         .L8x_reduction_loop:
 33,504 (0.0%)  	leaq	(%rdi,%r9,1),%rdi
      .         .byte	0x66
      .         	movq	0(%rdi),%rbx
 16,752 (0.0%)  	movq	8(%rdi),%r9
 16,752 (0.0%)  	movq	16(%rdi),%r10
 16,752 (0.0%)  	movq	24(%rdi),%r11
 16,752 (0.0%)  	movq	32(%rdi),%r12
 16,752 (0.0%)  	movq	40(%rdi),%r13
 16,752 (0.0%)  	movq	48(%rdi),%r14
 16,752 (0.0%)  	movq	56(%rdi),%r15
 16,752 (0.0%)  	movq	%rax,(%rdx)
 33,504 (0.0%)  	leaq	64(%rdi),%rdi
      .         
      .         .byte	0x67
      .         	movq	%rbx,%r8
 16,752 (0.0%)  	imulq	32+8(%rsp),%rbx
 16,752 (0.0%)  	movq	0(%rbp),%rax
 16,752 (0.0%)  	movl	$8,%ecx
 16,752 (0.0%)  	jmp	.L8x_reduce
      .         
      .         .align	32
      .         .L8x_reduce:
134,016 (0.2%)  	mulq	%rbx
134,016 (0.2%)  	movq	8(%rbp),%rax
134,016 (0.2%)  	negq	%r8
134,016 (0.2%)  	movq	%rdx,%r8
134,016 (0.2%)  	adcq	$0,%r8
      .         
134,016 (0.2%)  	mulq	%rbx
134,016 (0.2%)  	addq	%rax,%r9
134,016 (0.2%)  	movq	16(%rbp),%rax
134,016 (0.2%)  	adcq	$0,%rdx
134,016 (0.2%)  	addq	%r9,%r8
134,016 (0.2%)  	movq	%rbx,48-8+8(%rsp,%rcx,8)
134,016 (0.2%)  	movq	%rdx,%r9
134,016 (0.2%)  	adcq	$0,%r9
      .         
134,016 (0.2%)  	mulq	%rbx
134,016 (0.2%)  	addq	%rax,%r10
134,016 (0.2%)  	movq	24(%rbp),%rax
134,016 (0.2%)  	adcq	$0,%rdx
134,016 (0.2%)  	addq	%r10,%r9
134,016 (0.2%)  	movq	32+8(%rsp),%rsi
134,016 (0.2%)  	movq	%rdx,%r10
134,016 (0.2%)  	adcq	$0,%r10
      .         
134,016 (0.2%)  	mulq	%rbx
134,016 (0.2%)  	addq	%rax,%r11
134,016 (0.2%)  	movq	32(%rbp),%rax
134,016 (0.2%)  	adcq	$0,%rdx
134,016 (0.2%)  	imulq	%r8,%rsi
134,016 (0.2%)  	addq	%r11,%r10
134,016 (0.2%)  	movq	%rdx,%r11
134,016 (0.2%)  	adcq	$0,%r11
      .         
134,016 (0.2%)  	mulq	%rbx
134,016 (0.2%)  	addq	%rax,%r12
134,016 (0.2%)  	movq	40(%rbp),%rax
134,016 (0.2%)  	adcq	$0,%rdx
134,016 (0.2%)  	addq	%r12,%r11
134,016 (0.2%)  	movq	%rdx,%r12
134,016 (0.2%)  	adcq	$0,%r12
      .         
134,016 (0.2%)  	mulq	%rbx
134,016 (0.2%)  	addq	%rax,%r13
134,016 (0.2%)  	movq	48(%rbp),%rax
134,016 (0.2%)  	adcq	$0,%rdx
134,016 (0.2%)  	addq	%r13,%r12
134,016 (0.2%)  	movq	%rdx,%r13
134,016 (0.2%)  	adcq	$0,%r13
      .         
134,016 (0.2%)  	mulq	%rbx
134,016 (0.2%)  	addq	%rax,%r14
134,016 (0.2%)  	movq	56(%rbp),%rax
134,016 (0.2%)  	adcq	$0,%rdx
134,016 (0.2%)  	addq	%r14,%r13
134,016 (0.2%)  	movq	%rdx,%r14
134,016 (0.2%)  	adcq	$0,%r14
      .         
134,016 (0.2%)  	mulq	%rbx
134,016 (0.2%)  	movq	%rsi,%rbx
134,016 (0.2%)  	addq	%rax,%r15
134,016 (0.2%)  	movq	0(%rbp),%rax
134,016 (0.2%)  	adcq	$0,%rdx
134,016 (0.2%)  	addq	%r15,%r14
134,016 (0.2%)  	movq	%rdx,%r15
134,016 (0.2%)  	adcq	$0,%r15
      .         
134,016 (0.2%)  	decl	%ecx
134,016 (0.2%)  	jnz	.L8x_reduce
      .         
 16,752 (0.0%)  	leaq	64(%rbp),%rbp
 16,752 (0.0%)  	xorq	%rax,%rax
 16,752 (0.0%)  	movq	8+8(%rsp),%rdx
 16,752 (0.0%)  	cmpq	0+8(%rsp),%rbp
 33,504 (0.0%)  	jae	.L8x_no_tail
      .         
      .         .byte	0x66
      .         	addq	0(%rdi),%r8
 16,752 (0.0%)  	adcq	8(%rdi),%r9
 16,752 (0.0%)  	adcq	16(%rdi),%r10
 16,752 (0.0%)  	adcq	24(%rdi),%r11
 16,752 (0.0%)  	adcq	32(%rdi),%r12
 16,752 (0.0%)  	adcq	40(%rdi),%r13
 16,752 (0.0%)  	adcq	48(%rdi),%r14
 16,752 (0.0%)  	adcq	56(%rdi),%r15
 16,752 (0.0%)  	sbbq	%rsi,%rsi
      .         
 16,752 (0.0%)  	movq	48+56+8(%rsp),%rbx
 16,752 (0.0%)  	movl	$8,%ecx
 16,752 (0.0%)  	movq	0(%rbp),%rax
 16,752 (0.0%)  	jmp	.L8x_tail
      .         
      .         .align	32
      .         .L8x_tail:
410,752 (0.5%)  	mulq	%rbx
410,752 (0.5%)  	addq	%rax,%r8
410,752 (0.5%)  	movq	8(%rbp),%rax
410,752 (0.5%)  	movq	%r8,(%rdi)
410,752 (0.5%)  	movq	%rdx,%r8
410,752 (0.5%)  	adcq	$0,%r8
      .         
410,752 (0.5%)  	mulq	%rbx
410,752 (0.5%)  	addq	%rax,%r9
410,752 (0.5%)  	movq	16(%rbp),%rax
410,752 (0.5%)  	adcq	$0,%rdx
410,752 (0.5%)  	addq	%r9,%r8
410,752 (0.5%)  	leaq	8(%rdi),%rdi
410,752 (0.5%)  	movq	%rdx,%r9
410,752 (0.5%)  	adcq	$0,%r9
      .         
410,752 (0.5%)  	mulq	%rbx
410,752 (0.5%)  	addq	%rax,%r10
410,752 (0.5%)  	movq	24(%rbp),%rax
410,752 (0.5%)  	adcq	$0,%rdx
410,752 (0.5%)  	addq	%r10,%r9
410,752 (0.5%)  	movq	%rdx,%r10
410,752 (0.5%)  	adcq	$0,%r10
      .         
410,752 (0.5%)  	mulq	%rbx
410,752 (0.5%)  	addq	%rax,%r11
410,752 (0.5%)  	movq	32(%rbp),%rax
410,752 (0.5%)  	adcq	$0,%rdx
410,752 (0.5%)  	addq	%r11,%r10
410,752 (0.5%)  	movq	%rdx,%r11
410,752 (0.5%)  	adcq	$0,%r11
      .         
410,752 (0.5%)  	mulq	%rbx
410,752 (0.5%)  	addq	%rax,%r12
410,752 (0.5%)  	movq	40(%rbp),%rax
410,752 (0.5%)  	adcq	$0,%rdx
410,752 (0.5%)  	addq	%r12,%r11
410,752 (0.5%)  	movq	%rdx,%r12
410,752 (0.5%)  	adcq	$0,%r12
      .         
410,752 (0.5%)  	mulq	%rbx
410,752 (0.5%)  	addq	%rax,%r13
410,752 (0.5%)  	movq	48(%rbp),%rax
410,752 (0.5%)  	adcq	$0,%rdx
410,752 (0.5%)  	addq	%r13,%r12
410,752 (0.5%)  	movq	%rdx,%r13
410,752 (0.5%)  	adcq	$0,%r13
      .         
410,752 (0.5%)  	mulq	%rbx
410,752 (0.5%)  	addq	%rax,%r14
410,752 (0.5%)  	movq	56(%rbp),%rax
410,752 (0.5%)  	adcq	$0,%rdx
410,752 (0.5%)  	addq	%r14,%r13
410,752 (0.5%)  	movq	%rdx,%r14
410,752 (0.5%)  	adcq	$0,%r14
      .         
410,752 (0.5%)  	mulq	%rbx
410,752 (0.5%)  	movq	48-16+8(%rsp,%rcx,8),%rbx
410,752 (0.5%)  	addq	%rax,%r15
410,752 (0.5%)  	adcq	$0,%rdx
410,752 (0.5%)  	addq	%r15,%r14
410,752 (0.5%)  	movq	0(%rbp),%rax
410,752 (0.5%)  	movq	%rdx,%r15
410,752 (0.5%)  	adcq	$0,%r15
      .         
410,752 (0.5%)  	decl	%ecx
410,752 (0.5%)  	jnz	.L8x_tail
      .         
 51,344 (0.1%)  	leaq	64(%rbp),%rbp
 51,344 (0.1%)  	movq	8+8(%rsp),%rdx
 51,344 (0.1%)  	cmpq	0+8(%rsp),%rbp
 51,344 (0.1%)  	jae	.L8x_tail_done
      .         
 34,592 (0.0%)  	movq	48+56+8(%rsp),%rbx
 34,592 (0.0%)  	negq	%rsi
 34,592 (0.0%)  	movq	0(%rbp),%rax
 34,592 (0.0%)  	adcq	0(%rdi),%r8
 34,592 (0.0%)  	adcq	8(%rdi),%r9
 34,592 (0.0%)  	adcq	16(%rdi),%r10
 34,592 (0.0%)  	adcq	24(%rdi),%r11
 34,592 (0.0%)  	adcq	32(%rdi),%r12
 34,592 (0.0%)  	adcq	40(%rdi),%r13
 34,592 (0.0%)  	adcq	48(%rdi),%r14
 34,592 (0.0%)  	adcq	56(%rdi),%r15
 34,592 (0.0%)  	sbbq	%rsi,%rsi
      .         
 34,592 (0.0%)  	movl	$8,%ecx
 34,592 (0.0%)  	jmp	.L8x_tail
      .         
      .         .align	32
      .         .L8x_tail_done:
 16,752 (0.0%)  	xorq	%rax,%rax
 16,752 (0.0%)  	addq	(%rdx),%r8
 16,752 (0.0%)  	adcq	$0,%r9
 16,752 (0.0%)  	adcq	$0,%r10
 16,752 (0.0%)  	adcq	$0,%r11
 16,752 (0.0%)  	adcq	$0,%r12
 16,752 (0.0%)  	adcq	$0,%r13
 16,752 (0.0%)  	adcq	$0,%r14
 16,752 (0.0%)  	adcq	$0,%r15
 16,752 (0.0%)  	adcq	$0,%rax
      .         
 16,752 (0.0%)  	negq	%rsi
      .         .L8x_no_tail:
 16,752 (0.0%)  	adcq	0(%rdi),%r8
 16,752 (0.0%)  	adcq	8(%rdi),%r9
 16,752 (0.0%)  	adcq	16(%rdi),%r10
 16,752 (0.0%)  	adcq	24(%rdi),%r11
 16,752 (0.0%)  	adcq	32(%rdi),%r12
 16,752 (0.0%)  	adcq	40(%rdi),%r13
 16,752 (0.0%)  	adcq	48(%rdi),%r14
 16,752 (0.0%)  	adcq	56(%rdi),%r15
 16,752 (0.0%)  	adcq	$0,%rax
 16,752 (0.0%)  	movq	-8(%rbp),%rcx
 33,504 (0.0%)  	xorq	%rsi,%rsi
      .         
      .         .byte	102,72,15,126,213
      .         
 16,752 (0.0%)  	movq	%r8,0(%rdi)
 33,504 (0.0%)  	movq	%r9,8(%rdi)
      .         .byte	102,73,15,126,217
 16,752 (0.0%)  	movq	%r10,16(%rdi)
 16,752 (0.0%)  	movq	%r11,24(%rdi)
 16,752 (0.0%)  	movq	%r12,32(%rdi)
 16,752 (0.0%)  	movq	%r13,40(%rdi)
 16,752 (0.0%)  	movq	%r14,48(%rdi)
 16,752 (0.0%)  	movq	%r15,56(%rdi)
 16,752 (0.0%)  	leaq	64(%rdi),%rdi
      .         
 16,752 (0.0%)  	cmpq	%rdx,%rdi
 20,906 (0.0%)  	jb	.L8x_reduction_loop
      .         	.byte	0xf3,0xc3
      .         .cfi_endproc	
      .         .size	bn_sqr8x_internal,.-bn_sqr8x_internal
      .         .type	__bn_post4x_internal,@function
      .         .align	32
      .         __bn_post4x_internal:
      .         .cfi_startproc	
  4,090 (0.0%)  	movq	0(%rbp),%r12
  4,090 (0.0%)  	leaq	(%rdi,%r9,1),%rbx
  8,180 (0.0%)  	movq	%r9,%rcx
      .         .byte	102,72,15,126,207
  8,180 (0.0%)  	negq	%rax
      .         .byte	102,72,15,126,206
  4,090 (0.0%)  	sarq	$3+2,%rcx
  4,090 (0.0%)  	decq	%r12
  4,090 (0.0%)  	xorq	%r10,%r10
  4,090 (0.0%)  	movq	8(%rbp),%r13
  4,090 (0.0%)  	movq	16(%rbp),%r14
  4,090 (0.0%)  	movq	24(%rbp),%r15
  4,090 (0.0%)  	jmp	.Lsqr4x_sub_entry
      .         
      .         .align	16
      .         .Lsqr4x_sub:
 34,566 (0.0%)  	movq	0(%rbp),%r12
 34,566 (0.0%)  	movq	8(%rbp),%r13
 34,566 (0.0%)  	movq	16(%rbp),%r14
 34,566 (0.0%)  	movq	24(%rbp),%r15
      .         .Lsqr4x_sub_entry:
 39,504 (0.1%)  	leaq	32(%rbp),%rbp
 39,504 (0.1%)  	notq	%r12
 39,504 (0.1%)  	notq	%r13
 39,504 (0.1%)  	notq	%r14
 39,504 (0.1%)  	notq	%r15
 39,504 (0.1%)  	andq	%rax,%r12
 39,504 (0.1%)  	andq	%rax,%r13
 39,504 (0.1%)  	andq	%rax,%r14
 39,504 (0.1%)  	andq	%rax,%r15
      .         
 39,504 (0.1%)  	negq	%r10
 39,504 (0.1%)  	adcq	0(%rbx),%r12
 39,504 (0.1%)  	adcq	8(%rbx),%r13
 39,504 (0.1%)  	adcq	16(%rbx),%r14
 39,504 (0.1%)  	adcq	24(%rbx),%r15
 39,504 (0.1%)  	movq	%r12,0(%rdi)
 39,504 (0.1%)  	leaq	32(%rbx),%rbx
 39,504 (0.1%)  	movq	%r13,8(%rdi)
 39,504 (0.1%)  	sbbq	%r10,%r10
 39,504 (0.1%)  	movq	%r14,16(%rdi)
 39,504 (0.1%)  	movq	%r15,24(%rdi)
 39,504 (0.1%)  	leaq	32(%rdi),%rdi
      .         
 39,504 (0.1%)  	incq	%rcx
 39,504 (0.1%)  	jnz	.Lsqr4x_sub
      .         
  4,938 (0.0%)  	movq	%r9,%r10
  9,876 (0.0%)  	negq	%r9
      .         	.byte	0xf3,0xc3
      .         .cfi_endproc	
      .         .size	__bn_post4x_internal,.-__bn_post4x_internal
      .         .type	bn_mulx4x_mont_gather5,@function
      .         .align	32
      .         bn_mulx4x_mont_gather5:
      .         .cfi_startproc	
      .         	movq	%rsp,%rax
-- line 2071 ----------------------------------------
-- line 3411 ----------------------------------------
      .         .cfi_endproc	
      .         .size	__bn_postx4x_internal,.-__bn_postx4x_internal
      .         .globl	bn_scatter5
      .         .hidden bn_scatter5
      .         .type	bn_scatter5,@function
      .         .align	16
      .         bn_scatter5:
      .         .cfi_startproc	
     64 (0.0%)  	cmpl	$0,%esi
     64 (0.0%)  	jz	.Lscatter_epilogue
      .         
      .         
      .         
      .         
      .         
      .         
      .         
      .         
      .         
     64 (0.0%)  	leaq	(%rdx,%rcx,8),%rdx
      .         .Lscatter:
  2,048 (0.0%)  	movq	(%rdi),%rax
  2,048 (0.0%)  	leaq	8(%rdi),%rdi
  2,048 (0.0%)  	movq	%rax,(%rdx)
  2,048 (0.0%)  	leaq	256(%rdx),%rdx
  2,048 (0.0%)  	subl	$1,%esi
  2,116 (0.0%)  	jnz	.Lscatter
      .         .Lscatter_epilogue:
      .         	.byte	0xf3,0xc3
      .         .cfi_endproc	
      .         .size	bn_scatter5,.-bn_scatter5
      .         
      .         .globl	bn_gather5
      .         .hidden bn_gather5
      .         .type	bn_gather5,@function
-- line 3445 ----------------------------------------
-- line 3446 ----------------------------------------
      .         .align	32
      .         bn_gather5:
      .         .cfi_startproc	
      .         .LSEH_begin_bn_gather5:
      .         
      .         .byte	0x4c,0x8d,0x14,0x24
      .         .cfi_def_cfa_register	%r10
      .         .byte	0x48,0x81,0xec,0x08,0x01,0x00,0x00
      2 (0.0%)  	leaq	.Linc(%rip),%rax
      2 (0.0%)  	andq	$-16,%rsp
      .         
      2 (0.0%)  	movd	%ecx,%xmm5
      2 (0.0%)  	movdqa	0(%rax),%xmm0
      2 (0.0%)  	movdqa	16(%rax),%xmm1
      2 (0.0%)  	leaq	128(%rdx),%r11
      2 (0.0%)  	leaq	128(%rsp),%rax
      .         
      2 (0.0%)  	pshufd	$0,%xmm5,%xmm5
      2 (0.0%)  	movdqa	%xmm1,%xmm4
      2 (0.0%)  	movdqa	%xmm1,%xmm2
      2 (0.0%)  	paddd	%xmm0,%xmm1
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm0
      2 (0.0%)  	movdqa	%xmm4,%xmm3
      .         
      2 (0.0%)  	paddd	%xmm1,%xmm2
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm1
      2 (0.0%)  	movdqa	%xmm0,-128(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm0
      .         
      2 (0.0%)  	paddd	%xmm2,%xmm3
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm2
      2 (0.0%)  	movdqa	%xmm1,-112(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm1
      .         
      2 (0.0%)  	paddd	%xmm3,%xmm0
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm3
      2 (0.0%)  	movdqa	%xmm2,-96(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm2
      2 (0.0%)  	paddd	%xmm0,%xmm1
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm0
      2 (0.0%)  	movdqa	%xmm3,-80(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm3
      .         
      2 (0.0%)  	paddd	%xmm1,%xmm2
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm1
      2 (0.0%)  	movdqa	%xmm0,-64(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm0
      .         
      2 (0.0%)  	paddd	%xmm2,%xmm3
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm2
      2 (0.0%)  	movdqa	%xmm1,-48(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm1
      .         
      2 (0.0%)  	paddd	%xmm3,%xmm0
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm3
      2 (0.0%)  	movdqa	%xmm2,-32(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm2
      2 (0.0%)  	paddd	%xmm0,%xmm1
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm0
      2 (0.0%)  	movdqa	%xmm3,-16(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm3
      .         
      2 (0.0%)  	paddd	%xmm1,%xmm2
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm1
      2 (0.0%)  	movdqa	%xmm0,0(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm0
      .         
      2 (0.0%)  	paddd	%xmm2,%xmm3
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm2
      2 (0.0%)  	movdqa	%xmm1,16(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm1
      .         
      2 (0.0%)  	paddd	%xmm3,%xmm0
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm3
      2 (0.0%)  	movdqa	%xmm2,32(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm2
      2 (0.0%)  	paddd	%xmm0,%xmm1
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm0
      2 (0.0%)  	movdqa	%xmm3,48(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm3
      .         
      2 (0.0%)  	paddd	%xmm1,%xmm2
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm1
      2 (0.0%)  	movdqa	%xmm0,64(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm0
      .         
      2 (0.0%)  	paddd	%xmm2,%xmm3
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm2
      2 (0.0%)  	movdqa	%xmm1,80(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm1
      .         
      2 (0.0%)  	paddd	%xmm3,%xmm0
      2 (0.0%)  	pcmpeqd	%xmm5,%xmm3
      2 (0.0%)  	movdqa	%xmm2,96(%rax)
      2 (0.0%)  	movdqa	%xmm4,%xmm2
      2 (0.0%)  	movdqa	%xmm3,112(%rax)
      2 (0.0%)  	jmp	.Lgather
      .         
      .         .align	32
      .         .Lgather:
     64 (0.0%)  	pxor	%xmm4,%xmm4
     64 (0.0%)  	pxor	%xmm5,%xmm5
     64 (0.0%)  	movdqa	-128(%r11),%xmm0
     64 (0.0%)  	movdqa	-112(%r11),%xmm1
     64 (0.0%)  	movdqa	-96(%r11),%xmm2
     64 (0.0%)  	pand	-128(%rax),%xmm0
     64 (0.0%)  	movdqa	-80(%r11),%xmm3
     64 (0.0%)  	pand	-112(%rax),%xmm1
     64 (0.0%)  	por	%xmm0,%xmm4
     64 (0.0%)  	pand	-96(%rax),%xmm2
     64 (0.0%)  	por	%xmm1,%xmm5
     64 (0.0%)  	pand	-80(%rax),%xmm3
     64 (0.0%)  	por	%xmm2,%xmm4
     64 (0.0%)  	por	%xmm3,%xmm5
     64 (0.0%)  	movdqa	-64(%r11),%xmm0
     64 (0.0%)  	movdqa	-48(%r11),%xmm1
     64 (0.0%)  	movdqa	-32(%r11),%xmm2
     64 (0.0%)  	pand	-64(%rax),%xmm0
     64 (0.0%)  	movdqa	-16(%r11),%xmm3
     64 (0.0%)  	pand	-48(%rax),%xmm1
     64 (0.0%)  	por	%xmm0,%xmm4
     64 (0.0%)  	pand	-32(%rax),%xmm2
     64 (0.0%)  	por	%xmm1,%xmm5
     64 (0.0%)  	pand	-16(%rax),%xmm3
     64 (0.0%)  	por	%xmm2,%xmm4
     64 (0.0%)  	por	%xmm3,%xmm5
     64 (0.0%)  	movdqa	0(%r11),%xmm0
     64 (0.0%)  	movdqa	16(%r11),%xmm1
     64 (0.0%)  	movdqa	32(%r11),%xmm2
     64 (0.0%)  	pand	0(%rax),%xmm0
     64 (0.0%)  	movdqa	48(%r11),%xmm3
     64 (0.0%)  	pand	16(%rax),%xmm1
     64 (0.0%)  	por	%xmm0,%xmm4
     64 (0.0%)  	pand	32(%rax),%xmm2
     64 (0.0%)  	por	%xmm1,%xmm5
     64 (0.0%)  	pand	48(%rax),%xmm3
     64 (0.0%)  	por	%xmm2,%xmm4
     64 (0.0%)  	por	%xmm3,%xmm5
     64 (0.0%)  	movdqa	64(%r11),%xmm0
     64 (0.0%)  	movdqa	80(%r11),%xmm1
     64 (0.0%)  	movdqa	96(%r11),%xmm2
     64 (0.0%)  	pand	64(%rax),%xmm0
     64 (0.0%)  	movdqa	112(%r11),%xmm3
     64 (0.0%)  	pand	80(%rax),%xmm1
     64 (0.0%)  	por	%xmm0,%xmm4
     64 (0.0%)  	pand	96(%rax),%xmm2
     64 (0.0%)  	por	%xmm1,%xmm5
     64 (0.0%)  	pand	112(%rax),%xmm3
     64 (0.0%)  	por	%xmm2,%xmm4
     64 (0.0%)  	por	%xmm3,%xmm5
     64 (0.0%)  	por	%xmm5,%xmm4
     64 (0.0%)  	leaq	256(%r11),%r11
      .         
     64 (0.0%)  	pshufd	$0x4e,%xmm4,%xmm0
     64 (0.0%)  	por	%xmm4,%xmm0
     64 (0.0%)  	movq	%xmm0,(%rdi)
     64 (0.0%)  	leaq	8(%rdi),%rdi
     64 (0.0%)  	subl	$1,%esi
     64 (0.0%)  	jnz	.Lgather
      .         
      4 (0.0%)  	leaq	(%r10),%rsp
      .         .cfi_def_cfa_register	%rsp
      .         	.byte	0xf3,0xc3
      .         .LSEH_end_bn_gather5:
      .         .cfi_endproc	
      .         .size	bn_gather5,.-bn_gather5
      .         .section	.rodata
      .         .align	64
      .         .Linc:
-- line 3614 ----------------------------------------

--------------------------------------------------------------------------------
-- Annotated source file: /home/ubuntu/proj/s2n/bindings/rust/s2n-tls-sys/lib/crypto/s2n_hmac.c
--------------------------------------------------------------------------------
Ir___________ 

-- line 44 ----------------------------------------
     .             case S2N_HASH_MD5_SHA1:   /* Fall through ... */
     .             default:
     .                 POSIX_BAIL(S2N_ERR_HASH_INVALID_ALGORITHM);
     .             }
     .             return S2N_SUCCESS;
     .         }
     .         
     .         int s2n_hmac_hash_alg(s2n_hmac_algorithm hmac_alg, s2n_hash_algorithm *out)
 1,932 (0.0%)  {
   552 (0.0%)      POSIX_ENSURE(S2N_MEM_IS_WRITABLE_CHECK(out, sizeof(*out)), S2N_ERR_PRECONDITION_VIOLATION);
 2,760 (0.0%)      switch(hmac_alg) {
    99 (0.0%)      case S2N_HMAC_NONE:       *out = S2N_HASH_NONE;   break;
     .             case S2N_HMAC_MD5:        *out = S2N_HASH_MD5;    break;
     .             case S2N_HMAC_SHA1:       *out = S2N_HASH_SHA1;   break;
     .             case S2N_HMAC_SHA224:     *out = S2N_HASH_SHA224; break;
   729 (0.0%)      case S2N_HMAC_SHA256:     *out = S2N_HASH_SHA256; break;
     .             case S2N_HMAC_SHA384:     *out = S2N_HASH_SHA384; break;
     .             case S2N_HMAC_SHA512:     *out = S2N_HASH_SHA512; break;
     .             case S2N_HMAC_SSLv3_MD5:  *out = S2N_HASH_MD5;    break;
     .             case S2N_HMAC_SSLv3_SHA1: *out = S2N_HASH_SHA1;   break;
     .             default:
     .                 POSIX_BAIL(S2N_ERR_HMAC_INVALID_ALGORITHM);
     .             }
   276 (0.0%)      return S2N_SUCCESS;
   828 (0.0%)  }
     .         
     .         int s2n_hmac_digest_size(s2n_hmac_algorithm hmac_alg, uint8_t *out)
 1,863 (0.0%)  {
     .             s2n_hash_algorithm hash_alg;
 1,449 (0.0%)      POSIX_GUARD(s2n_hmac_hash_alg(hmac_alg, &hash_alg));
 1,449 (0.0%)      POSIX_GUARD(s2n_hash_digest_size(hash_alg, out));
   207 (0.0%)      return S2N_SUCCESS;
 1,035 (0.0%)  }
     .         
     .         /* Return 1 if hmac algorithm is available, 0 otherwise. */
     .         bool s2n_hmac_is_available(s2n_hmac_algorithm hmac_alg)
   280 (0.0%)  {
   468 (0.0%)      switch(hmac_alg) {
     .             case S2N_HMAC_MD5:
     .             case S2N_HMAC_SSLv3_MD5:
     .             case S2N_HMAC_SSLv3_SHA1:
     .                 /* Some libcryptos, such as OpenSSL, disable MD5 by default when in FIPS mode, which is
     .                  * required in order to negotiate SSLv3. However, this is supported in AWS-LC.
     .                  */
     .                 return !s2n_is_in_fips_mode() || s2n_libcrypto_is_awslc();
     .             case S2N_HMAC_NONE:
     .             case S2N_HMAC_SHA1:
     .             case S2N_HMAC_SHA224:
     .             case S2N_HMAC_SHA256:
     .             case S2N_HMAC_SHA384:
     .             case S2N_HMAC_SHA512:
   112 (0.0%)          return true;
     .             }
     .             return false;
   112 (0.0%)  }
     .         
     .         static int s2n_sslv3_mac_init(struct s2n_hmac_state *state, s2n_hmac_algorithm alg, const void *key, uint32_t klen)
     .         {
     .             for (int i = 0; i < state->xor_pad_size; i++) {
     .                 state->xor_pad[i] = 0x36;
     .             }
     .         
     .             POSIX_GUARD(s2n_hash_update(&state->inner_just_key, key, klen));
-- line 106 ----------------------------------------
-- line 112 ----------------------------------------
     .         
     .             POSIX_GUARD(s2n_hash_update(&state->outer_just_key, key, klen));
     .             POSIX_GUARD(s2n_hash_update(&state->outer_just_key, state->xor_pad, state->xor_pad_size));
     .         
     .             return S2N_SUCCESS;
     .         }
     .         
     .         static int s2n_tls_hmac_init(struct s2n_hmac_state *state, s2n_hmac_algorithm alg, const void *key, uint32_t klen)
   504 (0.0%)  {
   336 (0.0%)      memset(&state->xor_pad, 0, sizeof(state->xor_pad));
     .         
   280 (0.0%)      if (klen > state->xor_pad_size) {
     .                 POSIX_GUARD(s2n_hash_update(&state->outer, key, klen));
     .                 POSIX_GUARD(s2n_hash_digest(&state->outer, state->digest_pad, state->digest_size));
     .                 POSIX_CHECKED_MEMCPY(state->xor_pad, state->digest_pad, state->digest_size);
     .             } else {
   852 (0.0%)          POSIX_CHECKED_MEMCPY(state->xor_pad, key, klen);
     .             }
     .         
21,896 (0.0%)      for (int i = 0; i < state->xor_pad_size; i++) {
35,840 (0.0%)          state->xor_pad[i] ^= 0x36;
     .             }
     .         
   672 (0.0%)      POSIX_GUARD(s2n_hash_update(&state->inner_just_key, state->xor_pad, state->xor_pad_size));
     .         
     .             /* 0x36 xor 0x5c == 0x6a */
21,896 (0.0%)      for (int i = 0; i < state->xor_pad_size; i++) {
35,840 (0.0%)          state->xor_pad[i] ^= 0x6a;
     .             }
     .         
   672 (0.0%)      POSIX_GUARD(s2n_hash_update(&state->outer_just_key, state->xor_pad, state->xor_pad_size));
    56 (0.0%)      return S2N_SUCCESS;
   168 (0.0%)  }
     .         
     .         int s2n_hmac_xor_pad_size(s2n_hmac_algorithm hmac_alg, uint16_t *xor_pad_size)
   392 (0.0%)  {
   112 (0.0%)      POSIX_ENSURE(S2N_MEM_IS_WRITABLE_CHECK(xor_pad_size, sizeof(*xor_pad_size)), S2N_ERR_PRECONDITION_VIOLATION);
   560 (0.0%)      switch(hmac_alg) {
    30 (0.0%)      case S2N_HMAC_NONE:       *xor_pad_size = 64;   break;
     .             case S2N_HMAC_MD5:        *xor_pad_size = 64;   break;
     .             case S2N_HMAC_SHA1:       *xor_pad_size = 64;   break;
     .             case S2N_HMAC_SHA224:     *xor_pad_size = 64;   break;
   138 (0.0%)      case S2N_HMAC_SHA256:     *xor_pad_size = 64;   break;
     .             case S2N_HMAC_SHA384:     *xor_pad_size = 128;  break;
     .             case S2N_HMAC_SHA512:     *xor_pad_size = 128;  break;
     .             case S2N_HMAC_SSLv3_MD5:  *xor_pad_size = 48;   break;
     .             case S2N_HMAC_SSLv3_SHA1: *xor_pad_size = 40;   break;
     .             default:
     .                 POSIX_BAIL(S2N_ERR_HMAC_INVALID_ALGORITHM);
     .             }
    56 (0.0%)      return S2N_SUCCESS;
   168 (0.0%)  }
     .         
     .         int s2n_hmac_hash_block_size(s2n_hmac_algorithm hmac_alg, uint16_t *block_size)
   392 (0.0%)  {
   112 (0.0%)      POSIX_ENSURE(S2N_MEM_IS_WRITABLE_CHECK(block_size, sizeof(*block_size)), S2N_ERR_PRECONDITION_VIOLATION);
   560 (0.0%)      switch(hmac_alg) {
    30 (0.0%)      case S2N_HMAC_NONE:       *block_size = 64;   break;
     .             case S2N_HMAC_MD5:        *block_size = 64;   break;
     .             case S2N_HMAC_SHA1:       *block_size = 64;   break;
     .             case S2N_HMAC_SHA224:     *block_size = 64;   break;
   138 (0.0%)      case S2N_HMAC_SHA256:     *block_size = 64;   break;
     .             case S2N_HMAC_SHA384:     *block_size = 128;  break;
     .             case S2N_HMAC_SHA512:     *block_size = 128;  break;
     .             case S2N_HMAC_SSLv3_MD5:  *block_size = 64;   break;
     .             case S2N_HMAC_SSLv3_SHA1: *block_size = 64;   break;
     .             default:
     .                 POSIX_BAIL(S2N_ERR_HMAC_INVALID_ALGORITHM);
     .             }
    56 (0.0%)      return S2N_SUCCESS;
   168 (0.0%)  }
     .         
     .         int s2n_hmac_new(struct s2n_hmac_state *state)
   300 (0.0%)  {
   100 (0.0%)      POSIX_ENSURE_REF(state);
   300 (0.0%)      POSIX_GUARD(s2n_hash_new(&state->inner));
   300 (0.0%)      POSIX_GUARD(s2n_hash_new(&state->inner_just_key));
   300 (0.0%)      POSIX_GUARD(s2n_hash_new(&state->outer));
   300 (0.0%)      POSIX_GUARD(s2n_hash_new(&state->outer_just_key));
   750 (0.0%)      POSIX_POSTCONDITION(s2n_hmac_state_validate(state));
    50 (0.0%)      return S2N_SUCCESS;
   150 (0.0%)  }
     .         
     .         S2N_RESULT s2n_hmac_state_validate(struct s2n_hmac_state *state)
 1,812 (0.0%)  {
   604 (0.0%)      RESULT_ENSURE_REF(state);
 2,718 (0.0%)      RESULT_GUARD(s2n_hash_state_validate(&state->inner));
 2,718 (0.0%)      RESULT_GUARD(s2n_hash_state_validate(&state->inner_just_key));
 2,718 (0.0%)      RESULT_GUARD(s2n_hash_state_validate(&state->outer));
 2,718 (0.0%)      RESULT_GUARD(s2n_hash_state_validate(&state->outer_just_key));
   302 (0.0%)      return S2N_RESULT_OK;
   906 (0.0%)  }
     .         
     .         int s2n_hmac_init(struct s2n_hmac_state *state, s2n_hmac_algorithm alg, const void *key, uint32_t klen)
   672 (0.0%)  {
   112 (0.0%)      POSIX_ENSURE_REF(state);
   336 (0.0%)      if (!s2n_hmac_is_available(alg)) {
     .                 /* Prevent hmacs from being used if they are not available. */
     .                 POSIX_BAIL(S2N_ERR_HMAC_INVALID_ALGORITHM);
     .             }
     .         
   168 (0.0%)      state->alg = alg;
   448 (0.0%)      POSIX_GUARD(s2n_hmac_hash_block_size(alg, &state->hash_block_size));
   112 (0.0%)      state->currently_in_hash_block = 0;
   448 (0.0%)      POSIX_GUARD(s2n_hmac_xor_pad_size(alg, &state->xor_pad_size));
   448 (0.0%)      POSIX_GUARD(s2n_hmac_digest_size(alg, &state->digest_size));
     .         
   224 (0.0%)      POSIX_ENSURE_GTE(sizeof(state->xor_pad), state->xor_pad_size);
   224 (0.0%)      POSIX_ENSURE_GTE(sizeof(state->digest_pad), state->digest_size);
     .             /* key needs to be as large as the biggest block size */
   224 (0.0%)      POSIX_ENSURE_GTE(sizeof(state->xor_pad), state->hash_block_size);
     .         
     .             s2n_hash_algorithm hash_alg;
   392 (0.0%)      POSIX_GUARD(s2n_hmac_hash_alg(alg, &hash_alg));
     .         
   448 (0.0%)      POSIX_GUARD(s2n_hash_init(&state->inner, hash_alg));
   448 (0.0%)      POSIX_GUARD(s2n_hash_init(&state->inner_just_key, hash_alg));
   448 (0.0%)      POSIX_GUARD(s2n_hash_init(&state->outer, hash_alg));
   448 (0.0%)      POSIX_GUARD(s2n_hash_init(&state->outer_just_key, hash_alg));
     .         
   224 (0.0%)      if (alg == S2N_HMAC_SSLv3_SHA1 || alg == S2N_HMAC_SSLv3_MD5) {
     .                 POSIX_GUARD(s2n_sslv3_mac_init(state, alg, key, klen));
     .             } else {
   448 (0.0%)          POSIX_GUARD(s2n_tls_hmac_init(state, alg, key, klen));
     .             }
     .         
     .             /* Once we have produced inner_just_key and outer_just_key, don't need the key material in xor_pad, so wipe it.
     .              * Since xor_pad is used as a source of bytes in s2n_hmac_digest_two_compression_rounds,
     .              * this also prevents uninitilized bytes being used.
     .              */
   336 (0.0%)      memset(&state->xor_pad, 0, sizeof(state->xor_pad));
   280 (0.0%)      POSIX_GUARD(s2n_hmac_reset(state));
     .         
    56 (0.0%)      return S2N_SUCCESS;
   336 (0.0%)  }
     .         
     .         int s2n_hmac_update(struct s2n_hmac_state *state, const void *in, uint32_t size)
   752 (0.0%)  {
 1,410 (0.0%)      POSIX_PRECONDITION(s2n_hmac_state_validate(state));
   376 (0.0%)      POSIX_ENSURE(state->hash_block_size != 0, S2N_ERR_PRECONDITION_VIOLATION);
     .             /* Keep track of how much of the current hash block is full
     .              *
     .              * Why the 4294949760 constant in this code? 4294949760 is the highest 32-bit
     .              * value that is congruent to 0 modulo all of our HMAC block sizes, that is also
     .              * at least 16k smaller than 2^32. It therefore has no effect on the mathematical
     .              * result, and no valid record size can cause it to overflow.
     .              *
     .              * The value was found with the following python code;
-- line 259 ----------------------------------------
-- line 265 ----------------------------------------
     .              *   x -= 1
     .              * print x
     .              *
     .              * What it does do however is ensure that the mod operation takes a
     .              * constant number of instruction cycles, regardless of the size of the
     .              * input. On some platforms, including Intel, the operation can take a
     .              * smaller number of cycles if the input is "small".
     .              */
    94 (0.0%)      const uint32_t HIGHEST_32_BIT = 4294949760;
   376 (0.0%)      POSIX_ENSURE(size <= (UINT32_MAX - HIGHEST_32_BIT), S2N_ERR_INTEGER_OVERFLOW);
   940 (0.0%)      uint32_t value = (HIGHEST_32_BIT + size) % state->hash_block_size;
   940 (0.0%)      POSIX_GUARD(s2n_add_overflow(state->currently_in_hash_block, value, &state->currently_in_hash_block));
 1,034 (0.0%)      state->currently_in_hash_block %= state->hash_block_size;
     .         
   658 (0.0%)      return s2n_hash_update(&state->inner, in, size);
   282 (0.0%)  }
     .         
     .         int s2n_hmac_digest(struct s2n_hmac_state *state, void *out, uint32_t size)
   350 (0.0%)  {
   750 (0.0%)      POSIX_PRECONDITION(s2n_hmac_state_validate(state));
   600 (0.0%)      POSIX_GUARD(s2n_hash_digest(&state->inner, state->digest_pad, state->digest_size));
   450 (0.0%)      POSIX_GUARD(s2n_hash_copy(&state->outer, &state->outer_just_key));
   600 (0.0%)      POSIX_GUARD(s2n_hash_update(&state->outer, state->digest_pad, state->digest_size));
     .         
   350 (0.0%)      return s2n_hash_digest(&state->outer, out, size);
   100 (0.0%)  }
     .         
     .         int s2n_hmac_digest_two_compression_rounds(struct s2n_hmac_state *state, void *out, uint32_t size)
     .         {
     .             /* Do the "real" work of this function. */
     .             POSIX_GUARD(s2n_hmac_digest(state, out, size));
     .         
     .             /* If there were 9 or more bytes of space left in the current hash block
     .              * then the serialized length, plus an 0x80 byte, will have fit in that block.
-- line 298 ----------------------------------------
-- line 310 ----------------------------------------
     .             /* Can't reuse a hash after it has been finalized, so reset and push another block in */
     .             POSIX_GUARD(s2n_hash_reset(&state->inner));
     .         
     .             /* No-op s2n_hash_update to normalize timing and guard against Lucky13. This does not affect the value of *out. */
     .             return s2n_hash_update(&state->inner, state->xor_pad, state->hash_block_size);
     .         }
     .         
     .         int s2n_hmac_free(struct s2n_hmac_state *state)
   200 (0.0%)  {
    80 (0.0%)      if (state) {
   240 (0.0%)          POSIX_GUARD(s2n_hash_free(&state->inner));
   240 (0.0%)          POSIX_GUARD(s2n_hash_free(&state->inner_just_key));
   240 (0.0%)          POSIX_GUARD(s2n_hash_free(&state->outer));
   240 (0.0%)          POSIX_GUARD(s2n_hash_free(&state->outer_just_key));
     .             }
     .         
    40 (0.0%)      return S2N_SUCCESS;
    80 (0.0%)  }
     .         
     .         int s2n_hmac_reset(struct s2n_hmac_state *state)
   972 (0.0%)  {
 1,620 (0.0%)      POSIX_PRECONDITION(s2n_hmac_state_validate(state));
   432 (0.0%)      POSIX_ENSURE(state->hash_block_size != 0, S2N_ERR_PRECONDITION_VIOLATION);
   972 (0.0%)      POSIX_GUARD(s2n_hash_copy(&state->inner, &state->inner_just_key));
     .         
   108 (0.0%)      uint64_t bytes_in_hash = 0;
   864 (0.0%)      POSIX_GUARD(s2n_hash_get_currently_in_hash_total(&state->inner, &bytes_in_hash));
   972 (0.0%)      bytes_in_hash %= state->hash_block_size;
   432 (0.0%)      POSIX_ENSURE(bytes_in_hash <= UINT32_MAX, S2N_ERR_INTEGER_OVERFLOW);
     .             /* The length of the key is not private, so don't need to do tricky math here */
   432 (0.0%)      state->currently_in_hash_block = bytes_in_hash;
   108 (0.0%)      return S2N_SUCCESS;
   648 (0.0%)  }
     .         
     .         int s2n_hmac_digest_verify(const void *a, const void *b, uint32_t len)
    28 (0.0%)  {
    36 (0.0%)      return S2N_SUCCESS - !s2n_constant_time_equals(a, b, len);
     8 (0.0%)  }
     .         
     .         int s2n_hmac_copy(struct s2n_hmac_state *to, struct s2n_hmac_state *from)
     .         {
     .             POSIX_PRECONDITION(s2n_hmac_state_validate(to));
     .             POSIX_PRECONDITION(s2n_hmac_state_validate(from));
     .             /* memcpy cannot be used on s2n_hmac_state as the underlying s2n_hash implementation's
     .              * copy must be used. This is enforced when the s2n_hash implementation is s2n_evp_hash.
     .              */
-- line 355 ----------------------------------------

--------------------------------------------------------------------------------
-- Annotated source file: /home/ubuntu/proj/s2n/bindings/rust/s2n-tls-sys/lib/stuffer/s2n_stuffer.c
--------------------------------------------------------------------------------
Ir___________ 

-- line 18 ----------------------------------------
     .         #include <sys/param.h>
     .         
     .         #include "error/s2n_errno.h"
     .         #include "utils/s2n_blob.h"
     .         #include "utils/s2n_mem.h"
     .         #include "utils/s2n_safety.h"
     .         
     .         S2N_RESULT s2n_stuffer_validate(const struct s2n_stuffer *stuffer)
24,390 (0.0%)  {
     .             /**
     .              * Note that we do not assert any properties on the tainted field,
     .              * as any boolean value in that field is valid.
     .              */
 8,130 (0.0%)      RESULT_ENSURE_REF(stuffer);
32,520 (0.0%)      RESULT_GUARD(s2n_blob_validate(&stuffer->blob));
44,168 (0.1%)      RESULT_DEBUG_ENSURE(S2N_IMPLIES(stuffer->growable, stuffer->alloced), S2N_ERR_SAFETY);
     .         
     .             /* <= is valid because we can have a fully written/read stuffer */
36,585 (0.0%)      RESULT_DEBUG_ENSURE(stuffer->high_water_mark <= stuffer->blob.size, S2N_ERR_SAFETY);
36,585 (0.0%)      RESULT_DEBUG_ENSURE(stuffer->write_cursor <= stuffer->high_water_mark, S2N_ERR_SAFETY);
36,585 (0.0%)      RESULT_DEBUG_ENSURE(stuffer->read_cursor <= stuffer->write_cursor, S2N_ERR_SAFETY);
 4,065 (0.0%)      return S2N_RESULT_OK;
12,195 (0.0%)  }
     .         
     .         S2N_RESULT s2n_stuffer_reservation_validate(const struct s2n_stuffer_reservation *reservation)
   306 (0.0%)  {
     .             /**
     .              * Note that we need two dereferences here to decrease proof complexity
     .              * for CBMC (see https://github.com/awslabs/s2n/issues/2290). We can roll back
     .              * this change once CBMC can handle common subexpression elimination.
     .              */
   102 (0.0%)      RESULT_ENSURE_REF(reservation);
   255 (0.0%)      const struct s2n_stuffer_reservation reserve_obj = *reservation;
   408 (0.0%)      RESULT_GUARD(s2n_stuffer_validate(reserve_obj.stuffer));
   561 (0.0%)      const struct s2n_stuffer stuffer_obj = *(reserve_obj.stuffer);
     .         
     .             /* Verify that write_cursor + length can be represented as a uint32_t without overflow */
   306 (0.0%)      RESULT_ENSURE_LTE(reserve_obj.write_cursor, UINT32_MAX - reserve_obj.length);
     .             /* The entire reservation must fit between the stuffer read and write cursors */
   357 (0.0%)      RESULT_ENSURE_LTE(reserve_obj.write_cursor + reserve_obj.length, stuffer_obj.write_cursor);
   204 (0.0%)      RESULT_ENSURE_GTE(reserve_obj.write_cursor, stuffer_obj.read_cursor);
     .         
    51 (0.0%)      return S2N_RESULT_OK;
   153 (0.0%)  }
     .         
     .         int s2n_stuffer_init(struct s2n_stuffer *stuffer, struct s2n_blob *in)
   658 (0.0%)  {
   188 (0.0%)      POSIX_ENSURE_MUT(stuffer);
 1,410 (0.0%)      POSIX_PRECONDITION(s2n_blob_validate(in));
   752 (0.0%)      stuffer->blob = *in;
   188 (0.0%)      stuffer->read_cursor = 0;
   188 (0.0%)      stuffer->write_cursor = 0;
   188 (0.0%)      stuffer->high_water_mark = 0;
   376 (0.0%)      stuffer->alloced = 0;
   376 (0.0%)      stuffer->growable = 0;
   376 (0.0%)      stuffer->tainted = 0;
 1,410 (0.0%)      POSIX_POSTCONDITION(s2n_stuffer_validate(stuffer));
    94 (0.0%)      return S2N_SUCCESS;
   282 (0.0%)  }
     .         
     .         int s2n_stuffer_init_written(struct s2n_stuffer *stuffer, struct s2n_blob *in)
     7 (0.0%)  {
     2 (0.0%)      POSIX_ENSURE_REF(in);
     7 (0.0%)      POSIX_GUARD(s2n_stuffer_init(stuffer, in));
     8 (0.0%)      POSIX_GUARD(s2n_stuffer_skip_write(stuffer, in->size));
     1 (0.0%)      return S2N_SUCCESS;
     3 (0.0%)  }
     .         
     .         int s2n_stuffer_alloc(struct s2n_stuffer *stuffer, const uint32_t size)
    84 (0.0%)  {
    24 (0.0%)      POSIX_ENSURE_REF(stuffer);
    60 (0.0%)      *stuffer = (struct s2n_stuffer){ 0 };
    84 (0.0%)      POSIX_GUARD(s2n_alloc(&stuffer->blob, size));
    84 (0.0%)      POSIX_GUARD(s2n_stuffer_init(stuffer, &stuffer->blob));
     .         
    48 (0.0%)      stuffer->alloced = 1;
     .         
   180 (0.0%)      POSIX_POSTCONDITION(s2n_stuffer_validate(stuffer));
    12 (0.0%)      return S2N_SUCCESS;
    36 (0.0%)  }
     .         
     .         int s2n_stuffer_growable_alloc(struct s2n_stuffer *stuffer, const uint32_t size)
    60 (0.0%)  {
    70 (0.0%)      POSIX_GUARD(s2n_stuffer_alloc(stuffer, size));
     .         
    40 (0.0%)      stuffer->growable = 1;
     .         
   150 (0.0%)      POSIX_POSTCONDITION(s2n_stuffer_validate(stuffer));
    10 (0.0%)      return S2N_SUCCESS;
    20 (0.0%)  }
     .         
     .         int s2n_stuffer_free(struct s2n_stuffer *stuffer)
   130 (0.0%)  {
   390 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
   130 (0.0%)      if (stuffer->alloced) {
    20 (0.0%)          POSIX_GUARD(s2n_free(&stuffer->blob));
     .             }
   130 (0.0%)      *stuffer = (struct s2n_stuffer){ 0 };
    26 (0.0%)      return S2N_SUCCESS;
    52 (0.0%)  }
     .         
     .         int s2n_stuffer_free_without_wipe(struct s2n_stuffer *stuffer)
     .         {
     .             POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
     .             if (stuffer->alloced) {
     .                 POSIX_GUARD(s2n_free_without_wipe(&stuffer->blob));
     .             }
     .             *stuffer = (struct s2n_stuffer){ 0 };
     .             return S2N_SUCCESS;
     .         }
     .         
     .         int s2n_stuffer_resize(struct s2n_stuffer *stuffer, const uint32_t size)
    70 (0.0%)  {
   150 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
    50 (0.0%)      POSIX_ENSURE(!stuffer->tainted, S2N_ERR_RESIZE_TAINTED_STUFFER);
    50 (0.0%)      POSIX_ENSURE(stuffer->growable, S2N_ERR_RESIZE_STATIC_STUFFER);
     .         
    40 (0.0%)      if (size == stuffer->blob.size) {
    12 (0.0%)          return S2N_SUCCESS;
     .             }
     .         
     8 (0.0%)      if (size == 0) {
     6 (0.0%)          s2n_stuffer_wipe(stuffer);
     8 (0.0%)          return s2n_free(&stuffer->blob);
     .             }
     .         
     8 (0.0%)      if (size < stuffer->blob.size) {
     .                 POSIX_CHECKED_MEMSET(stuffer->blob.data + size, S2N_WIPE_PATTERN, (stuffer->blob.size - size));
     .                 if (stuffer->read_cursor > size) {
     .                     stuffer->read_cursor = size;
     .                 }
     .                 if (stuffer->write_cursor > size) {
     .                     stuffer->write_cursor = size;
     .                 }
     .                 if (stuffer->high_water_mark > size) {
     .                     stuffer->high_water_mark = size;
     .                 }
     .                 stuffer->blob.size = size;
     .                 POSIX_POSTCONDITION(s2n_stuffer_validate(stuffer));
     .                 return S2N_SUCCESS;
     .             }
     .         
    14 (0.0%)      POSIX_GUARD(s2n_realloc(&stuffer->blob, size));
    30 (0.0%)      POSIX_POSTCONDITION(s2n_stuffer_validate(stuffer));
     2 (0.0%)      return S2N_SUCCESS;
    30 (0.0%)  }
     .         
     .         int s2n_stuffer_resize_if_empty(struct s2n_stuffer *stuffer, const uint32_t size)
   140 (0.0%)  {
   300 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
    80 (0.0%)      if (stuffer->blob.data == NULL) {
    10 (0.0%)          POSIX_ENSURE(!stuffer->tainted, S2N_ERR_RESIZE_TAINTED_STUFFER);
    10 (0.0%)          POSIX_ENSURE(stuffer->growable, S2N_ERR_RESIZE_STATIC_STUFFER);
    14 (0.0%)          POSIX_GUARD(s2n_realloc(&stuffer->blob, size));
     .             }
   300 (0.0%)      POSIX_POSTCONDITION(s2n_stuffer_validate(stuffer));
    20 (0.0%)      return S2N_SUCCESS;
    60 (0.0%)  }
     .         
     .         int s2n_stuffer_rewrite(struct s2n_stuffer *stuffer)
   270 (0.0%)  {
   810 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
   108 (0.0%)      stuffer->write_cursor = 0;
   108 (0.0%)      stuffer->read_cursor = 0;
   810 (0.0%)      POSIX_POSTCONDITION(s2n_stuffer_validate(stuffer));
    54 (0.0%)      return S2N_SUCCESS;
   108 (0.0%)  }
     .         
     .         int s2n_stuffer_rewind_read(struct s2n_stuffer *stuffer, const uint32_t size)
   161 (0.0%)  {
   345 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
    92 (0.0%)      POSIX_ENSURE(stuffer->read_cursor >= size, S2N_ERR_STUFFER_OUT_OF_DATA);
   138 (0.0%)      stuffer->read_cursor -= size;
   345 (0.0%)      POSIX_POSTCONDITION(s2n_stuffer_validate(stuffer));
    23 (0.0%)      return S2N_SUCCESS;
    69 (0.0%)  }
     .         
     .         int s2n_stuffer_reread(struct s2n_stuffer *stuffer)
   330 (0.0%)  {
   990 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
   132 (0.0%)      stuffer->read_cursor = 0;
    66 (0.0%)      return S2N_SUCCESS;
   132 (0.0%)  }
     .         
     .         int s2n_stuffer_wipe_n(struct s2n_stuffer *stuffer, const uint32_t size)
    98 (0.0%)  {
   210 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
    84 (0.0%)      uint32_t wipe_size = MIN(size, stuffer->write_cursor);
     .         
    84 (0.0%)      stuffer->write_cursor -= wipe_size;
   112 (0.0%)      stuffer->read_cursor = MIN(stuffer->read_cursor, stuffer->write_cursor);
   238 (0.0%)      POSIX_CHECKED_MEMSET(stuffer->blob.data + stuffer->write_cursor, S2N_WIPE_PATTERN, wipe_size);
     .         
   210 (0.0%)      POSIX_POSTCONDITION(s2n_stuffer_validate(stuffer));
    14 (0.0%)      return S2N_SUCCESS;
    42 (0.0%)  }
     .         
     .         bool s2n_stuffer_is_consumed(struct s2n_stuffer *stuffer)
    36 (0.0%)  {
   144 (0.0%)      return stuffer && (stuffer->read_cursor == stuffer->write_cursor) && !stuffer->tainted;
    18 (0.0%)  }
     .         
     .         int s2n_stuffer_wipe(struct s2n_stuffer *stuffer)
   378 (0.0%)  {
   945 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
   252 (0.0%)      if (!s2n_stuffer_is_wiped(stuffer)) {
   810 (0.0%)          POSIX_CHECKED_MEMSET(stuffer->blob.data, S2N_WIPE_PATTERN, stuffer->high_water_mark);
     .             }
     .         
   252 (0.0%)      stuffer->tainted = 0;
   126 (0.0%)      stuffer->write_cursor = 0;
   126 (0.0%)      stuffer->read_cursor = 0;
   126 (0.0%)      stuffer->high_water_mark = 0;
   945 (0.0%)      POSIX_POSTCONDITION(s2n_stuffer_validate(stuffer));
    63 (0.0%)      return S2N_SUCCESS;
   189 (0.0%)  }
     .         
     .         int s2n_stuffer_skip_read(struct s2n_stuffer *stuffer, uint32_t n)
 1,862 (0.0%)  {
 3,990 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
 1,862 (0.0%)      POSIX_ENSURE(s2n_stuffer_data_available(stuffer) >= n, S2N_ERR_STUFFER_OUT_OF_DATA);
     .         
 1,596 (0.0%)      stuffer->read_cursor += n;
   266 (0.0%)      return S2N_SUCCESS;
   798 (0.0%)  }
     .         
     .         void *s2n_stuffer_raw_read(struct s2n_stuffer *stuffer, uint32_t data_len)
   420 (0.0%)  {
   490 (0.0%)      PTR_GUARD_POSIX(s2n_stuffer_skip_read(stuffer, data_len));
     .         
   280 (0.0%)      stuffer->tainted = 1;
     .         
   910 (0.0%)      return (stuffer->blob.data) ? (stuffer->blob.data + stuffer->read_cursor - data_len) : NULL;
   140 (0.0%)  }
     .         
     .         int s2n_stuffer_read(struct s2n_stuffer *stuffer, struct s2n_blob *out)
     7 (0.0%)  {
     2 (0.0%)      POSIX_ENSURE_REF(out);
     .         
     8 (0.0%)      return s2n_stuffer_read_bytes(stuffer, out->data, out->size);
     3 (0.0%)  }
     .         
     .         int s2n_stuffer_erase_and_read(struct s2n_stuffer *stuffer, struct s2n_blob *out)
     .         {
     .             POSIX_GUARD(s2n_stuffer_skip_read(stuffer, out->size));
     .         
     .             void *ptr = (stuffer->blob.data) ? (stuffer->blob.data + stuffer->read_cursor - out->size) : NULL;
     .             POSIX_ENSURE(S2N_MEM_IS_READABLE(ptr, out->size), S2N_ERR_NULL);
     .         
     .             POSIX_CHECKED_MEMCPY(out->data, ptr, out->size);
     .             POSIX_CHECKED_MEMSET(ptr, 0, out->size);
     .         
     .             return S2N_SUCCESS;
     .         }
     .         
     .         int s2n_stuffer_read_bytes(struct s2n_stuffer *stuffer, uint8_t *data, uint32_t size)
 1,120 (0.0%)  {
   280 (0.0%)      POSIX_ENSURE_REF(data);
 2,100 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
   980 (0.0%)      POSIX_GUARD(s2n_stuffer_skip_read(stuffer, size));
   560 (0.0%)      POSIX_ENSURE_REF(stuffer->blob.data);
 1,260 (0.0%)      void *ptr = stuffer->blob.data + stuffer->read_cursor - size;
     .         
 2,240 (0.0%)      POSIX_CHECKED_MEMCPY(data, ptr, size);
     .         
   140 (0.0%)      return S2N_SUCCESS;
   420 (0.0%)  }
     .         
     .         int s2n_stuffer_erase_and_read_bytes(struct s2n_stuffer *stuffer, uint8_t *data, uint32_t size)
     8 (0.0%)  {
     7 (0.0%)      POSIX_GUARD(s2n_stuffer_skip_read(stuffer, size));
     4 (0.0%)      POSIX_ENSURE_REF(stuffer->blob.data);
     9 (0.0%)      void *ptr = stuffer->blob.data + stuffer->read_cursor - size;
     .         
    16 (0.0%)      POSIX_CHECKED_MEMCPY(data, ptr, size);
    16 (0.0%)      POSIX_CHECKED_MEMSET(ptr, 0, size);
     .         
     1 (0.0%)      return S2N_SUCCESS;
     3 (0.0%)  }
     .         
     .         int s2n_stuffer_skip_write(struct s2n_stuffer *stuffer, const uint32_t n)
 3,258 (0.0%)  {
 8,145 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
 3,801 (0.0%)      POSIX_GUARD(s2n_stuffer_reserve_space(stuffer, n));
 3,258 (0.0%)      stuffer->write_cursor += n;
 4,344 (0.0%)      stuffer->high_water_mark = MAX(stuffer->write_cursor, stuffer->high_water_mark);
 8,145 (0.0%)      POSIX_POSTCONDITION(s2n_stuffer_validate(stuffer));
   543 (0.0%)      return S2N_SUCCESS;
 1,086 (0.0%)  }
     .         
     .         void *s2n_stuffer_raw_write(struct s2n_stuffer *stuffer, const uint32_t data_len)
   126 (0.0%)  {
   147 (0.0%)      PTR_GUARD_POSIX(s2n_stuffer_skip_write(stuffer, data_len));
     .         
    84 (0.0%)      stuffer->tainted = 1;
     .         
   273 (0.0%)      return (stuffer->blob.data) ? (stuffer->blob.data + stuffer->write_cursor - data_len) : NULL;
    42 (0.0%)  }
     .         
     .         int s2n_stuffer_write(struct s2n_stuffer *stuffer, const struct s2n_blob *in)
   462 (0.0%)  {
 1,155 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
 1,155 (0.0%)      POSIX_PRECONDITION(s2n_blob_validate(in));
   616 (0.0%)      return s2n_stuffer_write_bytes(stuffer, in->data, in->size);
   154 (0.0%)  }
     .         
     .         int s2n_stuffer_write_bytes(struct s2n_stuffer *stuffer, const uint8_t *data, const uint32_t size)
 2,336 (0.0%)  {
 1,128 (0.0%)      POSIX_ENSURE(S2N_MEM_IS_READABLE(data, size), S2N_ERR_SAFETY);
 4,380 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
 2,044 (0.0%)      POSIX_GUARD(s2n_stuffer_skip_write(stuffer, size));
     .         
 2,628 (0.0%)      void *ptr = stuffer->blob.data + stuffer->write_cursor - size;
 1,128 (0.0%)      POSIX_ENSURE(S2N_MEM_IS_READABLE(ptr, size), S2N_ERR_NULL);
     .         
   876 (0.0%)      if (ptr == data) {
    30 (0.0%)          POSIX_POSTCONDITION(s2n_stuffer_validate(stuffer));
     4 (0.0%)          return S2N_SUCCESS;
     .             }
     .         
 4,460 (0.0%)      POSIX_CHECKED_MEMCPY(ptr, data, size);
     .         
 4,350 (0.0%)      POSIX_POSTCONDITION(s2n_stuffer_validate(stuffer));
   290 (0.0%)      return S2N_SUCCESS;
   876 (0.0%)  }
     .         
     .         int s2n_stuffer_writev_bytes(struct s2n_stuffer *stuffer, const struct iovec *iov, size_t iov_count, uint32_t offs,
     .                 uint32_t size)
    90 (0.0%)  {
   135 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
    18 (0.0%)      POSIX_ENSURE_REF(iov);
    54 (0.0%)      void *ptr = s2n_stuffer_raw_write(stuffer, size);
    36 (0.0%)      POSIX_ENSURE(S2N_MEM_IS_READABLE(ptr, size), S2N_ERR_NULL);
     .         
    36 (0.0%)      size_t size_left = size, to_skip = offs;
    45 (0.0%)      for (size_t i = 0; i < iov_count; i++) {
    72 (0.0%)          if (to_skip >= iov[i].iov_len) {
     .                     to_skip -= iov[i].iov_len;
     .                     continue;
     .                 }
    72 (0.0%)          size_t iov_len_op = iov[i].iov_len - to_skip;
    27 (0.0%)          POSIX_ENSURE_LTE(iov_len_op, UINT32_MAX);
    18 (0.0%)          uint32_t iov_len = (uint32_t) iov_len_op;
    45 (0.0%)          uint32_t iov_size_to_take = MIN(size_left, iov_len);
    72 (0.0%)          POSIX_ENSURE_REF(iov[i].iov_base);
    72 (0.0%)          POSIX_ENSURE_LT(to_skip, iov[i].iov_len);
   207 (0.0%)          POSIX_CHECKED_MEMCPY(ptr, ((uint8_t *) (iov[i].iov_base)) + to_skip, iov_size_to_take);
    18 (0.0%)          size_left -= iov_size_to_take;
    18 (0.0%)          if (size_left == 0) {
     9 (0.0%)              break;
     .                 }
     .                 ptr = (void *) ((uint8_t *) ptr + iov_size_to_take);
     .                 to_skip = 0;
     .             }
     .         
     9 (0.0%)      return S2N_SUCCESS;
    27 (0.0%)  }
     .         
     .         static int s2n_stuffer_copy_impl(struct s2n_stuffer *from, struct s2n_stuffer *to, const uint32_t len)
   216 (0.0%)  {
   189 (0.0%)      POSIX_GUARD(s2n_stuffer_skip_read(from, len));
   189 (0.0%)      POSIX_GUARD(s2n_stuffer_skip_write(to, len));
     .         
   378 (0.0%)      uint8_t *from_ptr = (from->blob.data) ? (from->blob.data + from->read_cursor - len) : NULL;
   378 (0.0%)      uint8_t *to_ptr = (to->blob.data) ? (to->blob.data + to->write_cursor - len) : NULL;
     .         
   414 (0.0%)      POSIX_CHECKED_MEMCPY(to_ptr, from_ptr, len);
     .         
    27 (0.0%)      return S2N_SUCCESS;
    81 (0.0%)  }
     .         
     .         int s2n_stuffer_reserve_space(struct s2n_stuffer *stuffer, uint32_t n)
 5,630 (0.0%)  {
 8,445 (0.0%)      POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
 3,941 (0.0%)      if (s2n_stuffer_space_remaining(stuffer) < n) {
     .                 POSIX_ENSURE(stuffer->growable, S2N_ERR_STUFFER_IS_FULL);
     .                 /* Always grow a stuffer by at least 1k */
     .                 const uint32_t growth = MAX(n - s2n_stuffer_space_remaining(stuffer), S2N_MIN_STUFFER_GROWTH_IN_BYTES);
     .                 uint32_t new_size = 0;
     .                 POSIX_GUARD(s2n_add_overflow(stuffer->blob.size, growth, &new_size));
     .                 POSIX_GUARD(s2n_stuffer_resize(stuffer, new_size));
     .             }
 8,445 (0.0%)      POSIX_POSTCONDITION(s2n_stuffer_validate(stuffer));
   563 (0.0%)      return S2N_SUCCESS;
 3,378 (0.0%)  }
     .         
     .         /* Copies "len" bytes from "from" to "to".
     .          * If the copy cannot succeed (i.e. there are either not enough bytes available, or there is not enough space to write them
     .          * restore the old value of the stuffer */
     .         int s2n_stuffer_copy(struct s2n_stuffer *from, struct s2n_stuffer *to, const uint32_t len)
   189 (0.0%)  {
    81 (0.0%)      const uint32_t orig_read_cursor = from->read_cursor;
    81 (0.0%)      const uint32_t orig_write_cursor = to->write_cursor;
     .         
   216 (0.0%)      if (s2n_stuffer_copy_impl(from, to, len) < 0) {
     .                 from->read_cursor = orig_read_cursor;
     .                 to->write_cursor = orig_write_cursor;
     .                 S2N_ERROR_PRESERVE_ERRNO();
     .             }
     .         
    27 (0.0%)      return S2N_SUCCESS;
    54 (0.0%)  }
     .         
     .         int s2n_stuffer_extract_blob(struct s2n_stuffer *stuffer, struct s2n_blob *out)
     .         {
     .             POSIX_PRECONDITION(s2n_stuffer_validate(stuffer));
     .             POSIX_ENSURE_REF(out);
     .             POSIX_GUARD(s2n_realloc(out, s2n_stuffer_data_available(stuffer)));
     .         
     .             if (s2n_stuffer_data_available(stuffer) > 0) {
-- line 427 ----------------------------------------

--------------------------------------------------------------------------------
-- Annotated source file: /home/ubuntu/proj/s2n/bindings/rust/s2n-tls-sys/lib/utils/s2n_blob.c
--------------------------------------------------------------------------------
Ir___________ 

-- line 19 ----------------------------------------
     .         #include <string.h>
     .         #include <sys/param.h>
     .         
     .         #include "api/s2n.h"
     .         #include "error/s2n_errno.h"
     .         #include "utils/s2n_safety.h"
     .         
     .         S2N_RESULT s2n_blob_validate(const struct s2n_blob *b)
28,830 (0.0%)  {
 9,610 (0.0%)      RESULT_ENSURE_REF(b);
35,889 (0.0%)      RESULT_DEBUG_ENSURE(S2N_IMPLIES(b->data == NULL, b->size == 0), S2N_ERR_SAFETY);
35,889 (0.0%)      RESULT_DEBUG_ENSURE(S2N_IMPLIES(b->data == NULL, b->allocated == 0), S2N_ERR_SAFETY);
61,344 (0.1%)      RESULT_DEBUG_ENSURE(S2N_IMPLIES(b->growable == 0, b->allocated == 0), S2N_ERR_SAFETY);
52,237 (0.1%)      RESULT_DEBUG_ENSURE(S2N_IMPLIES(b->growable != 0, b->size <= b->allocated), S2N_ERR_SAFETY);
44,366 (0.1%)      RESULT_DEBUG_ENSURE(S2N_MEM_IS_READABLE(b->data, b->allocated), S2N_ERR_SAFETY);
64,820 (0.1%)      RESULT_DEBUG_ENSURE(S2N_MEM_IS_READABLE(b->data, b->size), S2N_ERR_SAFETY);
 4,805 (0.0%)      return S2N_RESULT_OK;
14,415 (0.0%)  }
     .         
     .         int s2n_blob_init(struct s2n_blob *b, uint8_t *data, uint32_t size)
 1,952 (0.0%)  {
   488 (0.0%)      POSIX_ENSURE_REF(b);
   968 (0.0%)      POSIX_ENSURE(S2N_MEM_IS_READABLE(data, size), S2N_ERR_SAFETY);
 2,928 (0.0%)      *b = (struct s2n_blob){ .data = data, .size = size, .allocated = 0, .growable = 0 };
 3,660 (0.0%)      POSIX_POSTCONDITION(s2n_blob_validate(b));
   244 (0.0%)      return S2N_SUCCESS;
   732 (0.0%)  }
     .         
     .         int s2n_blob_zero(struct s2n_blob *b)
   684 (0.0%)  {
 1,710 (0.0%)      POSIX_PRECONDITION(s2n_blob_validate(b));
 1,708 (0.0%)      POSIX_CHECKED_MEMSET(b->data, 0, MAX(b->allocated, b->size));
 1,710 (0.0%)      POSIX_POSTCONDITION(s2n_blob_validate(b));
   114 (0.0%)      return S2N_SUCCESS;
   342 (0.0%)  }
     .         
     .         int s2n_blob_slice(const struct s2n_blob *b, struct s2n_blob *slice, uint32_t offset, uint32_t size)
   144 (0.0%)  {
   180 (0.0%)      POSIX_PRECONDITION(s2n_blob_validate(b));
   180 (0.0%)      POSIX_PRECONDITION(s2n_blob_validate(slice));
     .         
    12 (0.0%)      uint32_t slice_size = 0;
    96 (0.0%)      POSIX_GUARD(s2n_add_overflow(offset, size, &slice_size));
    60 (0.0%)      POSIX_ENSURE(b->size >= slice_size, S2N_ERR_SIZE_MISMATCH);
   132 (0.0%)      slice->data = (b->data) ? (b->data + offset) : NULL;
    36 (0.0%)      slice->size = size;
    48 (0.0%)      slice->growable = 0;
    24 (0.0%)      slice->allocated = 0;
     .         
   180 (0.0%)      POSIX_POSTCONDITION(s2n_blob_validate(slice));
    12 (0.0%)      return S2N_SUCCESS;
    72 (0.0%)  }
     .         
     .         int s2n_blob_char_to_lower(struct s2n_blob *b)
     .         {
     .             POSIX_PRECONDITION(s2n_blob_validate(b));
     .             for (size_t i = 0; i < b->size; i++) {
     .                 b->data[i] = tolower(b->data[i]);
     .             }
     .             POSIX_POSTCONDITION(s2n_blob_validate(b));
-- line 78 ----------------------------------------

--------------------------------------------------------------------------------
-- Annotated source file: /home/ubuntu/proj/s2n/bindings/rust/s2n-tls-sys/lib/utils/s2n_result.c
--------------------------------------------------------------------------------
Ir___________ 

-- line 79 ----------------------------------------
     .         #include "utils/s2n_result.h"
     .         
     .         #include <stdbool.h>
     .         
     .         #include "api/s2n.h"
     .         
     .         /* returns true when the result is S2N_RESULT_OK */
     .         inline bool s2n_result_is_ok(s2n_result result)
72,896 (0.1%)  {
54,672 (0.1%)      return result.__error_signal == S2N_SUCCESS;
36,448 (0.0%)  }
     .         
     .         /* returns true when the result is S2N_RESULT_ERROR */
     .         inline bool s2n_result_is_error(s2n_result result)
    64 (0.0%)  {
    48 (0.0%)      return result.__error_signal == S2N_FAILURE;
    32 (0.0%)  }
     .         
     .         /* ignores the returned result of a function */
     .         inline void s2n_result_ignore(s2n_result result)
    40 (0.0%)  {
     .             /* noop */
    30 (0.0%)  }

--------------------------------------------------------------------------------
-- Annotation summary
--------------------------------------------------------------------------------
Ir________________ 

76,402,989 (96.9%)    annotated: files known & above threshold & readable, line numbers known
         0            annotated: files known & above threshold & readable, line numbers unknown
         0          unannotated: files known & above threshold & two or more non-identical
   123,691  (0.2%)  unannotated: files known & above threshold & unreadable 
 2,344,716  (3.0%)  unannotated: files known & below threshold
     8,662  (0.0%)  unannotated: files unknown

